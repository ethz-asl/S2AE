{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPH Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from pyquaternion.quaternion import Quaternion\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from nuscenes.utils.data_classes import PointCloud, LidarPointCloud, RadarPointCloud, Box\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import os.path as osp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "from matplotlib import cm\n",
    "from sphere import Sphere\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "from functools import partial\n",
    "from dh_grid import DHGrid\n",
    "from sphere import Sphere\n",
    "from img_sphere import ImageSphere\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "34149 lidarseg,\n",
      "Done loading in 33.756 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 7.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "#dataset_path = '/mnt/data/datasets/nuscenes/v1.0-mini/'\n",
    "dataset_path = '/media/berlukas/T7 Touch/data/nuscenes'\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot=dataset_path, verbose=True)\n",
    "all_cam_strings = ['CAM_FRONT','CAM_FRONT_RIGHT','CAM_BACK_RIGHT','CAM_BACK','CAM_BACK_LEFT','CAM_FRONT_LEFT']\n",
    "#all_cam_strings = ['CAM_FRONT_LEFT','CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT']\n",
    "lidar_string = 'LIDAR_TOP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGrid(bw):\n",
    "    n_grid = 2 * bw\n",
    "    k = 0;\n",
    "    points = np.empty([2, n_grid, n_grid])\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            points[0, i, j] = (np.pi*(2*i+1))/(4*bw)\n",
    "            points[1, i, j] = (2*np.pi*j)/(2*bw);\n",
    "            k = k + 1;\n",
    "    return points\n",
    "\n",
    "def createGrid_old(bw):\n",
    "        n_grid = 2 * bw\n",
    "        k = 0;\n",
    "        points = np.empty([n_grid * n_grid, 2])\n",
    "        for i in range(n_grid):\n",
    "            for j in range(n_grid):\n",
    "                points[k, 0] = (np.pi*(2*i+1))/(4*bw)\n",
    "                points[k, 1] = (2*np.pi*j)/(2*bw);\n",
    "                k = k + 1;\n",
    "        return points\n",
    "\n",
    "def ConvertGridToEuclidean(grid):\n",
    "    cart_grid = np.zeros([3, grid.shape[1], grid.shape[2]])\n",
    "    cart_grid[0,:,:] = np.multiply(np.sin(grid[0, :,:]), np.cos(grid[1,:,:]))\n",
    "    cart_grid[1,:,:] = np.multiply(np.sin(grid[0, :, :]), np.sin(grid[1, :, :]))\n",
    "    cart_grid[2,:,:] = np.cos(grid[0, :, :])    \n",
    "    return cart_grid\n",
    "\n",
    "def convertGridToEuclidean_old(grid):\n",
    "    cart_grid = np.zeros([ grid.shape[0], 3])\n",
    "    cart_grid[:,0] = np.multiply(np.sin(grid[:,0]), np.cos(grid[:,1]))\n",
    "    cart_grid[:,1] = np.multiply(np.sin(grid[:,0]), np.sin(grid[:,1]))\n",
    "    cart_grid[:,2] = np.cos(grid[:,0])    \n",
    "    return cart_grid\n",
    "\n",
    "def rgb_to_greyscale(r,g,b):\n",
    "    return 0.2126*r + 0.7152*g + 0.0722*b\n",
    "\n",
    "def transform_from_pcl_to_cam(nusc, pointsensor, cam, pc):\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def transform_from_cam_to_pcl(nusc, pointsensor, cam, pc):\n",
    "    # Transform from the camera into the vehicle's ego frame\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])    \n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "    \n",
    "    # Transform from the ego frame (cam) to the global frame\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "        \n",
    "    # Transform from the global frame to the ego frame of the LiDAR.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])    \n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "        \n",
    "    # Transform from the ego frame (LiDAR) to the LiDAR frame\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])    \n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def project_pc_on_cam(pc, cam_intrinsics, depths):\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    points = view_points(pc.points[:3, :], cam_intrinsics, normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "    # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "    # casing for non-keyframes which are slightly out of sync.\n",
    "    min_dist = 0.0001\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths < -1.0)\n",
    "    mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "    mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "    return points, mask\n",
    "\n",
    "def sample_mono_from_image(im, pc, mask):\n",
    "    n_mask = len(mask)\n",
    "    #print(f\"mask len is {n_mask}\")\n",
    "    for i in range(0,n_mask):    \n",
    "        visible = mask[i]\n",
    "        if not visible:\n",
    "            continue\n",
    "        cur_point = (points[0,i], points[1,i])    \n",
    "        px = im.getpixel(cur_point)    \n",
    "        intensity = rgb_to_greyscale(px[0], px[1], px[2])\n",
    "        pc.points[3,i] = intensity\n",
    "    return pc\n",
    "\n",
    "def visualizeRawPointCloud(cloud, jupyter = False):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    colors = mapIntensityToRGB(cloud[:, 3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "\n",
    "    if jupyter:\n",
    "        self.__visualizeJupyter(pcd)\n",
    "    else:\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "        \n",
    "def mapIntensityToRGB(i):\n",
    "    return cm.jet(plt.Normalize(min(i), max(i))(i))\n",
    "\n",
    "def writeRawPointCloud(cloud, filename):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    colors = mapIntensityToRGB(cloud[:, 3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.io.write_point_cloud(filename, pcd)\n",
    "\n",
    "class SamplingPointCloud(PointCloud):\n",
    "\n",
    "    @staticmethod\n",
    "    def nbr_dims() -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of dimensions.\n",
    "        :return: Number of dimensions.\n",
    "        \"\"\"\n",
    "        return 4\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_name: str) -> 'SamplingPointCloud':\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_bw(cls, bw, scale = 100) -> 'SamplingPointCloud':\n",
    "        grid = createGrid_old(bw)\n",
    "        xyz_grid = convertGridToEuclidean_old(grid) * scale\n",
    "        intensities = np.zeros((xyz_grid.shape[0],1))\n",
    "        sampling_grid = np.hstack((xyz_grid, np.ones((xyz_grid.shape[0], 1), dtype=xyz_grid.dtype)))\n",
    "        return cls(sampling_grid.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(nusc.scene)} scenes in the dataset.\")\n",
    "#for scene in nusc.scene:\n",
    "scene = nusc.scene[1]\n",
    "first_sample_token = scene['first_sample_token']\n",
    "sample = nusc.get('sample', first_sample_token)\n",
    "pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "bw = 100\n",
    "scale = 100\n",
    "pc = SamplingPointCloud.from_bw(bw, scale)\n",
    "\n",
    "for cam_str in all_cam_strings:  \n",
    "    print(f\"Sampling for cam {cam_str}\")\n",
    "    cam = nusc.get('sample_data', sample['data'][cam_str])\n",
    "    im = Image.open(osp.join(dataset_path, cam['filename']))    \n",
    "\n",
    "    # Transform pointcloud into the camera frame\n",
    "    pc = transform_from_pcl_to_cam(nusc, pointsensor, cam, pc)\n",
    "    # Grab the depths (camera frame z axis points away from the camera).\n",
    "    depths = pc.points[2, :]\n",
    "#    print(f\"depths {depths[1:20]}\")\n",
    "\n",
    "    # Project the points onto the image plane\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    intrinsics = np.array(cs_record['camera_intrinsic'])\n",
    "    points, mask = project_pc_on_cam(pc, intrinsics, depths)\n",
    "    filtered_points = points[:, mask]\n",
    "\n",
    "    # Sample the intensity values from the image\n",
    "    pc = sample_mono_from_image(im, pc, mask)    \n",
    "    \n",
    "    # Transform back into the LiDAR frame\n",
    "    pc = transform_from_cam_to_pcl(nusc, pointsensor, cam, pc)\n",
    "    #visualizeRawPointCloud(pc.points.T)\n",
    "        \n",
    "visualizeRawPointCloud(pc.points.T / 100)\n",
    "#writeRawPointCloud(pc.points.T, 'test_full.ply')\n",
    "#writeRawPointCloud(pc.points.T/100, 'test_small.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init axes.\n",
    "dot_size = 2\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 16))\n",
    "ax.imshow(im)\n",
    "ax.scatter(filtered_points[0, :], filtered_points[1, :], s=dot_size)\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "# Convert dataset to PLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "# export_images = export_ds + '/SPH_IMAGES/'\n",
    "# export_clouds = export_ds + '/SPH_CLOUDS/'\n",
    "bw = 100\n",
    "scale = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the images to PLY files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progresser_images(sample, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample_sphere = ImageSphere(sample)\n",
    "    return sample_sphere.sampleUsingGrid(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 850 scenes in the dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ef8fb02a2149e4b699e87270ef8c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=850.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'progresser_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-76d256ddce14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading complete. Computing features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDHGrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogresser_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sph_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{export_ds}/images.npy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'progresser_images' is not defined"
     ]
    }
   ],
   "source": [
    "n_scenes = len(nusc.scene)\n",
    "print(f\"Processing {len(nusc.scene)} scenes in the dataset.\")\n",
    "\n",
    "all_sph_images = [None] * n_scenes\n",
    "for i in tqdm(range(0, n_scenes)):\n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    sample = nusc.get('sample', first_sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    pc = SamplingPointCloud.from_bw(bw, scale)\n",
    "\n",
    "    for cam_str in all_cam_strings:  \n",
    "        #print(f\"Sampling for cam {cam_str}\")\n",
    "        cam = nusc.get('sample_data', sample['data'][cam_str])\n",
    "        im = Image.open(osp.join(dataset_path, cam['filename']))    \n",
    "\n",
    "        # Transform pointcloud into the camera frame\n",
    "        pc = transform_from_pcl_to_cam(nusc, pointsensor, cam, pc)\n",
    "        # Grab the depths (camera frame z axis points away from the camera).\n",
    "        depths = pc.points[2, :]\n",
    "\n",
    "        # Project the points onto the image plane\n",
    "        cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "        intrinsics = np.array(cs_record['camera_intrinsic'])\n",
    "        points, mask = project_pc_on_cam(pc, intrinsics, depths)\n",
    "        filtered_points = points[:, mask]\n",
    "\n",
    "        # Sample the intensity values from the image\n",
    "        pc = sample_mono_from_image(im, pc, mask)    \n",
    "\n",
    "        # Transform back into the LiDAR frame\n",
    "        pc = transform_from_cam_to_pcl(nusc, pointsensor, cam, pc)\n",
    "        #visualizeRawPointCloud(pc.points.T)\n",
    "        \n",
    "        #filename = f\"{export_images}images{i}.ply\"\n",
    "        #writeRawPointCloud(pc.points.T, filename)\n",
    "    all_sph_images[i] = pc.points.T / scale\n",
    "    \n",
    "print(f\"Loading complete. Computing features...\")    \n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "img_features = process_map(partial(progresser_images, grid=grid), all_sph_images, max_workers=8)\n",
    "\n",
    "filename = f\"{export_ds}/images.npy\"\n",
    "np.save(filename, img_features)\n",
    "print(f\"Wrote features to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the pointclouds to PLYs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progresser(sample, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample_sphere = Sphere(sample)\n",
    "    return sample_sphere.sampleUsingGrid(grid)\n",
    "\n",
    "def writeRawPointCloud(cloud, filename):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    colors = mapIntensityToRGB(cloud[:, 3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.io.write_point_cloud(filename, pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 850 scenes in the dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff23dfc336f4b13bedabfdbbafa5f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=850.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ef2f6eb849483fa57ea4288cd19d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=850.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote features to /mnt/data/datasets/nuscenes/processed/clouds.npy\n"
     ]
    }
   ],
   "source": [
    "n_scenes = len(nusc.scene)\n",
    "print(f\"Processing {len(nusc.scene)} scenes in the dataset.\")\n",
    "\n",
    "all_point_clouds = [None] * n_scenes\n",
    "for i in tqdm(range(0, n_scenes)):\n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    sample = nusc.get('sample', first_sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "\n",
    "    # Load a single lidar point cloud.\n",
    "    pcl_path = osp.join(dataset_path, pointsensor['filename'])\n",
    "    pc = LidarPointCloud.from_file(pcl_path)\n",
    "    \n",
    "    all_point_clouds[i] = pc.points.T\n",
    "    \n",
    "print(f\"Loading complete. Computing features...\")    \n",
    "#visualizeRawPointCloud(all_point_clouds[0])\n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "pcl_features = process_map(partial(progresser, grid=grid), all_point_clouds, max_workers=8)\n",
    "\n",
    "filename = f\"{export_ds}/clouds.npy\"\n",
    "np.save(filename, pcl_features)\n",
    "print(f\"Wrote features to {filename}\")\n",
    "\n",
    "#for i in range(0, n_scenes):   \n",
    "#    filename = f\"{export_clouds}cloud{i}.ply\"\n",
    "#    writeRawPointCloud(pcl_features[i], filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiDAR segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 850 scenes in the dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880a6be94c5c4faf89a3ca00f331589b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=850.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489078a650614f24b9d25c3551b22c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=850.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote features to /mnt/data/datasets/nuscenes/processed/sem_clouds.npy\n"
     ]
    }
   ],
   "source": [
    "n_scenes = len(nusc.scene)\n",
    "print(f\"Processing {len(nusc.scene)} scenes in the dataset.\")\n",
    "\n",
    "all_sem_clouds = [None] * n_scenes\n",
    "for i in tqdm(range(0, n_scenes)):\n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    sample = nusc.get('sample', first_sample_token)\n",
    "\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "\n",
    "    # Load a single lidar point cloud.\n",
    "    pcl_path = osp.join(dataset_path, pointsensor['filename'])\n",
    "    pc = LidarPointCloud.from_file(pcl_path)\n",
    "    points_xyz = pc.points.T[:,0:3]\n",
    "\n",
    "    # Load the semantic segmentation.\n",
    "    #nusc.get_sample_lidarseg_stats(sample['token'], sort_by='count')\n",
    "    sample_data_token = sample['data'][lidar_string]\n",
    "    lidarseg_labels_filename = osp.join(nusc.dataroot, nusc.get('lidarseg', sample_data_token)['filename'])\n",
    "    points_label = np.fromfile(lidarseg_labels_filename, dtype=np.uint8)  # [num_points]\n",
    "\n",
    "    # Combine the two.\n",
    "    points_xyzl = np.column_stack((points_xyz, points_label))\n",
    "    all_sem_clouds[i] = points_xyzl\n",
    "    \n",
    "print(f\"Loading complete. Computing features...\")    \n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "sem_features = process_map(partial(progresser, grid=grid), all_sem_clouds, max_workers=8)    \n",
    "\n",
    "filename = f\"{export_ds}/sem_clouds.npy\"\n",
    "np.save(filename, sem_features)\n",
    "print(f\"Wrote features to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current spherical cloud shape: (2, 200, 200) and current spherical image shape (1, 200, 200)\n",
      "current reshaped cloud shape (40000, 2) and current reshaped image shape (40000, 1)\n",
      "sampling pointcloud shape is (40000, 3)\n"
     ]
    }
   ],
   "source": [
    "n_clouds = len(pcl_features)\n",
    "n_images = len(img_features)\n",
    "assert n_clouds > 0\n",
    "assert n_clouds == n_images\n",
    "\n",
    "i = 15\n",
    "cur_cloud = pcl_features[i]\n",
    "cur_sem_cloud = sem_features[i]\n",
    "cur_image = img_features[i]\n",
    "print(f\"current spherical cloud shape: {cur_cloud.shape} and current spherical image shape {cur_image.shape}\")\n",
    "cur_cloud = np.reshape(cur_cloud, (2, -1)).T\n",
    "cur_sem_cloud = np.reshape(cur_sem_cloud, (2, -1)).T\n",
    "cur_image = np.reshape(cur_image, (1, -1)).T\n",
    "print(f\"current reshaped cloud shape {cur_cloud.shape} and current reshaped image shape {cur_image.shape}\")\n",
    "\n",
    "pc = SamplingPointCloud.from_bw(bw, 1)\n",
    "points_xyz = pc.points.T[:,0:3]\n",
    "print(f\"sampling pointcloud shape is {points_xyz.shape}\")\n",
    "points_xyzi = np.column_stack((points_xyz, cur_cloud[:,1]))\n",
    "points_xyzp = np.column_stack((points_xyz, cur_image))\n",
    "points_xyzl = np.column_stack((points_xyz, cur_sem_cloud[:,1]))\n",
    "\n",
    "visualizeRawPointCloud(points_xyzl)\n",
    "#visualizeRawPointCloud(points_xyzp)\n",
    "#visualizeRawPointCloud(points_xyzl)\n",
    "\n",
    "\n",
    "#print(f\"reshaped cloud shape is {cloud.shape}\")\n",
    "#visualizeRawPointCloud(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  2., 13., 17., 24., 25., 26., 27., 28., 30., 31.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(cur_sem_cloud[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Check decoded signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature(feature, idx = 0, bw = 100):\n",
    "    pc = SamplingPointCloud.from_bw(bw, 1)\n",
    "    points_xyz = pc.points.T[:,0:3]\n",
    "    points_xyzd = np.column_stack((points_xyz, feature[:,idx]))\n",
    "    visualizeRawPointCloud(points_xyzd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the decoded/sem signal: (85, 32, 200, 200)/(850, 2, 200, 200).\n",
      "shape of the reshaped decoded/sem: (40000, 1)/(40000, 2)\n"
     ]
    }
   ],
   "source": [
    "decoded_filename = f\"{export_ds}/decoded.npy\"\n",
    "sem_filename = f\"{export_ds}/sem_clouds.npy\"\n",
    "indices_filename = f\"{export_ds}/indices.npy\"\n",
    "\n",
    "decoded_features = np.load(decoded_filename)\n",
    "sem_features = np.load(sem_filename)\n",
    "test_indices = np.load(indices_filename)\n",
    "print(f\"shape of the decoded/sem signal: {decoded_features.shape}/{sem_features.shape}.\")\n",
    "\n",
    "n_decoded = decoded_features.shape[0]\n",
    "assert test_indices.shape[0] == n_decoded\n",
    "\n",
    "#for i in range(0, n_decoded):\n",
    "i = 10\n",
    "cur_idx = test_indices[i]\n",
    "cur_decoded = decoded_features[i, :, :, :]\n",
    "cur_decoded = np.argmax(cur_decoded, axis=0)\n",
    "cur_decoded = np.reshape(cur_decoded, (1, -1)).T\n",
    "\n",
    "cur_sem_cloud = sem_features[cur_idx, :, :, :]\n",
    "cur_sem_cloud = np.reshape(cur_sem_cloud, (2, -1)).T\n",
    "\n",
    "print(f\"shape of the reshaped decoded/sem: {cur_decoded.shape}/{cur_sem_cloud.shape}\")\n",
    "\n",
    "visualize_feature(cur_decoded, 0, 100)\n",
    "visualize_feature(cur_sem_cloud, 1, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
