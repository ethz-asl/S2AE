{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPH Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from pyquaternion.quaternion import Quaternion\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from nuscenes.utils.data_classes import PointCloud, LidarPointCloud, RadarPointCloud, Box\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import os.path as osp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "from matplotlib import cm\n",
    "from sphere import Sphere\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "from functools import partial\n",
    "from dh_grid import DHGrid\n",
    "from sphere import Sphere\n",
    "from img_sphere import ImageSphere\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 0.353 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/mnt/data/datasets/nuscenes/v1.0-mini/'\n",
    "# dataset_path = '/media/berlukas/T7 Touch/data/nuscenes'\n",
    "# nusc = NuScenes(version='v1.0-trainval', dataroot=dataset_path, verbose=True)\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=dataset_path, verbose=True)\n",
    "all_cam_strings = ['CAM_FRONT','CAM_FRONT_RIGHT','CAM_BACK_RIGHT','CAM_BACK','CAM_BACK_LEFT','CAM_FRONT_LEFT']\n",
    "#all_cam_strings = ['CAM_FRONT_LEFT','CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT']\n",
    "lidar_string = 'LIDAR_TOP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGrid(bw):\n",
    "    n_grid = 2 * bw\n",
    "    k = 0;\n",
    "    points = np.empty([2, n_grid, n_grid])\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            points[0, i, j] = (np.pi*(2*i+1))/(4*bw)\n",
    "            points[1, i, j] = (2*np.pi*j)/(2*bw);\n",
    "            k = k + 1;\n",
    "    return points\n",
    "\n",
    "def createGrid_old(bw):\n",
    "        n_grid = 2 * bw\n",
    "        k = 0;\n",
    "        points = np.empty([n_grid * n_grid, 2])\n",
    "        for i in range(n_grid):\n",
    "            for j in range(n_grid):\n",
    "                points[k, 0] = (np.pi*(2*i+1))/(4*bw)\n",
    "                points[k, 1] = (2*np.pi*j)/(2*bw);\n",
    "                k = k + 1;\n",
    "        return points\n",
    "\n",
    "def ConvertGridToEuclidean(grid):\n",
    "    cart_grid = np.zeros([3, grid.shape[1], grid.shape[2]])\n",
    "    cart_grid[0,:,:] = np.multiply(np.sin(grid[0, :,:]), np.cos(grid[1,:,:]))\n",
    "    cart_grid[1,:,:] = np.multiply(np.sin(grid[0, :, :]), np.sin(grid[1, :, :]))\n",
    "    cart_grid[2,:,:] = np.cos(grid[0, :, :])    \n",
    "    return cart_grid\n",
    "\n",
    "def convertGridToEuclidean_old(grid):\n",
    "    cart_grid = np.zeros([ grid.shape[0], 3])\n",
    "    cart_grid[:,0] = np.multiply(np.sin(grid[:,0]), np.cos(grid[:,1]))\n",
    "    cart_grid[:,1] = np.multiply(np.sin(grid[:,0]), np.sin(grid[:,1]))\n",
    "    cart_grid[:,2] = np.cos(grid[:,0])    \n",
    "    return cart_grid\n",
    "\n",
    "def rgb_to_greyscale(r,g,b):\n",
    "    return 0.2126*r + 0.7152*g + 0.0722*b\n",
    "#     return 0.3*r + 0.59*g + 0.11*b    \n",
    "\n",
    "def transform_from_pcl_to_cam(nusc, pointsensor, cam, pc):\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def transform_from_cam_to_pcl(nusc, pointsensor, cam, pc):\n",
    "    # Transform from the camera into the vehicle's ego frame\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])    \n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "    \n",
    "    # Transform from the ego frame (cam) to the global frame\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "        \n",
    "    # Transform from the global frame to the ego frame of the LiDAR.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])    \n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "        \n",
    "    # Transform from the ego frame (LiDAR) to the LiDAR frame\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])    \n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def project_pc_on_cam(pc, cam_intrinsics, depths):\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    points = view_points(pc.points[:3, :], cam_intrinsics, normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "    # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "    # casing for non-keyframes which are slightly out of sync.\n",
    "    min_dist = 0.0001\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths < -1.0)\n",
    "    mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "    mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "    return points, mask\n",
    "\n",
    "def sample_mono_from_image(im, pc, mask):\n",
    "    n_mask = len(mask)\n",
    "    #print(f\"mask len is {n_mask}\")\n",
    "    for i in range(0,n_mask):    \n",
    "        visible = mask[i]\n",
    "        if not visible:\n",
    "            continue\n",
    "        cur_point = (points[0,i], points[1,i])    \n",
    "        px = im.getpixel(cur_point)    \n",
    "        intensity = rgb_to_greyscale(px[0], px[1], px[2])\n",
    "        pc.points[3,i] = intensity\n",
    "    return pc\n",
    "\n",
    "def visualizeRawPointCloud(cloud, jupyter = False):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    if cloud.shape[1] == 4:\n",
    "        colors = mapIntensityToRGB(cloud[:, 3])\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "\n",
    "    if jupyter:\n",
    "        self.__visualizeJupyter(pcd)\n",
    "    else:\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "        \n",
    "def mapIntensityToRGB(i):\n",
    "    return cm.jet(plt.Normalize(min(i), max(i))(i))\n",
    "\n",
    "def writeRawPointCloud(cloud, filename):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    colors = mapIntensityToRGB(cloud[:, 3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.io.write_point_cloud(filename, pcd)\n",
    "\n",
    "    \n",
    "def construct_transformation_matrix(R, t):\n",
    "    T = np.eye(4)\n",
    "    T[0:3, 0:3] = R\n",
    "    T[0:3, 3] = t\n",
    "    return T\n",
    "\n",
    "def get_global_lidar_pose_from_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    \n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    R = Quaternion(cs_record['rotation']).rotation_matrix\n",
    "    t = np.array(cs_record['translation'])\n",
    "    T_E_L = construct_transformation_matrix(R, t)\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    R = Quaternion(poserecord['rotation']).rotation_matrix\n",
    "    t = np.array(poserecord['translation'])\n",
    "    T_G_E = construct_transformation_matrix(R, t)\n",
    "         \n",
    "    # T_G_L\n",
    "    return np.matmul(T_G_E, T_E_L), sample['next']\n",
    "\n",
    "def visualize_poses(Ts):\n",
    "    n_poses = len(Ts)\n",
    "    xy = np.empty((2, n_poses))\n",
    "    for i in range(0, n_poses):\n",
    "        xy[0:2, i] = Ts[i][0:2, 3]\n",
    "    \n",
    "    plt.scatter(xy[0,:], xy[1,:])\n",
    "    plt.show()\n",
    "    \n",
    "class SamplingPointCloud(PointCloud):\n",
    "\n",
    "    @staticmethod\n",
    "    def nbr_dims() -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of dimensions.\n",
    "        :return: Number of dimensions.\n",
    "        \"\"\"\n",
    "        return 4\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_name: str) -> 'SamplingPointCloud':\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_bw(cls, bw, scale = 100) -> 'SamplingPointCloud':\n",
    "        grid = createGrid_old(bw)\n",
    "        xyz_grid = convertGridToEuclidean_old(grid) * scale\n",
    "        intensities = np.zeros((xyz_grid.shape[0],1))\n",
    "        sampling_grid = np.hstack((xyz_grid, np.ones((xyz_grid.shape[0], 1), dtype=xyz_grid.dtype)))\n",
    "        return cls(sampling_grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "# Convert dataset to PLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "# export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "# export_images = export_ds + '/SPH_IMAGES/'\n",
    "# export_clouds = export_ds + '/SPH_CLOUDS/'\n",
    "poses_out = export_ds + '/poses.npy'\n",
    "\n",
    "bw = 100\n",
    "scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_scenes = 10\n",
      "chunk 1 [0, 3]\n",
      "chunk 2 [3, 6]\n",
      "chunk 3 [6, 10]\n"
     ]
    }
   ],
   "source": [
    "n_scenes = len(nusc.scene)\n",
    "per_chunk = int(n_scenes / 3)\n",
    "\n",
    "chunk1_start = 0\n",
    "chunk1_end = per_chunk\n",
    "chunk2_start = chunk1_end\n",
    "chunk2_end = chunk2_start + per_chunk\n",
    "chunk3_start = chunk2_end\n",
    "chunk3_end = n_scenes\n",
    "\n",
    "print(f\"n_scenes = {n_scenes}\")\n",
    "print(f\"chunk 1 [{chunk1_start}, {chunk1_end}]\")\n",
    "print(f\"chunk 2 [{chunk2_start}, {chunk2_end}]\")\n",
    "print(f\"chunk 3 [{chunk3_start}, {chunk3_end}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the global LiDAR poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenes from 0 to 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b35f3cad4849139c8c52807a1cd8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Found 120 poses.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poses extraction complete.\n"
     ]
    }
   ],
   "source": [
    "start = chunk1_start\n",
    "end = chunk1_end\n",
    "\n",
    "print(f\"Processing scenes from {start} to {end}.\")\n",
    "\n",
    "all_poses = []\n",
    "for i in tqdm(range(start, end)):\n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    pose_first, next_sample_token = get_global_lidar_pose_from_token(first_sample_token)\n",
    "    all_poses.append(pose_first)\n",
    "    \n",
    "    while next_sample_token != '':    \n",
    "        pose_next, next_sample_token = get_global_lidar_pose_from_token(next_sample_token)    \n",
    "        all_poses.append(pose_next)        \n",
    "    \n",
    "print(f\"Loading complete. Found {len(all_poses)} poses.\")    \n",
    "visualize_poses(all_poses)\n",
    "np.save(poses_out, all_poses)\n",
    "print(f\"Poses extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the images to spherical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progresser_images(sample, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample_sphere = ImageSphere(sample)\n",
    "    return sample_sphere.sampleUsingGrid(grid)\n",
    "\n",
    "def get_sampled_image_sphere_from_token(sample_token):\n",
    "    sample = nusc.get('sample', first_sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    pc = SamplingPointCloud.from_bw(bw, scale)\n",
    "    for cam_str in all_cam_strings:          \n",
    "        cam = nusc.get('sample_data', sample['data'][cam_str])\n",
    "        im = Image.open(osp.join(dataset_path, cam['filename']))    \n",
    "\n",
    "        # Transform pointcloud into the camera frame\n",
    "        pc = transform_from_pcl_to_cam(nusc, pointsensor, cam, pc)\n",
    "        # Grab the depths (camera frame z axis points away from the camera).\n",
    "        depths = pc.points[2, :]\n",
    "\n",
    "        # Project the points onto the image plane\n",
    "        cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "        intrinsics = np.array(cs_record['camera_intrinsic'])\n",
    "        points, mask = project_pc_on_cam(pc, intrinsics, depths)\n",
    "        filtered_points = points[:, mask]\n",
    "\n",
    "        pc = sample_mono_from_image(im, pc, mask) # Sample the intensity values from the image\n",
    "        pc = transform_from_cam_to_pcl(nusc, pointsensor, cam, pc) # Transform back into the LiDAR frame\n",
    "    return pc.points.T / scale, sample['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenes from 0 to 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdacf7744aa24775b490353688277960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "image index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9dc1720e8592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mscene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnusc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfirst_sample_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first_sample_token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_sample_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sampled_image_sphere_from_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_sample_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mall_sph_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-6f97124c39c0>\u001b[0m in \u001b[0;36mget_sampled_image_sphere_from_token\u001b[0;34m(sample_token)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mfiltered_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_mono_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Sample the intensity values from the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_from_cam_to_pcl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnusc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointsensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Transform back into the LiDAR frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-cf747d8fd0e3>\u001b[0m in \u001b[0;36msample_mono_from_image\u001b[0;34m(im, pc, mask)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mcur_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mpx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mintensity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb_to_greyscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintensity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mgetpixel\u001b[0;34m(self, xy)\u001b[0m\n\u001b[1;32m   1367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyaccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyaccess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpixel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetprojection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: image index out of range"
     ]
    }
   ],
   "source": [
    "start = chunk1_start\n",
    "end = chunk1_end\n",
    "\n",
    "print(f\"Processing scenes from {start} to {end}.\")\n",
    "\n",
    "\n",
    "all_sph_images = []\n",
    "for i in tqdm(range(start, end)):\n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    pc, next_sample_token = get_sampled_image_sphere_from_token(first_sample_token)        \n",
    "    all_sph_images.append(pc)\n",
    "    \n",
    "    while next_sample_token != '':    \n",
    "        pc, next_sample_token = get_sampled_image_sphere_from_token(next_sample_token)    \n",
    "        all_sph_images.append(pc)  \n",
    "    \n",
    "print(f\"Loading complete. Computing features...\")    \n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "img_features = process_map(partial(progresser_images, grid=grid), all_sph_images, max_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the pointclouds to spherical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progresser(sample, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample_sphere = Sphere(sample)\n",
    "    return sample_sphere.sampleUsingGrid(grid)\n",
    "\n",
    "def writeRawPointCloud(cloud, filename):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    colors = mapIntensityToRGB(cloud[:, 3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.io.write_point_cloud(filename, pcd)\n",
    "    \n",
    "def get_data_from_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    # Load a single lidar point cloud.\n",
    "    pcl_path = osp.join(dataset_path, pointsensor['filename'])\n",
    "    pc = LidarPointCloud.from_file(pcl_path)    \n",
    "    return pc.points.T, sample['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_point_cloud_from_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    # Load a single lidar point cloud.\n",
    "    pcl_path = osp.join(dataset_path, pointsensor['filename'])\n",
    "    pc = LidarPointCloud.from_file(pcl_path)\n",
    "    points_xyzi = pc.points.T\n",
    "    \n",
    "    # Load the semantic segmentation.\n",
    "    sample_data_token = sample['data'][lidar_string]\n",
    "    lidarseg_labels_filename = osp.join(nusc.dataroot, nusc.get('lidarseg', sample_data_token)['filename'])\n",
    "    points_label = np.fromfile(lidarseg_labels_filename, dtype=np.uint8)  # [num_points]\n",
    "\n",
    "    # Combine the two.\n",
    "    return np.column_stack((points_xyzi, points_label)), sample['next']\n",
    "\n",
    "def combine_every_nth_point_cloud(point_clouds, n=3):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenes from 566 to 850.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91784988de9a4d24a307ea6ac3a2cf5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=284.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020f1b93b90948279782f86ac04d391f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11467.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote features to /mnt/data/datasets/nuscenes/processed/sem_clouds3.npy\n"
     ]
    }
   ],
   "source": [
    "start = chunk1_start\n",
    "end = chunk1_end\n",
    "\n",
    "print(f\"Processing scenes from {start} to {end}.\")\n",
    "\n",
    "all_sem_clouds = []\n",
    "for i in tqdm(range(start, end)):\n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    pc, next_sample_token = get_semantic_point_cloud_from_token(first_sample_token)\n",
    "    all_sem_clouds.append(pc)\n",
    "    \n",
    "    while next_sample_token != '':    \n",
    "        pc, next_sample_token = get_semantic_point_cloud_from_token(next_sample_token)    \n",
    "        all_sem_clouds.append(pc)   \n",
    "    \n",
    "print(f\"Loading complete. Computing features...\")    \n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "sem_features = process_map(partial(progresser, grid=grid), all_sem_clouds, max_workers=8, chunksize=100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling pointcloud shape is (40000, 3)\n"
     ]
    }
   ],
   "source": [
    "# n_clouds = len(pcl_features)\n",
    "# n_images = len(img_features)\n",
    "# assert n_clouds > 0\n",
    "# assert n_clouds == n_images\n",
    "\n",
    "i = 15\n",
    "# cur_cloud = pcl_features[i]\n",
    "cur_sem_cloud = sem_features[i]\n",
    "# cur_image = img_features[i]\n",
    "# print(f\"current spherical cloud shape: {cur_cloud.shape} and current spherical image shape {cur_image.shape}\")\n",
    "# cur_cloud = np.reshape(cur_cloud, (2, -1)).T\n",
    "cur_sem_cloud = np.reshape(cur_sem_cloud, (2, -1)).T\n",
    "# cur_image = np.reshape(cur_image, (1, -1)).T\n",
    "# print(f\"current reshaped cloud shape {cur_cloud.shape} and current reshaped image shape {cur_image.shape}\")\n",
    "\n",
    "pc = SamplingPointCloud.from_bw(bw, 1)\n",
    "points_xyz = pc.points.T[:,0:3]\n",
    "print(f\"sampling pointcloud shape is {points_xyz.shape}\")\n",
    "# points_xyzi = np.column_stack((points_xyz, cur_cloud[:,1]))\n",
    "# points_xyzp = np.column_stack((points_xyz, cur_image))\n",
    "points_xyzl = np.column_stack((points_xyz, cur_sem_cloud[:,1]))\n",
    "\n",
    "visualizeRawPointCloud(points_xyzl)\n",
    "#visualizeRawPointCloud(points_xyzp)\n",
    "#visualizeRawPointCloud(points_xyzl)\n",
    "\n",
    "#print(f\"reshaped cloud shape is {cloud.shape}\")\n",
    "#visualizeRawPointCloud(cloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
