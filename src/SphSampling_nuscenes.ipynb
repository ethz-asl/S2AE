{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPH Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from pyquaternion.quaternion import Quaternion\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from nuscenes.utils.data_classes import PointCloud, LidarPointCloud, RadarPointCloud, Box\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import os.path as osp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "from matplotlib import cm\n",
    "from sphere import Sphere\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "from functools import partial\n",
    "from dh_grid import DHGrid\n",
    "from sphere import Sphere\n",
    "from img_sphere import ImageSphere, ColorImageSphere\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "from utils import Utils\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "404 lidarseg,\n",
      "Done loading in 0.358 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# dataset_path = '/media/scratch/berlukas/nuscenes'\n",
    "dataset_path = '/mnt/data/datasets/nuscenes/v1.0-mini/'\n",
    "# dataset_path = '/media/berlukas/T7 Touch/data/nuscenes'\n",
    "# nusc = NuScenes(version='v1.0-trainval', dataroot=dataset_path, verbose=True)\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=dataset_path, verbose=True)\n",
    "all_cam_strings = ['CAM_FRONT','CAM_FRONT_RIGHT','CAM_BACK_RIGHT','CAM_BACK','CAM_BACK_LEFT','CAM_FRONT_LEFT']\n",
    "#all_cam_strings = ['CAM_FRONT_LEFT','CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT']\n",
    "lidar_string = 'LIDAR_TOP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGrid(bw):\n",
    "    n_grid = 2 * bw\n",
    "    k = 0;\n",
    "    points = np.empty([2, n_grid, n_grid])\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            points[0, i, j] = (np.pi*(2*i+1))/(4*bw)\n",
    "            points[1, i, j] = (2*np.pi*j)/(2*bw);\n",
    "            k = k + 1;\n",
    "    return points\n",
    "\n",
    "def createGrid_old(bw):\n",
    "        n_grid = 2 * bw\n",
    "        k = 0;\n",
    "        points = np.empty([n_grid * n_grid, 2])\n",
    "        for i in range(n_grid):\n",
    "            for j in range(n_grid):\n",
    "                points[k, 0] = (np.pi*(2*i+1))/(4*bw)\n",
    "                points[k, 1] = (2*np.pi*j)/(2*bw);\n",
    "                k = k + 1;\n",
    "        return points\n",
    "\n",
    "def ConvertGridToEuclidean(grid):\n",
    "    cart_grid = np.zeros([3, grid.shape[1], grid.shape[2]])\n",
    "    cart_grid[0,:,:] = np.multiply(np.sin(grid[0, :,:]), np.cos(grid[1,:,:]))\n",
    "    cart_grid[1,:,:] = np.multiply(np.sin(grid[0, :, :]), np.sin(grid[1, :, :]))\n",
    "    cart_grid[2,:,:] = np.cos(grid[0, :, :])    \n",
    "    return cart_grid\n",
    "\n",
    "def convertGridToEuclidean_old(grid):\n",
    "    cart_grid = np.zeros([ grid.shape[0], 3])\n",
    "    cart_grid[:,0] = np.multiply(np.sin(grid[:,0]), np.cos(grid[:,1]))\n",
    "    cart_grid[:,1] = np.multiply(np.sin(grid[:,0]), np.sin(grid[:,1]))\n",
    "    cart_grid[:,2] = np.cos(grid[:,0])    \n",
    "    return cart_grid\n",
    "\n",
    "def rgb_to_greyscale(r,g,b):\n",
    "    return 0.2126*r + 0.7152*g + 0.0722*b\n",
    "#     return 0.3*r + 0.59*g + 0.11*b    \n",
    "\n",
    "def transform_from_pcl_to_cam(nusc, pointsensor, cam, pc):\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def transform_from_cam_to_pcl(nusc, pointsensor, cam, pc):\n",
    "    # Transform from the camera into the vehicle's ego frame\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])    \n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "    \n",
    "    # Transform from the ego frame (cam) to the global frame\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "        \n",
    "    # Transform from the global frame to the ego frame of the LiDAR.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])    \n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "        \n",
    "    # Transform from the ego frame (LiDAR) to the LiDAR frame\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])    \n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def project_pc_on_cam(pc, cam_intrinsics, depths, im):\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    points = view_points(pc.points[:3, :], cam_intrinsics, normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "    # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "    # casing for non-keyframes which are slightly out of sync.\n",
    "    min_dist = 0.0001\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths < -1.0)\n",
    "    mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "    mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "    return points, mask\n",
    "\n",
    "def sample_mono_from_image(im, pc, mask, points):\n",
    "    n_mask = len(mask)\n",
    "        \n",
    "    for i in range(0,n_mask):    \n",
    "        visible = mask[i]\n",
    "        if not visible:\n",
    "            continue\n",
    "        cur_point = (points[0,i], points[1,i])    \n",
    "        px = im.getpixel(cur_point)    \n",
    "        intensity = rgb_to_greyscale(px[0], px[1], px[2])\n",
    "        pc.points[3,i] = intensity\n",
    "    return pc\n",
    "\n",
    "def sample_color_from_image(im, pc, mask, points):\n",
    "    n_mask = len(mask)    \n",
    "    \n",
    "    for i in range(0,n_mask):    \n",
    "        visible = mask[i]\n",
    "        if not visible:\n",
    "            continue\n",
    "        cur_point = (points[0,i], points[1,i])            \n",
    "        pc.points[3:6,i] = im.getpixel(cur_point)\n",
    "    return pc\n",
    "\n",
    "def visualizeRawPointCloud(cloud, jupyter = False):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    if cloud.shape[1] == 4:\n",
    "        colors = mapIntensityToRGB(cloud[:, 3])\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    if cloud.shape[1] == 6:\n",
    "        pcd.colors = o3d.utility.Vector3dVector(cloud[:,3:6])\n",
    "\n",
    "    if jupyter:\n",
    "        self.__visualizeJupyter(pcd)\n",
    "    else:\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "        \n",
    "def mapIntensityToRGB(i):\n",
    "    return cm.jet(plt.Normalize(min(i), max(i))(i))\n",
    "\n",
    "def writeRawPointCloud(cloud, filename):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    colors = mapIntensityToRGB(cloud[:, 3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.io.write_point_cloud(filename, pcd)\n",
    "\n",
    "    \n",
    "def construct_transformation_matrix(R, t):\n",
    "    T = np.eye(4)\n",
    "    T[0:3, 0:3] = R\n",
    "    T[0:3, 3] = t\n",
    "    return T\n",
    "  \n",
    "class SamplingPointCloud(PointCloud):\n",
    "\n",
    "    @staticmethod\n",
    "    def nbr_dims() -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of dimensions.\n",
    "        :return: Number of dimensions.\n",
    "        \"\"\"\n",
    "        return 4\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_name: str) -> 'SamplingPointCloud':\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_bw(cls, bw, scale = 100) -> 'SamplingPointCloud':\n",
    "        grid = createGrid_old(bw)\n",
    "        xyz_grid = convertGridToEuclidean_old(grid) * scale\n",
    "        intensities = np.zeros((xyz_grid.shape[0],1), dtype=xyz_grid.dtype)\n",
    "        sampling_grid = np.hstack((xyz_grid, intensities))\n",
    "        return cls(sampling_grid.T)\n",
    "    \n",
    "class ColorSamplingPointCloud(PointCloud):\n",
    "\n",
    "    @staticmethod\n",
    "    def nbr_dims() -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of dimensions.\n",
    "        :return: Number of dimensions.\n",
    "        \"\"\"\n",
    "        return 6\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_name: str) -> 'ColorSamplingPointCloud':\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_bw(cls, bw, scale = 100) -> 'ColorSamplingPointCloud':\n",
    "        grid = createGrid_old(bw)\n",
    "        xyz_grid = convertGridToEuclidean_old(grid) * scale\n",
    "        rgb = np.zeros((xyz_grid.shape[0], 3), dtype=xyz_grid.dtype)\n",
    "        sampling_grid = np.hstack((xyz_grid, rgb))\n",
    "        return cls(sampling_grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "# Convert dataset to PLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "# export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "# export_images = export_ds + '/SPH_IMAGES/'\n",
    "# export_clouds = export_ds + '/SPH_CLOUDS/'\n",
    "poses_out = export_ds + '/poses.npy'\n",
    "\n",
    "bw = 200\n",
    "scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_scenes = 10\n",
      "chunk 1 [0, 1]\n",
      "chunk 2 [1, 4]\n",
      "chunk 3 [4, 10]\n"
     ]
    }
   ],
   "source": [
    "n_scenes = len(nusc.scene)\n",
    "per_chunk = int(n_scenes / 3)\n",
    "\n",
    "chunk1_start = 0\n",
    "# chunk1_end = per_chunk\n",
    "chunk1_end = 1\n",
    "chunk2_start = chunk1_end\n",
    "chunk2_end = chunk2_start + per_chunk\n",
    "chunk3_start = chunk2_end\n",
    "chunk3_end = n_scenes\n",
    "\n",
    "print(f\"n_scenes = {n_scenes}\")\n",
    "print(f\"chunk 1 [{chunk1_start}, {chunk1_end}]\")\n",
    "print(f\"chunk 2 [{chunk2_start}, {chunk2_end}]\")\n",
    "print(f\"chunk 3 [{chunk3_start}, {chunk3_end}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the global LiDAR poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_lidar_pose_from_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    \n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    R = Quaternion(cs_record['rotation']).rotation_matrix\n",
    "    t = np.array(cs_record['translation'])\n",
    "    T_E_L = construct_transformation_matrix(R, t)\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    R = Quaternion(poserecord['rotation']).rotation_matrix\n",
    "    t = np.array(poserecord['translation'])\n",
    "    T_G_E = construct_transformation_matrix(R, t)\n",
    "         \n",
    "    # T_G_L\n",
    "    return np.matmul(T_G_E, T_E_L), sample['next']\n",
    "\n",
    "def visualize_poses(Ts):\n",
    "    n_poses = len(Ts)\n",
    "    xy = np.empty((2, n_poses))\n",
    "    for i in range(0, n_poses):\n",
    "        xy[0:2, i] = Ts[i][0:2, 3]\n",
    "    \n",
    "    plt.scatter(xy[0,:], xy[1,:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenes from 0 to 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194c4a24b8224a52bbea416d82289489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Found 39 poses.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poses extraction complete.\n"
     ]
    }
   ],
   "source": [
    "start = chunk1_start\n",
    "end = chunk1_end\n",
    "\n",
    "print(f\"Processing scenes from {start} to {end}.\")\n",
    "\n",
    "all_poses = []\n",
    "all_scene_poses = [None] * n_scenes\n",
    "for i in tqdm(range(start, end)):\n",
    "    scene_poses = []\n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    pose_first, next_sample_token = get_global_lidar_pose_from_token(first_sample_token)\n",
    "    all_poses.append(pose_first)\n",
    "    scene_poses.append(pose_first)\n",
    "    \n",
    "    while next_sample_token != '':    \n",
    "        pose_next, next_sample_token = get_global_lidar_pose_from_token(next_sample_token)    \n",
    "        all_poses.append(pose_next)\n",
    "        scene_poses.append(pose_next)\n",
    "    all_scene_poses[i] = scene_poses    \n",
    "    \n",
    "print(f\"Loading complete. Found {len(all_poses)} poses.\")    \n",
    "visualize_poses(all_poses)\n",
    "np.save(poses_out, all_poses)\n",
    "print(f\"Poses extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the images to spherical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progresser_images(sample, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample_sphere = ImageSphere(sample)\n",
    "    return sample_sphere.sampleUsingGrid(grid)\n",
    "\n",
    "def progresser_color_images(sample, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample_sphere = ColorImageSphere(sample)\n",
    "    return sample_sphere.sampleUsingGrid(grid)\n",
    "\n",
    "def get_sampled_image_sphere_from_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    pc = SamplingPointCloud.from_bw(bw, scale)\n",
    "\n",
    "    for cam_str in all_cam_strings:          \n",
    "        cam = nusc.get('sample_data', sample['data'][cam_str])\n",
    "        im = Image.open(osp.join(dataset_path, cam['filename']))    \n",
    "\n",
    "        # Transform pointcloud into the camera frame\n",
    "        pc = transform_from_pcl_to_cam(nusc, pointsensor, cam, pc)\n",
    "        # Grab the depths (camera frame z axis points away from the camera).\n",
    "        depths = pc.points[2, :]\n",
    "\n",
    "        # Project the points onto the image plane\n",
    "        cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "        intrinsics = np.array(cs_record['camera_intrinsic'])\n",
    "        points, mask = project_pc_on_cam(pc, intrinsics, depths, im)\n",
    "        filtered_points = points[:, mask]\n",
    "        \n",
    "        # Sample the intensity values from the image\n",
    "        # and transform back into the LiDAR frame.\n",
    "        pc = sample_mono_from_image(im, pc, mask, points) \n",
    "        pc = transform_from_cam_to_pcl(nusc, pointsensor, cam, pc) \n",
    "    \n",
    "    # Scale back to the original value but only use xyz.\n",
    "    cloud = pc.points.T\n",
    "    cloud[:,0:3] = cloud[:,0:3] / scale\n",
    "    \n",
    "    return cloud / scale, sample['next']\n",
    "\n",
    "def get_sampled_color_image_sphere_from_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    pc = ColorSamplingPointCloud.from_bw(bw, scale)\n",
    "\n",
    "    for cam_str in all_cam_strings:          \n",
    "        cam = nusc.get('sample_data', sample['data'][cam_str])\n",
    "        im = Image.open(osp.join(dataset_path, cam['filename']))    \n",
    "\n",
    "        # Transform pointcloud into the camera frame\n",
    "        pc = transform_from_pcl_to_cam(nusc, pointsensor, cam, pc)\n",
    "        # Grab the depths (camera frame z axis points away from the camera).\n",
    "        depths = pc.points[2, :]\n",
    "\n",
    "        # Project the points onto the image plane\n",
    "        cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "        intrinsics = np.array(cs_record['camera_intrinsic'])\n",
    "        points, mask = project_pc_on_cam(pc, intrinsics, depths, im)\n",
    "        filtered_points = points[:, mask]\n",
    "        \n",
    "        # Sample the intensity values from the image\n",
    "        # and transform back into the LiDAR frame.\n",
    "        pc = sample_color_from_image(im, pc, mask, points) \n",
    "        pc = transform_from_cam_to_pcl(nusc, pointsensor, cam, pc) \n",
    "    \n",
    "    # Scale back to the original value but only use xyz.\n",
    "    cloud = pc.points.T\n",
    "    cloud[:,0:3] = cloud[:,0:3] / scale\n",
    "    return cloud, sample['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenes from 0 to 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c092570df5fe463fbeaa03ecdec5772b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Found 1 images.\n",
      "Computing features now...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95909c4196c942728fb9f74abe617057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = chunk1_start\n",
    "end = chunk1_end\n",
    "\n",
    "print(f\"Processing scenes from {start} to {end}.\")\n",
    "\n",
    "all_sph_images = []\n",
    "for i in tqdm(range(start, end)):    \n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    pc, next_sample_token = get_sampled_image_sphere_from_token(first_sample_token)        \n",
    "    all_sph_images.append(pc)    \n",
    "    \n",
    "#     while next_sample_token != '':    \n",
    "#         pc, next_sample_token = get_sampled_image_sphere_from_token(next_sample_token)    \n",
    "#         all_sph_images.append(pc)\n",
    "    \n",
    "print(f\"Loading complete. Found {len(all_sph_images)} images.\")\n",
    "print(f\"Computing features now...\")    \n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "img_features = process_map(partial(progresser_images, grid=grid), all_sph_images, max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenes from 0 to 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b416012214a409296a798ccb99e7fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Found 1 images.\n",
      "Computing features now...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb550824830242359da94b745e9a1f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = chunk1_start\n",
    "end = chunk1_end\n",
    "\n",
    "print(f\"Processing scenes from {start} to {end}.\")\n",
    "\n",
    "all_color_sph_images = []\n",
    "for i in tqdm(range(start, end)):    \n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    pc, next_sample_token = get_sampled_color_image_sphere_from_token(first_sample_token)        \n",
    "    all_color_sph_images.append(pc)    \n",
    "    \n",
    "#     while next_sample_token != '':    \n",
    "#         pc, next_sample_token = get_sampled_color_image_sphere_from_token(next_sample_token)    \n",
    "#         all_sph_images.append(pc)\n",
    "    \n",
    "print(f\"Loading complete. Found {len(all_sph_images)} images.\")\n",
    "print(f\"Computing features now...\")    \n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "color_img_features = process_map(partial(progresser_color_images, grid=grid), all_color_sph_images, max_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the pointclouds to spherical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progresser_lidar(sample, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample_sphere = Sphere(sample)\n",
    "    return sample_sphere.sampleUsingGrid(grid)\n",
    "\n",
    "def writeRawPointCloud(cloud, filename):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    colors = mapIntensityToRGB(cloud[:, 3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.io.write_point_cloud(filename, pcd)\n",
    "    \n",
    "def get_data_from_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    # Load a single lidar point cloud.\n",
    "    pcl_path = osp.join(dataset_path, pointsensor['filename'])\n",
    "    pc = LidarPointCloud.from_file(pcl_path)    \n",
    "    return pc.points.T, sample['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_point_cloud_from_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    # Load a single lidar point cloud.\n",
    "    pcl_path = osp.join(dataset_path, pointsensor['filename'])\n",
    "    pc = LidarPointCloud.from_file(pcl_path)\n",
    "    points_xyzi = pc.points.T\n",
    "    \n",
    "    # Load the semantic segmentation.\n",
    "    sample_data_token = sample['data'][lidar_string]\n",
    "    lidarseg_labels_filename = osp.join(nusc.dataroot, nusc.get('lidarseg', sample_data_token)['filename'])\n",
    "    points_label = np.fromfile(lidarseg_labels_filename, dtype=np.uint8)  # [num_points]\n",
    "\n",
    "    # Combine the two.\n",
    "    return np.column_stack((points_xyzi, points_label)), sample['next']\n",
    "\n",
    "def combine_pointclouds(pointclouds, poses):\n",
    "    n_data = len(poses)    \n",
    "    pivot = n_data // 2  \n",
    "    T_G_L_pivot = poses[pivot]\n",
    "    T_L_pivot_G = np.linalg.inv(T_G_L_pivot)\n",
    "\n",
    "    acc_points = pointclouds[pivot]\n",
    "    for i in range(0, n_data):\n",
    "        if i == pivot:\n",
    "            continue\n",
    "        T_G_L = poses[i]\n",
    "        T_L_pivot_L = T_L_pivot_G @ T_G_L\n",
    "\n",
    "        points = Utils.transform_pointcloud(pointclouds[i], T_L_pivot_L)\n",
    "        acc_points = np.append(acc_points, points, axis=0)\n",
    "                        \n",
    "    return acc_points, pivot\n",
    "\n",
    "def combine_every_nth_point_cloud(pointclouds, poses, n=3):\n",
    "    n_poses = len(poses)    \n",
    "    \n",
    "    # Generate combinations.\n",
    "    combined = []\n",
    "    pivot_indices = []\n",
    "    for i in range(0, n_poses, 1):\n",
    "        if i+3 >= n_poses:\n",
    "            break\n",
    "            \n",
    "        indices = np.arange(i, i+3, 1)        \n",
    "        local_clouds = []\n",
    "        local_poses = []\n",
    "        for idx in indices:        \n",
    "            local_clouds.append(pointclouds[idx])\n",
    "            local_poses.append(poses[idx])\n",
    "        acc_points, pivot = combine_pointclouds(local_clouds, local_poses)\n",
    "        combined.append(acc_points)\n",
    "        pivot_indices.append(indices[pivot])\n",
    "    return combined, pivot_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenes from 0 to 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1445113c711e4191984cffe2ff77e678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Found 39 clouds.\n",
      "Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb2847cf8ff4b458ba72c4073b2f118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = chunk1_start\n",
    "end = chunk1_end\n",
    "\n",
    "print(f\"Processing scenes from {start} to {end}.\")\n",
    "\n",
    "all_sem_clouds = []\n",
    "for i in tqdm(range(start, end)):\n",
    "    scene_clouds = []\n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    pc, next_sample_token = get_semantic_point_cloud_from_token(first_sample_token)\n",
    "    all_sem_clouds.append(pc)\n",
    "    \n",
    "    while next_sample_token != '':    \n",
    "        pc, next_sample_token = get_semantic_point_cloud_from_token(next_sample_token)    \n",
    "        all_sem_clouds.append(pc)   \n",
    "\n",
    "# This is the code for combining/accumulating multiple clouds to one pointcloud\n",
    "# all_pivots = []\n",
    "# for i in tqdm(range(start, end)):\n",
    "#     scene_clouds = []\n",
    "#     scene = nusc.scene[i]\n",
    "#     first_sample_token = scene['first_sample_token']\n",
    "#     pc, next_sample_token = get_semantic_point_cloud_from_token(first_sample_token)\n",
    "#     scene_clouds.append(pc)\n",
    "    \n",
    "#     while next_sample_token != '':    \n",
    "#         pc, next_sample_token = get_semantic_point_cloud_from_token(next_sample_token)    \n",
    "#         scene_clouds.append(pc)   \n",
    "    \n",
    "#     combined_clouds, pivot_indices = combine_every_nth_point_cloud(scene_clouds, all_scene_poses[i], 3)\n",
    "#     all_sem_clouds.append(combined_clouds)\n",
    "#     all_pivots.append(pivot_indices)\n",
    "\n",
    "print(f\"Loading complete. Found {len(all_sem_clouds)} clouds.\")    \n",
    "print(f\"Computing features...\")    \n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "sem_features = process_map(partial(progresser_lidar, grid=grid), all_sem_clouds, max_workers=8)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling pointcloud shape is (160000, 3)\n"
     ]
    }
   ],
   "source": [
    "# n_clouds = len(pcl_features)\n",
    "# n_images = len(img_features)\n",
    "# assert n_clouds > 0\n",
    "# assert n_clouds == n_images\n",
    "\n",
    "i = 0\n",
    "# cur_cloud = pcl_features[i]\n",
    "# cur_sem_cloud = sem_features[i]\n",
    "cur_image = img_features[i]\n",
    "cur_color_image = img_features[i]\n",
    "\n",
    "# print(f\"current spherical cloud shape: {cur_cloud.shape} and current spherical image shape {cur_image.shape}\")\n",
    "# cur_cloud = np.reshape(cur_cloud, (2, -1)).T\n",
    "# cur_sem_cloud = np.reshape(cur_sem_cloud, (3, -1)).T\n",
    "cur_image = np.reshape(cur_image, (1, -1)).T\n",
    "cur_color_image = np.reshape(cur_color_image, (3, -1)).T\n",
    "\n",
    "\n",
    "pc = SamplingPointCloud.from_bw(bw, 1)\n",
    "points_xyz = pc.points.T[:,0:3]\n",
    "print(f\"sampling pointcloud shape is {points_xyz.shape}\")\n",
    "# points_xyzi = np.column_stack((points_xyz, cur_cloud[:,1]))\n",
    "points_xyzp = np.column_stack((points_xyz, cur_image))\n",
    "# points_xyzl = np.column_stack((points_xyz, cur_sem_cloud[:,1]))\n",
    "points_xyzrgb = np.column_stack((points_xyz, cur_color_image[:, 0:3]))\n",
    "\n",
    "# visualizeRawPointCloud(points_xyzl)\n",
    "visualizeRawPointCloud(points_xyzp)\n",
    "# visualizeRawPointCloud(points_xyzl)\n",
    "\n",
    "# print(f\"reshaped cloud shape is {cloud.shape}\")\n",
    "# visualizeRawPointCloud(cloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
