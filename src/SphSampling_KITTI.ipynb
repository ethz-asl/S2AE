{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sampled Dataset of KITTI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "from matplotlib import cm\n",
    "from functools import partial\n",
    "\n",
    "from sphere import Sphere\n",
    "from dh_grid import DHGrid\n",
    "from laserscan import SemLaserScan\n",
    "from visualize import Visualize\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting dataroot to /media/berlukas/Data/data/datasets/KITTI/sequences.\n",
      "Configured 1 sequences.\n",
      "Configured config file ../config/semantic-kitti.yaml\n"
     ]
    }
   ],
   "source": [
    "def load_sequence(dataroot, sequence):\n",
    "    scan_paths = f'{dataroot}/{sequence}/velodyne'\n",
    "    scan_names = [os.path.join(dp, f) for dp, dn, fn in os.walk(\n",
    "      os.path.expanduser(scan_paths)) for f in fn]\n",
    "    scan_names.sort()\n",
    "    \n",
    "    label_paths = f'{dataroot}/{sequence}/labels'\n",
    "    label_names = [os.path.join(dp, f) for dp, dn, fn in os.walk(\n",
    "        os.path.expanduser(label_paths)) for f in fn]\n",
    "    label_names.sort()    \n",
    "    assert len(label_names) == len(scan_names)\n",
    "    print(f'Found {len(scan_names)} pointclouds and labels for sequence {sequence}.')\n",
    "    return scan_names, label_names\n",
    "\n",
    "def load_config_file(config_file):    \n",
    "    try:        \n",
    "        CFG = yaml.safe_load(open(config_file, 'r'))        \n",
    "        return CFG\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "        return None\n",
    "\n",
    "# dataroot = '/mnt/data/datasets/KITTI/sequences'\n",
    "dataroot = '/media/berlukas/Data/data/datasets/KITTI/sequences'\n",
    "sequences = ['04']\n",
    "config_file = '../config/semantic-kitti.yaml'\n",
    "\n",
    "print(f'Setting dataroot to {dataroot}.')\n",
    "print(f'Configured {len(sequences)} sequences.')\n",
    "print(f'Configured config file {config_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequence 04.\n",
      "Found 271 pointclouds and labels for sequence 04.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce823a8ced7e4de888710b4e6deb4970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfe9c5c49b540a68b7fbb5986efb094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has semantics.\n",
      "Input data has semantics.\n",
      "Input data has semantics.\n",
      "Input data has semantics.\n",
      "Input data has semantics.\n",
      "\n",
      "Wrote features to /media/berlukas/Data/data/datasets/KITTI/processed/clouds.npy.\n"
     ]
    }
   ],
   "source": [
    "def progresser(sample, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample_sphere = Sphere(sample)\n",
    "    return sample_sphere.sampleUsingGrid(grid)\n",
    "\n",
    "CFG = load_config_file(config_file)\n",
    "color_dict = CFG[\"color_map\"]\n",
    "nclasses = len(color_dict)\n",
    "scan = SemLaserScan(nclasses, color_dict, project=False)\n",
    "bw = 100\n",
    "assert CFG is not None\n",
    "  \n",
    "sem_features = []\n",
    "export_ds = '/media/berlukas/Data/data/datasets/KITTI/processed'\n",
    "for seq in sequences:\n",
    "    print(f'Loading sequence {seq}.')\n",
    "    scan_names, label_names = load_sequence(dataroot, seq)\n",
    "    n_scans = len(scan_names)\n",
    "    n_scans = 5\n",
    "    all_sem_clouds = []    \n",
    "    for i in tqdm(range(0, n_scans)):\n",
    "        scan.open_scan(scan_names[i])\n",
    "        scan.open_label(label_names[i])\n",
    "        scan.colorize()\n",
    "        \n",
    "        pc = np.column_stack((scan.points, scan.remissions, scan.sem_label, scan.inst_label))\n",
    "        mask = pc[:,4] > 0\n",
    "        all_sem_clouds.append(pc[mask])         \n",
    "        \n",
    "    print(f\"Loading complete. Computing features...\")    \n",
    "    grid, _ = DHGrid.CreateGrid(bw)\n",
    "    \n",
    "    # parallel\n",
    "#     sem_features = process_map(partial(progresser, grid=grid), all_sem_clouds, max_workers=8, chunksize=100)    \n",
    "    \n",
    "    # seq    \n",
    "    for cloud in tqdm(all_sem_clouds):\n",
    "        cur_features = progresser(cloud, grid)\n",
    "        sem_features.append(cur_features)    \n",
    "        \n",
    "filename = f\"{export_ds}/clouds.npy\"\n",
    "np.save(filename, sem_features)\n",
    "print(f\"Wrote features to {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapIntensityToRGB(i):\n",
    "    return cm.jet(plt.Normalize(min(i), max(i))(i))\n",
    "\n",
    "def visualizeRawPointcloud(pcl, val):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pcl[:, 0:3])\n",
    "    colors = mapIntensityToRGB(val)\n",
    "#     colors = scan.sem_color_lut[pcl[:,4].astype(np.int)]\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.visualization.draw_geometries([pcd])    \n",
    "    \n",
    "pc = all_sem_clouds[3]\n",
    "visualizeRawPointcloud(pc, pc[:, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGrid_old(bw):\n",
    "        n_grid = 2 * bw\n",
    "        k = 0;\n",
    "        points = np.empty([n_grid * n_grid, 2])\n",
    "        for i in range(n_grid):\n",
    "            for j in range(n_grid):\n",
    "                points[k, 0] = (np.pi*(2*i+1))/(4*bw)\n",
    "                points[k, 1] = (2*np.pi*j)/(2*bw);\n",
    "                k = k + 1;\n",
    "        return points\n",
    "    \n",
    "def convertGridToEuclidean_old(grid):\n",
    "    cart_grid = np.zeros([ grid.shape[0], 3])\n",
    "    cart_grid[:,0] = np.multiply(np.sin(grid[:,0]), np.cos(grid[:,1]))\n",
    "    cart_grid[:,1] = np.multiply(np.sin(grid[:,0]), np.sin(grid[:,1]))\n",
    "    cart_grid[:,2] = np.cos(grid[:,0])    \n",
    "    return cart_grid\n",
    "\n",
    "def create_sampling_sphere(bw):\n",
    "    grid = createGrid_old(bw)\n",
    "    xyz_grid = convertGridToEuclidean_old(grid)\n",
    "    intensities = np.zeros((xyz_grid.shape[0],1))\n",
    "    sampling_grid = np.hstack((xyz_grid, np.ones((xyz_grid.shape[0], 1), dtype=xyz_grid.dtype)))\n",
    "    return sampling_grid.T\n",
    "\n",
    "cur_sem_cloud = sem_features[0]\n",
    "cur_sem_cloud = np.reshape(cur_sem_cloud, (4, -1)).T\n",
    "print(f'{cur_sem_cloud.shape}')\n",
    "pc = create_sampling_sphere(bw)\n",
    "points_xyz = pc.T[:,0:3]\n",
    "print(f\"sampling pointcloud shape is {points_xyz.shape}\")\n",
    "print(f\"feature shape is {cur_sem_cloud.shape}\")\n",
    "points_xyzl = np.column_stack((points_xyz, cur_sem_cloud[:,2]))\n",
    "# points_xyzn = np.column_stack((points_xyz, cur_sem_cloud[:,2]))\n",
    "\n",
    "\n",
    "visualizeRawPointcloud(points_xyzl, points_xyzl[:, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
