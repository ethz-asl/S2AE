{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sampled Dataset of KITTI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'waymo_open_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwaymo_open_dataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  frame_utils\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwaymo_open_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_pb2 \u001b[38;5;28;01mas\u001b[39;00m open_dataset\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwaymo_open_dataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m segmentation_metrics_pb2\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'waymo_open_dataset'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "from matplotlib import cm\n",
    "from functools import partial\n",
    "import concurrent.futures\n",
    "import random\n",
    "\n",
    "from sphere import Sphere\n",
    "from dh_grid import DHGrid\n",
    "from laserscan import SemLaserScan\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from waymo_open_dataset.utils import  frame_utils\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "from waymo_open_dataset.protos import segmentation_metrics_pb2\n",
    "from waymo_open_dataset.protos import segmentation_submission_pb2\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting dataroot to /media/scratch/berlukas/waymo/segments.\n",
      "Found 25 elements in the folder.\n"
     ]
    }
   ],
   "source": [
    "# dataroot = '/media/scratch/berlukas/waymo/segments'\n",
    "dataroot = '/media/berlukas/SSD_1TB/s2ae/waymo'\n",
    "segments = os.listdir(dataroot)\n",
    "\n",
    "print(f'Setting dataroot to {dataroot}.')\n",
    "print(f'Found {len(segments)} elements in the folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_range_image_to_point_cloud_labels(frame,\n",
    "                                              range_images,\n",
    "                                              segmentation_labels,\n",
    "                                              ri_index=0):\n",
    "    \"\"\"Convert segmentation labels from range images to point clouds.\n",
    "\n",
    "    Args:\n",
    "    frame: open dataset frame\n",
    "    range_images: A dict of {laser_name, [range_image_first_return,\n",
    "       range_image_second_return]}.\n",
    "    segmentation_labels: A dict of {laser_name, [range_image_first_return,\n",
    "       range_image_second_return]}.\n",
    "    ri_index: 0 for the first return, 1 for the second return.\n",
    "\n",
    "    Returns:\n",
    "    point_labels: {[N, 2]} list of 3d lidar points's segmentation labels. 0 for\n",
    "      points that are not labeled.\n",
    "    \"\"\"\n",
    "    calibrations = sorted(frame.context.laser_calibrations, key=lambda c: c.name)\n",
    "    point_labels = []\n",
    "    for c in calibrations:\n",
    "        range_image = range_images[c.name][ri_index]\n",
    "        range_image_tensor = tf.reshape(\n",
    "            tf.convert_to_tensor(range_image.data), range_image.shape.dims)\n",
    "        range_image_mask = range_image_tensor[..., 0] > 0\n",
    "\n",
    "        if c.name in segmentation_labels:\n",
    "            sl = segmentation_labels[c.name][ri_index]\n",
    "            sl_tensor = tf.reshape(tf.convert_to_tensor(sl.data), sl.shape.dims)\n",
    "            sl_points_tensor = tf.gather_nd(sl_tensor, tf.where(range_image_mask))\n",
    "        else:\n",
    "            num_valid_point = tf.math.reduce_sum(tf.cast(range_image_mask, tf.int32))\n",
    "            sl_points_tensor = tf.zeros([num_valid_point, 2], dtype=tf.int32)\n",
    "\n",
    "        point_labels.append(sl_points_tensor.numpy())\n",
    "    return point_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce8bdcd2ecd4211b2fb1e1353f61584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Loading 710 clouds.\n"
     ]
    }
   ],
   "source": [
    "all_sem_clouds = []\n",
    "k = 0    \n",
    "for segment in tqdm(segments):\n",
    "    FILENAME = f'{dataroot}/{segment}'    \n",
    "    dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
    "    for data in dataset:\n",
    "        frame = open_dataset.Frame()\n",
    "        frame.ParseFromString(bytearray(data.numpy()))\n",
    "\n",
    "        if frame.lasers[0].ri_return1.segmentation_label_compressed:\n",
    "            (range_images, camera_projections, segmentation_labels, range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(frame)\n",
    "            points, cp_points = frame_utils.convert_range_image_to_point_cloud(frame, range_images, camera_projections, range_image_top_pose, keep_polar_features=True)\n",
    "            point_labels = convert_range_image_to_point_cloud_labels(frame, range_images, segmentation_labels)\n",
    "\n",
    "            points_all = np.concatenate(points, axis=0) # 3d points in vehicle frame.\n",
    "            points_all = points_all[:, [3, 4, 5, 1]]\n",
    "            point_labels_all = np.concatenate(point_labels, axis=0) # point labels.\n",
    "            point_labels_all = np.reshape(point_labels_all[:,1], (point_labels_all.shape[0], 1))\n",
    "\n",
    "            point_xyzil = np.hstack((points_all, point_labels_all))\n",
    "            all_sem_clouds.append(point_xyzil)\n",
    "\n",
    "print(f'Finished Loading {len(all_sem_clouds)} clouds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berlukas/.local/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clouds to /media/scratch/berlukas/waymo/segments/archive11.npy\n"
     ]
    }
   ],
   "source": [
    "filename = 'archive11'\n",
    "np.save(f'{dataroot}/{filename}.npy', all_sem_clouds)\n",
    "print(f'Saved clouds to {dataroot}/{filename}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features for 710 clouds.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b63c3ec961643c08fdfdc2c16921f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 62.12915795554577 ms on average per sample.\n"
     ]
    }
   ],
   "source": [
    "def progresser(sample_idx, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample = all_sem_clouds[sample_idx]\n",
    "    sample_sphere = Sphere(sample)\n",
    "    # sample_sphere = all_sem_clouds[sample_idx]\n",
    "    features = sample_sphere.sampleUsingGrid(grid)\n",
    "    return features\n",
    "\n",
    "print(f\"Computing features for {len(all_sem_clouds)} clouds.\")\n",
    "bw = 120\n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "\n",
    "sem_idx = np.arange(0, len(all_sem_clouds))\n",
    "sample_func = partial(progresser, grid=grid)\n",
    "start_time = time.time() * 1000\n",
    "sem_features = process_map(sample_func, sem_idx, max_workers=16)\n",
    "executionTime = (time.time() * 1000 - start_time)\n",
    "\n",
    "print(f\"It took {executionTime / len(all_sem_clouds)} ms on average per sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to /media/scratch/berlukas/waymo/segments/features11.npy.\n"
     ]
    }
   ],
   "source": [
    "filename = f\"{dataroot}/features11.npy\"\n",
    "np.save(filename, sem_features)\n",
    "print(f\"Wrote features to {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataroot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchive00\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m archive \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m all_sem_clouds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_sem_clouds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataroot' is not defined"
     ]
    }
   ],
   "source": [
    "filename = 'archive00'\n",
    "archive = f'{dataroot}/{filename}.npy'\n",
    "all_sem_clouds = np.load(f'', allow_pickle=True)\n",
    "print(f'Loaded {len(all_sem_clouds)} from {archive}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
