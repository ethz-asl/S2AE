{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPH Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from pyquaternion.quaternion import Quaternion\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from nuscenes.utils.data_classes import PointCloud, LidarPointCloud, RadarPointCloud, Box\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import os.path as osp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "from matplotlib import cm\n",
    "from sphere import Sphere\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "from functools import partial\n",
    "from dh_grid import DHGrid\n",
    "from sphere import Sphere\n",
    "from img_sphere import ImageSphere, ColorImageSphere\n",
    "from average_meter import AverageMeter\n",
    "import random\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "129 instance,\n",
      "10 sensor,\n",
      "10 calibrated_sensor,\n",
      "632 ego_pose,\n",
      "180 log,\n",
      "1 scene,\n",
      "126 sample,\n",
      "1260 sample_data,\n",
      "7062 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 0.1 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.0 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/media/berlukas/Data/data/datasets/s2ae/lyft/one_scene'\n",
    "json_path = f'{dataset_path}/data'\n",
    "\n",
    "level5data = LyftDataset(data_path=dataset_path, json_path=json_path, verbose=True)\n",
    "\n",
    "lidar_string = 'LIDAR_TOP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'log_token': 'da4ed9e02f64c544f4f1f10c6738216dcb0e6b0d50952e158e5589854af9f100',\n",
       "  'first_sample_token': '24b0962e44420e6322de3f25d9e4e5cc3c7a348ec00bfa69db21517e4ca92cc8',\n",
       "  'name': 'host-a101-lidar0-1241893239199111666-1241893264098084346',\n",
       "  'description': '',\n",
       "  'last_sample_token': '2346756c83f6ae8c4d1adec62b4d0d31b62116d2e1819e96e9512667d15e7cec',\n",
       "  'nbr_samples': 126,\n",
       "  'token': 'da4ed9e02f64c544f4f1f10c6738216dcb0e6b0d50952e158e5589854af9f100'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level5data.scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGrid(bw):\n",
    "    n_grid = 2 * bw\n",
    "    k = 0;\n",
    "    points = np.empty([2, n_grid, n_grid])\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            points[0, i, j] = (np.pi*(2*i+1))/(4*bw)\n",
    "            points[1, i, j] = (2*np.pi*j)/(2*bw);\n",
    "            k = k + 1;\n",
    "    return points\n",
    "\n",
    "def createGrid_old(bw):\n",
    "        n_grid = 2 * bw\n",
    "        k = 0;\n",
    "        points = np.empty([n_grid * n_grid, 2])\n",
    "        for i in range(n_grid):\n",
    "            for j in range(n_grid):\n",
    "                points[k, 0] = (np.pi*(2*i+1))/(4*bw)\n",
    "                points[k, 1] = (2*np.pi*j)/(2*bw);\n",
    "                k = k + 1;\n",
    "        return points\n",
    "\n",
    "def ConvertGridToEuclidean(grid):\n",
    "    cart_grid = np.zeros([3, grid.shape[1], grid.shape[2]])\n",
    "    cart_grid[0,:,:] = np.multiply(np.sin(grid[0, :,:]), np.cos(grid[1,:,:]))\n",
    "    cart_grid[1,:,:] = np.multiply(np.sin(grid[0, :, :]), np.sin(grid[1, :, :]))\n",
    "    cart_grid[2,:,:] = np.cos(grid[0, :, :])    \n",
    "    return cart_grid\n",
    "\n",
    "def convertGridToEuclidean_old(grid):\n",
    "    cart_grid = np.zeros([ grid.shape[0], 3])\n",
    "    cart_grid[:,0] = np.multiply(np.sin(grid[:,0]), np.cos(grid[:,1]))\n",
    "    cart_grid[:,1] = np.multiply(np.sin(grid[:,0]), np.sin(grid[:,1]))\n",
    "    cart_grid[:,2] = np.cos(grid[:,0])    \n",
    "    return cart_grid\n",
    "\n",
    "def rgb_to_greyscale(r,g,b):\n",
    "    return 0.2126*r + 0.7152*g + 0.0722*b\n",
    "#     return 0.3*r + 0.59*g + 0.11*b    \n",
    "\n",
    "def transform_from_pcl_to_cam(nusc, pointsensor, cam, pc):\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def transform_from_cam_to_pcl(nusc, pointsensor, cam, pc):\n",
    "    # Transform from the camera into the vehicle's ego frame\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])    \n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "    \n",
    "    # Transform from the ego frame (cam) to the global frame\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "        \n",
    "    # Transform from the global frame to the ego frame of the LiDAR.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])    \n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "        \n",
    "    # Transform from the ego frame (LiDAR) to the LiDAR frame\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])    \n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    return pc\n",
    "\n",
    "def project_pc_on_cam(pc, cam_intrinsics, depths, im):\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    points = view_points(pc.points[:3, :], cam_intrinsics, normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "    # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "    # casing for non-keyframes which are slightly out of sync.\n",
    "    min_dist = 0.0001\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths < -1.0)\n",
    "    mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "    mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "    return points, mask\n",
    "\n",
    "def sample_mono_from_image(im, pc, mask, points):\n",
    "    n_mask = len(mask)\n",
    "        \n",
    "    for i in range(0,n_mask):    \n",
    "        visible = mask[i]\n",
    "        if not visible:\n",
    "            continue\n",
    "        cur_point = (points[0,i], points[1,i])    \n",
    "        px = im.getpixel(cur_point)    \n",
    "        intensity = rgb_to_greyscale(px[0], px[1], px[2])\n",
    "        pc.points[3,i] = intensity\n",
    "    return pc\n",
    "\n",
    "def sample_color_from_image(im, pc, mask, points):\n",
    "    n_mask = len(mask)    \n",
    "    \n",
    "    for i in range(0,n_mask):    \n",
    "        visible = mask[i]\n",
    "        if not visible:\n",
    "            continue\n",
    "        cur_point = (points[0,i], points[1,i])            \n",
    "        pc.points[3:6,i] = im.getpixel(cur_point)\n",
    "    return pc\n",
    "\n",
    "def prepare_for_viz(cloud):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    if cloud.shape[1] == 4:\n",
    "        colors = mapIntensityToRGB(cloud[:, 3])\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    if cloud.shape[1] == 6:\n",
    "        pcd.colors = o3d.utility.Vector3dVector(cloud[:,3:6] / 255.0)\n",
    "    return pcd\n",
    "\n",
    "def visualize_pointcloud(cloud, jupyter = False):\n",
    "    pcd = prepare_for_viz(cloud)\n",
    "    \n",
    "    if jupyter:\n",
    "        self.__visualizeJupyter(pcd)\n",
    "    else:\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "        \n",
    "def mapIntensityToRGB(i):\n",
    "    mask = np.where(i < 0)    \n",
    "    colors = cm.jet(plt.Normalize(min(i), max(i))(i))\n",
    "    colors[mask] = 0\n",
    "    return colors\n",
    "\n",
    "def write_pointcloud(cloud, filename):\n",
    "    pcd = prepare_for_viz(cloud)\n",
    "    o3d.io.write_point_cloud(filename, pcd)\n",
    "    \n",
    "def construct_transformation_matrix(R, t):\n",
    "    T = np.eye(4)\n",
    "    T[0:3, 0:3] = R\n",
    "    T[0:3, 3] = t\n",
    "    return T\n",
    "  \n",
    "class SamplingPointCloud(PointCloud):\n",
    "\n",
    "    @staticmethod\n",
    "    def nbr_dims() -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of dimensions.\n",
    "        :return: Number of dimensions.\n",
    "        \"\"\"\n",
    "        return 4\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_name: str) -> 'SamplingPointCloud':\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_bw(cls, bw, scale = 100) -> 'SamplingPointCloud':\n",
    "        grid = createGrid_old(bw)\n",
    "        xyz_grid = convertGridToEuclidean_old(grid) * scale\n",
    "        intensities = np.zeros((xyz_grid.shape[0],1), dtype=xyz_grid.dtype)\n",
    "        sampling_grid = np.hstack((xyz_grid, intensities))\n",
    "        return cls(sampling_grid.T)\n",
    "    \n",
    "class ColorSamplingPointCloud(PointCloud):\n",
    "\n",
    "    @staticmethod\n",
    "    def nbr_dims() -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of dimensions.\n",
    "        :return: Number of dimensions.\n",
    "        \"\"\"\n",
    "        return 6\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_name: str) -> 'ColorSamplingPointCloud':\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_bw(cls, bw, scale = 100) -> 'ColorSamplingPointCloud':\n",
    "        grid = createGrid_old(bw)\n",
    "        xyz_grid = convertGridToEuclidean_old(grid) * scale\n",
    "        rgb = np.zeros((xyz_grid.shape[0], 3), dtype=xyz_grid.dtype)\n",
    "        sampling_grid = np.hstack((xyz_grid, rgb))\n",
    "        return cls(sampling_grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "# Convert dataset to PLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ds = '/media/berlukas/Data/data/datasets/s2ae/lyft/processed/'\n",
    "\n",
    "bw = 120\n",
    "scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_scenes = 1\n",
      "chunk 1 [0, 1]\n",
      "chunk 2 [1, 1]\n",
      "chunk 3 [1, 1]\n"
     ]
    }
   ],
   "source": [
    "n_scenes = len(level5data.scene)\n",
    "per_chunk = int(n_scenes / 3)\n",
    "\n",
    "chunk1_start = 0\n",
    "# chunk1_end = per_chunk\n",
    "chunk1_end = 1\n",
    "chunk2_start = chunk1_end\n",
    "chunk2_end = chunk2_start + per_chunk\n",
    "chunk3_start = chunk2_end\n",
    "chunk3_end = n_scenes\n",
    "\n",
    "print(f\"n_scenes = {n_scenes}\")\n",
    "print(f\"chunk 1 [{chunk1_start}, {chunk1_end}]\")\n",
    "print(f\"chunk 2 [{chunk2_start}, {chunk2_end}]\")\n",
    "print(f\"chunk 3 [{chunk3_start}, {chunk3_end}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the pointclouds to spherical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progresser_lidar(sample, grid, auto_position=True, write_safe=False, blocking=True, progress=False):    \n",
    "    sample_sphere = Sphere(sample)\n",
    "    return sample_sphere.sampleUsingGrid(grid, True)\n",
    "\n",
    "def writeRawPointCloud(cloud, filename):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    colors = mapIntensityToRGB(cloud[:, 3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.io.write_point_cloud(filename, pcd)\n",
    "    \n",
    "def get_data_from_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    pointsensor = nusc.get('sample_data', sample['data'][lidar_string])\n",
    "    # Load a single lidar point cloud.\n",
    "    pcl_path = osp.join(dataset_path, pointsensor['filename'])\n",
    "    pc = LidarPointCloud.from_file(pcl_path)    \n",
    "    return pc.points.T, sample['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_point_cloud_from_token(sample_token):\n",
    "    sample = level5data.get('sample', sample_token)\n",
    "    pointsensor = level5data.get('sample_data', sample['data'][lidar_string])\n",
    "    # Load a single lidar point cloud.\n",
    "    pcl_path = osp.join(dataset_path, pointsensor['filename'])\n",
    "    pc = LidarPointCloud.from_file(pcl_path)\n",
    "    points_xyzi = pc.points.T\n",
    "    \n",
    "    # Load the semantic segmentation.\n",
    "    sample_data_token = sample['data'][lidar_string]\n",
    "    lidarseg_labels_filename = osp.join(dataset_path, level5data.get('lidarseg', sample_data_token)['filename'])\n",
    "    points_label = np.fromfile(lidarseg_labels_filename, dtype=np.uint8)  # [num_points]\n",
    "\n",
    "    # Combine the two.\n",
    "    return np.column_stack((points_xyzi, points_label)), sample['next']\n",
    "\n",
    "def combine_pointclouds(pointclouds, poses):\n",
    "    n_data = len(poses)    \n",
    "    pivot = n_data // 2  \n",
    "    T_G_L_pivot = poses[pivot]\n",
    "    T_L_pivot_G = np.linalg.inv(T_G_L_pivot)\n",
    "\n",
    "    acc_points = pointclouds[pivot]\n",
    "    for i in range(0, n_data):\n",
    "        if i == pivot:\n",
    "            continue\n",
    "        T_G_L = poses[i]\n",
    "        T_L_pivot_L = T_L_pivot_G @ T_G_L\n",
    "\n",
    "        points = Utils.transform_pointcloud(pointclouds[i], T_L_pivot_L)\n",
    "        acc_points = np.append(acc_points, points, axis=0)\n",
    "                        \n",
    "    return acc_points, pivot\n",
    "\n",
    "def combine_every_nth_point_cloud(pointclouds, poses, n=3):\n",
    "    n_poses = len(poses)    \n",
    "    \n",
    "    # Generate combinations.\n",
    "    combined = []\n",
    "    pivot_indices = []\n",
    "    for i in range(0, n_poses, 1):\n",
    "        if i+3 >= n_poses:\n",
    "            break\n",
    "            \n",
    "        indices = np.arange(i, i+3, 1)        \n",
    "        local_clouds = []\n",
    "        local_poses = []\n",
    "        for idx in indices:        \n",
    "            local_clouds.append(pointclouds[idx])\n",
    "            local_poses.append(poses[idx])\n",
    "        acc_points, pivot = combine_pointclouds(local_clouds, local_poses)\n",
    "        combined.append(acc_points)\n",
    "        pivot_indices.append(indices[pivot])\n",
    "    return combined, pivot_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenes from 0 to 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a72d7916884fcc822ea20584cc91c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'LyftDataset' object has no attribute 'dataroot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m scene \u001b[38;5;241m=\u001b[39m level5data\u001b[38;5;241m.\u001b[39mscene[i]\n\u001b[1;32m     13\u001b[0m first_sample_token \u001b[38;5;241m=\u001b[39m scene[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_sample_token\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m pc, next_sample_token \u001b[38;5;241m=\u001b[39m \u001b[43mget_semantic_point_cloud_from_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_sample_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m all_sem_clouds\u001b[38;5;241m.\u001b[39mappend(pc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m next_sample_token \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:            \n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mget_semantic_point_cloud_from_token\u001b[0;34m(sample_token)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the semantic segmentation.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m sample_data_token \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][lidar_string]\n\u001b[0;32m---> 11\u001b[0m lidarseg_labels_filename \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mlevel5data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataroot\u001b[49m, level5data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlidarseg\u001b[39m\u001b[38;5;124m'\u001b[39m, sample_data_token)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m points_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromfile(lidarseg_labels_filename, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)  \u001b[38;5;66;03m# [num_points]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Combine the two.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LyftDataset' object has no attribute 'dataroot'"
     ]
    }
   ],
   "source": [
    "start = chunk1_start\n",
    "end = chunk1_end\n",
    "\n",
    "# start = n_scenes-10\n",
    "# end = n_scenes\n",
    "\n",
    "print(f\"Processing scenes from {start} to {end}.\")\n",
    "\n",
    "all_sem_clouds = []\n",
    "for i in tqdm(range(start, end)):\n",
    "    scene_clouds = []\n",
    "    scene = level5data.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    pc, next_sample_token = get_semantic_point_cloud_from_token(first_sample_token)\n",
    "    all_sem_clouds.append(pc)\n",
    "    \n",
    "    while next_sample_token != '':            \n",
    "        pc, next_sample_token = get_semantic_point_cloud_from_token(next_sample_token)        \n",
    "        all_sem_clouds.append(pc)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clouds to /media/berlukas/Data2/datasets/nuscenes/processed//archive01.npy\n"
     ]
    }
   ],
   "source": [
    "filename = 'archive01'\n",
    "np.save(f'{export_ds}/{filename}.npy', all_sem_clouds)\n",
    "print(f'Saved clouds to {export_ds}/{filename}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading complete. Found {len(all_sem_clouds)} clouds.\")    \n",
    "print(f\"Computing features...\")    \n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "\n",
    "start_time = time.time() * 1000\n",
    "sem_features = process_map(partial(progresser_lidar, grid=grid), all_sem_clouds, max_workers=8, chunksize=100)\n",
    "executionTime = (time.time() * 1000 - start_time)\n",
    "print(f\"It took {executionTime / len(all_sem_clouds)} ms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 240, 240)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"It took {executionTime / len(all_sem_clouds)} ms.\")\n",
    "n_features = len(sem_features)\n",
    "for i in range(n_features):\n",
    "  sem_features[i] = sem_features[i][[0, 2],:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44698834 -0.32775307 -0.28538856  1.         31.        ]\n",
      " [-3.2120266  -0.18916991 -1.8257709  10.         24.        ]]\n",
      "[[-2.7298481  -0.12322364 -1.617994   27.         27.        ]\n",
      " [-2.848908   -0.12197054 -1.5995693  29.         27.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(all_sem_clouds[0][0:2])\n",
    "np.random.shuffle(all_sem_clouds)\n",
    "print(all_sem_clouds[0][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scans = len(all_sem_clouds)\n",
    "n_scans = 1000\n",
    "for i in range(0, n_scans):\n",
    "    filename = f\"{export_ds}/raw/10/scan{i}.npy\"\n",
    "    np.save(filename, all_sem_clouds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4110\n",
    "cur_sem_cloud = sem_features[i]\n",
    "cur_sem_cloud = np.reshape(cur_sem_cloud, (3, -1)).T\n",
    "\n",
    "pc = SamplingPointCloud.from_bw(bw, 1)\n",
    "points_xyz = pc.points.T[:,0:3]\n",
    "points_xyzi = np.column_stack((points_xyz, cur_sem_cloud[:,0]))\n",
    "points_xyzl = np.column_stack((points_xyz, cur_sem_cloud[:, 2]))\n",
    "\n",
    "visualize_pointcloud(points_xyzi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 21\n",
    "cur_sem_cloud = sem_features[i]\n",
    "# cur_image = img_features[i]\n",
    "cur_color_image = color_img_features[i]\n",
    "\n",
    "cur_sem_cloud = np.reshape(cur_sem_cloud, (3, -1)).T\n",
    "# cur_image = np.reshape(cur_image, (1, -1)).T\n",
    "cur_color_image = np.reshape(cur_color_image, (3, -1)).T\n",
    "\n",
    "\n",
    "pc = SamplingPointCloud.from_bw(bw, 1)\n",
    "points_xyz = pc.points.T[:,0:3]\n",
    "points_xyzi = np.column_stack((points_xyz, cur_sem_cloud[:,2]))\n",
    "# points_xyzp = np.column_stack((points_xyz, cur_image))\n",
    "points_xyzl = np.column_stack((points_xyz, cur_sem_cloud[:, 2]))\n",
    "points_xyzrgb = np.column_stack((points_xyz, cur_color_image[:, 0:3]))\n",
    "\n",
    "visualize_pointcloud(points_xyzi)\n",
    "# visualize_pointcloud(points_xyzp)\n",
    "# visualize_pointcloud(points_xyzl)\n",
    "visualize_pointcloud(points_xyzrgb)\n",
    "# visualize_on_top(points_xyzrgb, points_xyzi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write pointclouds\n",
    "write_pointcloud(points_xyzrgb, '/tmp/sph_image.pcd')\n",
    "write_pointcloud(points_xyzi, '/tmp/sph_cloud.pcd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenes from 700 to 850.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45971b9790514d33806c8c6461c27acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We have 6053 clouds in the validation set.\n"
     ]
    }
   ],
   "source": [
    "def get_next_token(sample_token):\n",
    "    sample = nusc.get('sample', sample_token)    \n",
    "    return sample['next']\n",
    "\n",
    "\n",
    "start = 700\n",
    "end = 850\n",
    "\n",
    "print(f\"Processing scenes from {start} to {end}.\")\n",
    "\n",
    "n_clouds = 0\n",
    "for i in tqdm(range(start, end)):\n",
    "    scene_clouds = []\n",
    "    scene = nusc.scene[i]\n",
    "    first_sample_token = scene['first_sample_token']\n",
    "    next_sample_token = get_next_token(first_sample_token)\n",
    "    n_clouds = n_clouds + 1\n",
    "    \n",
    "    while next_sample_token != '':    \n",
    "        next_sample_token = get_next_token(next_sample_token)    \n",
    "        n_clouds = n_clouds + 1\n",
    "        \n",
    "print(f'We have {n_clouds} clouds in the validation set.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
