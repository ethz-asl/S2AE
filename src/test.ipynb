{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Decoded Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy import spatial\n",
    "\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from pyquaternion.quaternion import Quaternion\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from nuscenes.utils.data_classes import PointCloud, LidarPointCloud, RadarPointCloud, Box\n",
    "from matplotlib import cm\n",
    "from open3d import JVisualizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "from model import Model\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "[Model] We have [2, 20, 40, 120, 180, 120, 40, 20, 9] features.\n",
      "[Model] We have [100, 40, 30, 15, 10, 8, 10, 15, 30, 40, 100] bandwidths.\n",
      "All instances initialized.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 100\n",
    "batch_size = 1\n",
    "num_workers = 32\n",
    "n_classes = 9\n",
    "device_ids = [0]\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "criterion = MainLoss()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "# stored_model = 'test_lidarseg_20220502120200.pkl'\n",
    "stored_model = './test_lidarseg_20220502120200.pkl'\n",
    "model = Model(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "net = nn.DataParallel(model, device_ids = device_ids).to(0)\n",
    "\n",
    "net.load_state_dict(torch.load(stored_model))\n",
    "\n",
    "# chkp = './checkpoints/test_lidarseg_20220502120200_8.pth'\n",
    "# checkpoint = torch.load(chkp)\n",
    "# net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "print(f\"All instances initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clouds from /media/scratch/berlukas/nuscenes/sem_clouds_val_new.npy.\n",
      "Shape of clouds is (6053, 2, 200, 200) and sem clouds is (6053, 200, 200)\n",
      "Total size of the training set: 6053\n",
      "Dataset size for testing:  6053\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "cloud_filename = f\"{export_ds}/sem_clouds_val_new.npy\"\n",
    "dec_clouds = f\"{export_ds}/sem_clouds_val_decoded.npy\"\n",
    "dec_gt_clouds = f\"{export_ds}/sem_clouds_gt.npy\"\n",
    "\n",
    "print(f\"Loading clouds from {cloud_filename}.\")\n",
    "cloud_features = np.load(cloud_filename)\n",
    "\n",
    "sem_cloud_features = cloud_features[:, 2, :, :]\n",
    "cloud_features = cloud_features[:, 0:2, :, :]\n",
    "print(f\"Shape of clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")\n",
    "\n",
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, True, test_train_split=0.0, val_train_split=0.0, shuffle=False)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "_, _, data_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "data_size = split.get_test_size()\n",
    "\n",
    "print(\"Dataset size for testing: \", data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b18b1fbd384f2da9af02c1868e49c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6053.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Pixel Accuracy: 0.9613656401634216\n",
      "Average Pixel Accuracy per Class: 0.6270266771316528\n",
      "Average Jaccard Index: 0.5282055139541626\n",
      "Average DICE Coefficient: 0.5994076728820801\n",
      "Decoded clouds are (6053, 9, 200, 200)\n",
      "Decoded gt clouds are (6053, 200, 200)\n",
      "Wrote decoded spheres to /media/scratch/berlukas/nuscenes/sem_clouds_val_decoded.npy.\n",
      "Testing finished!\n"
     ]
    }
   ],
   "source": [
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_decoded_clouds = [None] * data_size\n",
    "    all_gt_clouds = [None] * data_size\n",
    "    k = 0\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    n_iter = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(tqdm(data_loader)):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            writer.add_scalar('Test/PixelAccuracy', pixel_acc, n_iter)   \n",
    "            writer.add_scalar('Test/PixelAccuracyPerClass', pixel_acc_per_class, n_iter)   \n",
    "            writer.add_scalar('Test/JaccardIndex', jacc, n_iter)\n",
    "            writer.add_scalar('Test/DiceCoefficient', dice, n_iter)  \n",
    "            \n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):                \n",
    "                all_decoded_clouds[k] = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_gt_clouds[k] = lidarseg_gt.cpu().data.numpy()[i,:,:]\n",
    "                k = k + 1     \n",
    "            n_iter += 1\n",
    "            \n",
    "        writer.add_scalar('Test/AvgPixelAccuracy', avg_pixel_acc.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgJaccardIndex', avg_jacc.avg, n_iter)\n",
    "        writer.add_scalar('Test/AvgDiceCoefficient', avg_dice.avg, n_iter)  \n",
    "        \n",
    "        print(f'Average Pixel Accuracy: {avg_pixel_acc.avg}')\n",
    "        print(f'Average Pixel Accuracy per Class: {avg_pixel_acc_per_class.avg}')\n",
    "        print(f'Average Jaccard Index: {avg_jacc.avg}')\n",
    "        print(f'Average DICE Coefficient: {avg_dice.avg}')\n",
    "\n",
    "    return np.array(all_decoded_clouds), np.array(all_gt_clouds)\n",
    "\n",
    "print(\"Starting testing...\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "decoded_clouds, gt_clouds = test_lidarseg(net, criterion, writer)\n",
    "print(f'Decoded clouds are {decoded_clouds.shape}')\n",
    "print(f'Decoded gt clouds are {gt_clouds.shape}')\n",
    "\n",
    "# np.save(dec_gt_clouds, gt_clouds)\n",
    "np.save(dec_clouds, decoded_clouds)\n",
    "print(f'Wrote decoded spheres to {dec_clouds}.')\n",
    "\n",
    "writer.close()\n",
    "print(\"Testing finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGrid(bw):\n",
    "    n_grid = 2 * bw\n",
    "    k = 0;\n",
    "    points = np.empty([2, n_grid, n_grid])\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            points[0, i, j] = (np.pi*(2*i+1))/(4*bw)\n",
    "            points[1, i, j] = (2*np.pi*j)/(2*bw);\n",
    "            k = k + 1;\n",
    "    return points\n",
    "\n",
    "def createGrid_old(bw):\n",
    "        n_grid = 2 * bw\n",
    "        k = 0;\n",
    "        points = np.empty([n_grid * n_grid, 2])\n",
    "        for i in range(n_grid):\n",
    "            for j in range(n_grid):\n",
    "                points[k, 0] = (np.pi*(2*i+1))/(4*bw)\n",
    "                points[k, 1] = (2*np.pi*j)/(2*bw);\n",
    "                k = k + 1;\n",
    "        return points\n",
    "    \n",
    "def convertGridToEuclidean_old(grid):\n",
    "    cart_grid = np.zeros([ grid.shape[0], 3])\n",
    "    cart_grid[:,0] = np.multiply(np.sin(grid[:,0]), np.cos(grid[:,1]))\n",
    "    cart_grid[:,1] = np.multiply(np.sin(grid[:,0]), np.sin(grid[:,1]))\n",
    "    cart_grid[:,2] = np.cos(grid[:,0])    \n",
    "    return cart_grid\n",
    "\n",
    "def mapIntensityToRGB(i):\n",
    "    return cm.jet(plt.Normalize(min(i), max(i))(i))\n",
    "\n",
    "class SamplingPointCloud(PointCloud):\n",
    "\n",
    "    @staticmethod\n",
    "    def nbr_dims() -> int:\n",
    "        return 4\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_name: str) -> 'SamplingPointCloud':\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_bw(cls, bw, scale = 100) -> 'SamplingPointCloud':\n",
    "        grid = createGrid_old(bw)\n",
    "        xyz_grid = convertGridToEuclidean_old(grid) * scale\n",
    "        intensities = np.zeros((xyz_grid.shape[0],1))\n",
    "        sampling_grid = np.hstack((xyz_grid, np.ones((xyz_grid.shape[0], 1), dtype=xyz_grid.dtype)))\n",
    "        return cls(sampling_grid.T)\n",
    "\n",
    "def create_spherical_pc(feature, trans = 0, bw = 100):\n",
    "    pc = SamplingPointCloud.from_bw(bw, 1)   \n",
    "    points_xyz = pc.points.T[:,0:3]\n",
    "    points_xyz[:,0] = points_xyz[:,0] + trans\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points_xyz[:, 0:3])\n",
    "    colors = mapIntensityToRGB(feature[:, 0])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    return pcd    \n",
    "\n",
    "def create_cloud_pc(cloud, trans = 0):\n",
    "    cloud[:,0] = cloud[:,0] + trans\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    if cloud.shape[1] == 4:\n",
    "        colors = mapIntensityToRGB(cloud[:, 3])\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    return pcd\n",
    "  \n",
    "def compare_estimation_clouds(decoded, gt, bw = 100):  \n",
    "    decoded_pc = create_cloud_pc(decoded, trans=0)\n",
    "    gt_pc = create_cloud_pc(gt, trans=100)\n",
    "    o3d.visualization.draw_geometries([decoded_pc, gt_pc])\n",
    "    \n",
    "def backproject_cloud(spherical, distance, bw = 100):    \n",
    "    grid, _ = DHGrid.CreateGrid(bw)\n",
    "    n_points = grid.shape[1] * grid.shape[2]\n",
    "    cart_sphere = np.zeros([n_points, 4])\n",
    "    k = 0\n",
    "    for i in range(0, grid.shape[1]):\n",
    "        for j in range(0, grid.shape[2]):\n",
    "            dist = distance[i,j]\n",
    "            if dist <= 0:\n",
    "                continue\n",
    "            cart_sphere[k,0] = dist * np.multiply(np.cos(grid[1,i,j]), np.sin(grid[0,i,j]))\n",
    "            cart_sphere[k,1] = dist * np.multiply(np.sin(grid[1,i,j]), np.sin(grid[0,i,j]))\n",
    "            cart_sphere[k,2] = dist * np.cos(grid[0,i,j])\n",
    "            cart_sphere[k,3] = spherical[i,j]\n",
    "            k = k + 1\n",
    "    return cart_sphere  \n",
    "\n",
    "def prepare_for_viz(cloud):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud[:, 0:3])\n",
    "    if cloud.shape[1] == 4:\n",
    "        colors = mapIntensityToRGB(cloud[:, 3])\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    if cloud.shape[1] == 6:\n",
    "        pcd.colors = o3d.utility.Vector3dVector(cloud[:,3:6] / 255.0)\n",
    "    return pcd\n",
    "\n",
    "def convert_sphere(sph, feature_idx, n_features, bw, trans = 0.0):\n",
    "    sph = np.reshape(sph, (n_features, -1)).T\n",
    "    \n",
    "    pc = SamplingPointCloud.from_bw(bw, 1)\n",
    "    points_xyz = pc.points.T[:,0:3]\n",
    "    points_xyz[:,0] = points_xyz[:,0] + trans\n",
    "    return np.column_stack((points_xyz, sph[:,feature_idx]))\n",
    "    \n",
    "def visualize_pointcloud(cloud):\n",
    "    pcd = prepare_for_viz(cloud)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "def visualize_sphere(sph, feature_idx = 0, n_features = 3, bw = 100):\n",
    "    points_xyzi = convert_sphere(sph, feature_idx, n_features, bw)\n",
    "    visualize_pointcloud(points_xyzi)\n",
    "    \n",
    "def compare_estimation_sphere(decoded, gt, bw = 100):  \n",
    "    decoded_pc = create_spherical_pc(decoded, trans=0, bw=bw)\n",
    "    gt_pc = create_spherical_pc(gt, trans=2.5, bw=bw)\n",
    "    o3d.visualization.draw_geometries([decoded_pc, gt_pc])    \n",
    "    \n",
    "def compare_estimation_sphere2(decoded, gt, bw = 100):  \n",
    "    decoded_pc = convert_sphere(decoded, 0, 1, 100)\n",
    "    gt_pc = convert_sphere(gt, 2, 3, 100, 2.5)\n",
    "    \n",
    "    decoded_pcd = prepare_for_viz(decoded_pc)\n",
    "    gt_pcd = prepare_for_viz(gt_pc)\n",
    "    o3d.visualization.draw_geometries([decoded_pcd, gt_pcd]) \n",
    "\n",
    "def compare_estimation_sphere3(decoded, gt, bw = 100):  \n",
    "    decoded_pcd = create_spherical_pc(decoded, trans=0, bw=bw)\n",
    "    gt_pcd = create_spherical_pc(gt, trans=2.5, bw=bw)\n",
    "    \n",
    "    visualizer = JVisualizer()\n",
    "    visualizer.add_geometry(decoded_pcd)\n",
    "    visualizer.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f5c6eba37b4badb6f0112867ea3392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall pixel acc = 0.9174749851226807\n",
      "pixel acc per class = 0.3855117857456207\n",
      "mean jaccard index = 0.3682934045791626\n",
      "mean dice coeff = 0.39813151955604553\n"
     ]
    }
   ],
   "source": [
    "n_decoded = decoded_clouds.shape[0]\n",
    "\n",
    "avg_pixel_acc = AverageMeter()\n",
    "avg_pixel_acc_per_class = AverageMeter()\n",
    "avg_jacc = AverageMeter()\n",
    "avg_dice = AverageMeter()\n",
    "for i in range(15, 16):\n",
    "    cur_decoded = decoded_clouds[i, :, :, :]\n",
    "    cur_decoded = np.argmax(cur_decoded, axis=0)\n",
    "    cur_sem_cloud = gt_clouds[i, :, :]\n",
    "    cur_input = cloud_features[i, :, :]\n",
    "    \n",
    "    pred_segmentation = torch.from_numpy(cur_decoded).cuda().int()\n",
    "    gt_segmentation = torch.from_numpy(cur_sem_cloud).cuda().int()\n",
    "    pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(gt_segmentation, pred_segmentation, num_classes = 9)\n",
    "    avg_pixel_acc.update(pixel_acc)\n",
    "    avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "    avg_jacc.update(jacc)\n",
    "    avg_dice.update(dice)\n",
    "        \n",
    "    cur_decoded = np.reshape(cur_decoded, (1, -1)).T    \n",
    "    cur_sem_cloud = np.reshape(cur_sem_cloud, (1, -1)).T    \n",
    "    \n",
    "    compare_estimation_sphere3(cur_decoded, cur_sem_cloud, 100)\n",
    "\n",
    "print(f'overall pixel acc = {avg_pixel_acc.avg}')\n",
    "print(f'pixel acc per class = {avg_pixel_acc_per_class.avg}')\n",
    "print(f'mean jaccard index = {avg_jacc.avg}')\n",
    "print(f'mean dice coeff = {avg_dice.avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "cur_decoded = decoded_clouds[i, :, :, :]\n",
    "cur_decoded = np.argmax(cur_decoded, axis=0)\n",
    "\n",
    "# cur_cloud = decoded_gt[i, :, :, :]\n",
    "# compare_estimation_sphere2(cur_decoded, cur_cloud)\n",
    "\n",
    "# compare_estimation_sphere(cur_decoded, cur_cloud)\n",
    "visualize_sphere(cur_decoded, 0, 1, 125)\n",
    "# visualize_sphere(cur_cloud, 2, 3, 100)\n",
    "# est_cloud = backproject_cloud(cur_decoded, cur_input[0,:,:], bw)\n",
    "# gt_cloud = backproject_cloud(cur_sem_sphere, cur_input[0,:,:], bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
