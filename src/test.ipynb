{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Existing Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy import spatial\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "# from model_fcn import ModelFCN\n",
    "# from model_simple_for_testing import ModelSimpleForTesting\n",
    "from model_unet import ModelUnet\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "All instances initialized.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 100\n",
    "learning_rate = 4.5e-3\n",
    "n_epochs = 1\n",
    "batch_size = 5\n",
    "num_workers = 32\n",
    "n_classes = 9\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "# net = ModelSimpleForTesting(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "net = ModelUnet(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# criterion = L2Loss(alpha=0.5, margin=0.2)\n",
    "# criterion = CrossEntropyLoss(n_classes=n_classes)\n",
    "criterion = NegativeLogLikelihoodLoss(n_classes=n_classes)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "stored_model = 'test_training_params.pkl'\n",
    "net.load_state_dict(torch.load(stored_model))\n",
    "\n",
    "print(f\"All instances initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from images from /media/scratch/berlukas/nuscenes/images.npy, clouds from /media/scratch/berlukas/nuscenes/clouds1.npy and sem clouds from /media/scratch/berlukas/nuscenes/sem_classes_gt1.npy\n",
      "Shape of clouds is (11230, 2, 200, 200) and sem clouds is (11230, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "img_filename = f\"{export_ds}/images.npy\"\n",
    "cloud_filename = f\"{export_ds}/clouds1.npy\"\n",
    "sem_clouds_filename = f\"{export_ds}/sem_classes_gt1.npy\"\n",
    "dec_clouds = f\"{export_ds}/decoded.npy\"\n",
    "dec_gt = f\"{export_ds}/decoded_gt.npy\"\n",
    "\n",
    "print(f\"Loading from images from {img_filename}, clouds from {cloud_filename} and sem clouds from {sem_clouds_filename}\")\n",
    "# img_features = np.load(img_filename)\n",
    "cloud_features = np.load(cloud_filename)\n",
    "sem_cloud_features = np.load(sem_clouds_filename)\n",
    "print(f\"Shape of clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of clouds is (400, 2, 200, 200) and sem clouds is (400, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "n_process = 400\n",
    "# img_features = img_features[0:n_process, :, :, :]\n",
    "cloud_features = cloud_features[0:n_process, :, :, :]\n",
    "sem_cloud_features = sem_cloud_features[0:n_process, :, :]\n",
    "print(f\"Shape of clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the training set: 400\n",
      "Training size:  324\n",
      "Validation size:  36\n",
      "Testing size:  40\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(bandwidth, cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, False, test_train_split=0.9, shuffle=True)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "\n",
    "\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_decoded_clouds = [None] * test_size\n",
    "    all_gt_clouds = [None] * test_size\n",
    "    k = 0\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    n_iter = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(test_loader):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            writer.add_scalar('Test/PixelAccuracy', pixel_acc, n_iter)   \n",
    "            writer.add_scalar('Test/PixelAccuracyPerClass', pixel_acc_per_class, n_iter)   \n",
    "            writer.add_scalar('Test/JaccardIndex', jacc, n_iter)\n",
    "            writer.add_scalar('Test/DiceCoefficient', dice, n_iter)  \n",
    "            \n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):                \n",
    "                all_decoded_clouds[k] = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_gt_clouds[k] = lidarseg_gt.cpu().data.numpy()[i,:,:]\n",
    "                k = k + 1     \n",
    "            n_iter += 1\n",
    "            \n",
    "        writer.add_scalar('Test/AvgPixelAccuracy', avg_pixel_acc.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgJaccardIndex', avg_jacc.avg, n_iter)\n",
    "        writer.add_scalar('Test/AvgDiceCoefficient', avg_dice.avg, n_iter)  \n",
    "\n",
    "    return all_decoded_clouds, all_gt_clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n",
      "Testing finished!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting testing...\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "decoded_clouds, gt_clouds = test_lidarseg(net, criterion, writer)\n",
    "\n",
    "np.save(dec_gt, gt_clouds)\n",
    "np.save(dec_clouds, decoded_clouds)\n",
    "\n",
    "writer.close()\n",
    "print(\"Testing finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
