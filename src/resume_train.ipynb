{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Test training for S2AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy import spatial\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "# from model_fcn import ModelFCN\n",
    "# from model_simple_for_testing import ModelSimpleForTesting\n",
    "from model_unet import ModelUnet\n",
    "from model_segnet import ModelSegnet\n",
    "from model import Model\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "[Model] We have [2, 20, 40, 120, 180, 120, 40, 20, 9] features.\n",
      "[Model] We have [100, 40, 30, 15, 10, 8, 10, 15, 30, 40, 100] bandwidths.\n",
      "\n",
      "\n",
      "All network instances are initialized.\n",
      "Saving final model to test_lidarseg_20220423130907.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 100\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 2\n",
    "batch_size = 5\n",
    "num_workers = 32\n",
    "n_classes = 9\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "net = Model(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "criterion = MainLoss()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "model_save = f'test_lidarseg_{timestamp}'\n",
    "\n",
    "print('\\n')\n",
    "print(f'All network instances are initialized.')\n",
    "print(f'Saving final model to {model_save}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clouds from /media/scratch/berlukas/nuscenes/sem_clouds_100_200.npy.\n",
      "Shape clouds is (200, 2, 200, 200) and sem clouds is (200, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "\n",
    "# training\n",
    "cloud_filename = f\"{export_ds}/sem_clouds_100_200.npy\"\n",
    "\n",
    "# testing\n",
    "dec_input = f\"{export_ds}/decoded_input_lidar.npy\"\n",
    "dec_clouds = f\"{export_ds}/decoded_lidar.npy\"\n",
    "dec_gt = f\"{export_ds}/decoded_gt_lidar.npy\"\n",
    "\n",
    "print(f\"Loading clouds from {cloud_filename}.\")\n",
    "cloud_features = np.load(cloud_filename)\n",
    "\n",
    "\n",
    "sem_cloud_features = np.copy(cloud_features[:, 2, :, :])\n",
    "cloud_features = cloud_features[:, 0:2, :, :]\n",
    "print(f\"Shape clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the training set: 200\n",
      "Training size:  190\n",
      "Validation size:  10\n",
      "Test size is 0. Configured for external tests\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, False, test_train_split=1.0, val_train_split=0.05, shuffle=True)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "\n",
    "\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "def train_lidarseg(net, criterion, optimizer, writer, epoch, n_iter, loss_, t0):\n",
    "    net.train()\n",
    "    for batch_idx, (cloud, lidarseg_gt) in enumerate(train_loader):\n",
    "        cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()        \n",
    "        enc_dec_cloud = net(cloud)\n",
    "        loss = criterion(enc_dec_cloud, lidarseg_gt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        loss_ += float(loss)\n",
    "\n",
    "        writer.add_scalar('Train/Loss', float(loss), n_iter)\n",
    "        n_iter += 1\n",
    "\n",
    "        if batch_idx % 10 == 9:\n",
    "            t1 = time.time()\n",
    "            print('[Epoch %d, Batch %4d] loss: %.8f time: %.5f lr: %.3e' %\n",
    "                  (epoch + 1, batch_idx + 1, loss_ / 10, (t1 - t0) / 60, lr))\n",
    "            t0 = t1\n",
    "            loss_ = 0.0\n",
    "    return n_iter\n",
    "\n",
    "def validate_lidarseg(net, criterion, optimizer, writer, epoch, n_iter):\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(val_loader):            \n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(enc_dec_cloud, lidarseg_gt)                                                                                        \n",
    "            writer.add_scalar('Validation/Loss', float(loss), n_iter)                        \n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "\n",
    "            n_iter += 1\n",
    "            \n",
    "        epoch_p_1 = epoch+1\n",
    "        writer.add_scalar('Validation/AvgPixelAccuracy', avg_pixel_acc.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgJaccardIndex', avg_jacc.avg, epoch_p_1)\n",
    "        writer.add_scalar('Validation/AvgDiceCoefficient', avg_dice.avg, epoch_p_1)  \n",
    "    return n_iter\n",
    "\n",
    "def save_checkpoint(net, optimizer, criterion, n_epoch):\n",
    "    checkpoint_path = f'./checkpoints/{model_save}_{n_epoch}.pth'\n",
    "    torch.save({\n",
    "            'epoch': n_epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            }, checkpoint_path)\n",
    "    print('================================')\n",
    "    print(f'Saved checkpoint to {checkpoint_path}')\n",
    "    print('================================')\n",
    "\n",
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_input_clouds = [None] * test_size\n",
    "    all_decoded_clouds = [None] * test_size\n",
    "    all_gt_clouds = [None] * test_size\n",
    "    k = 0\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    n_iter = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(test_loader):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            writer.add_scalar('Test/PixelAccuracy', pixel_acc, n_iter)   \n",
    "            writer.add_scalar('Test/PixelAccuracyPerClass', pixel_acc_per_class, n_iter)   \n",
    "            writer.add_scalar('Test/JaccardIndex', jacc, n_iter)\n",
    "            writer.add_scalar('Test/DiceCoefficient', dice, n_iter)  \n",
    "            \n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):                \n",
    "                all_input_clouds[k] = cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_decoded_clouds[k] = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_gt_clouds[k] = lidarseg_gt.cpu().data.numpy()[i,:,:]\n",
    "                k = k + 1     \n",
    "            n_iter += 1\n",
    "            \n",
    "        writer.add_scalar('Test/AvgPixelAccuracy', avg_pixel_acc.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgJaccardIndex', avg_jacc.avg, n_iter)\n",
    "        writer.add_scalar('Test/AvgDiceCoefficient', avg_dice.avg, n_iter)  \n",
    "\n",
    "    return all_input_clouds, all_decoded_clouds, all_gt_clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./checkpoints/test_lidarseg_20220423102145.pkl_1.pth...\n",
      "Loading trained model weights...\n",
      "Loading trained optimizer...\n",
      "Loading trained model loss function...\n",
      "Previously trained for 2 number of epochs...\n",
      "Starting training using 2 more epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9294bf9392f445bca637c301e1b1de42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2, Batch   10] loss: 0.54206480 time: 1.46805 lr: 1.000e-03\n",
      "[Epoch 2, Batch   20] loss: 0.51888722 time: 0.90931 lr: 1.000e-03\n",
      "[Epoch 2, Batch   30] loss: 0.50916074 time: 0.91189 lr: 1.000e-03\n",
      "================================\n",
      "Saved checkpoint to ./checkpoints/test_lidarseg_20220423124412_1.pth\n",
      "================================\n",
      "[Epoch 3, Batch   10] loss: 0.47568941 time: 0.92686 lr: 9.600e-04\n",
      "[Epoch 3, Batch   20] loss: 0.45820813 time: 0.91006 lr: 9.600e-04\n",
      "[Epoch 3, Batch   30] loss: 0.46434557 time: 0.90382 lr: 9.600e-04\n",
      "================================\n",
      "Saved checkpoint to ./checkpoints/test_lidarseg_20220423124412_2.pth\n",
      "================================\n",
      "\n",
      "Training finished!\n",
      "Saved final weights to ./test_lidarseg_20220423124412.pkl.\n"
     ]
    }
   ],
   "source": [
    "abort = False\n",
    "train_iter = 0\n",
    "val_iter = 0\n",
    "loss_ = 0.0\n",
    "\n",
    "chkp = './checkpoints/test_lidarseg_20220423102145.pkl_1.pth'\n",
    "\n",
    "print(f'Loading checkpoint from {chkp}...')\n",
    "checkpoint = torch.load(chkp)\n",
    "\n",
    "print('Loading trained model weights...')\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print('Loading trained optimizer...')\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print('Loading trained model loss function...')\n",
    "criterion = checkpoint['loss']\n",
    "prev_epochs = checkpoint['epoch'] + 1\n",
    "print(f\"Previously trained for {prev_epochs} epochs...\")\n",
    "\n",
    "# This is very important otherwise we might run in to a CUDA OOM error.\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Starting training using {n_epochs} more epochs')\n",
    "for epoch in tqdm(range(n_epochs)):    \n",
    "    lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch, lr=learning_rate)\n",
    "    t0 = time.time()\n",
    "\n",
    "    cur_epoch = epoch + prev_epochs\n",
    "    train_iter = train_lidarseg(net, criterion, optimizer, writer, cur_epoch, train_iter, loss_, t0)    \n",
    "    val_iter = validate_lidarseg(net, criterion, optimizer, writer, cur_epoch, val_iter)\n",
    "    writer.add_scalar('Train/lr', lr, cur_epoch)\n",
    "    save_checkpoint(net, optimizer, criterion, cur_epoch)\n",
    "        \n",
    "print(\"Training finished!\")\n",
    "final_save_path = f'./{model_save}.pkl'\n",
    "torch.save(net.state_dict(), final_save_path)\n",
    "print(f'Saved final weights to {final_save_path}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
