{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training code for S2AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy import spatial\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "# from model_fcn import ModelFCN\n",
    "# from model_simple_for_testing import ModelSimpleForTesting\n",
    "from model_unet import ModelUnet\n",
    "from model_segnet import ModelSegnet\n",
    "from model import Model\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "All instances initialized.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 100\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 1\n",
    "batch_size = 5\n",
    "num_workers = 32\n",
    "n_classes = 9\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "# net = ModelSimpleForTesting(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "# net = ModelUnet(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "# net = ModelSegnet(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "net = Model(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# criterion = L2Loss(alpha=0.5, margin=0.2)\n",
    "# criterion = CrossEntropyLoss(n_classes=n_classes)\n",
    "# criterion = NegativeLogLikelihoodLoss(n_classes=n_classes)\n",
    "criterion = MainLoss()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "model_save = 'tensorboard.pkl'\n",
    "\n",
    "print(f\"All instances initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from images from /media/scratch/berlukas/nuscenes/images.npy, clouds from /media/scratch/berlukas/nuscenes/clouds1.npy and sem clouds from /media/scratch/berlukas/nuscenes/new_sem_classes_gt1.npy\n",
      "Shape of clouds is (11230, 2, 200, 200) and sem clouds is (11230, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "\n",
    "# training\n",
    "img_filename = f\"{export_ds}/images.npy\"\n",
    "cloud_filename = f\"{export_ds}/clouds1.npy\"\n",
    "sem_clouds_filename = f\"{export_ds}/new_sem_classes_gt1.npy\"\n",
    "\n",
    "# testing\n",
    "dec_input = f\"{export_ds}/decoded_input.npy\"\n",
    "dec_clouds = f\"{export_ds}/decoded.npy\"\n",
    "dec_gt = f\"{export_ds}/decoded_gt.npy\"\n",
    "\n",
    "print(f\"Loading from images from {img_filename}, clouds from {cloud_filename} and sem clouds from {sem_clouds_filename}\")\n",
    "# img_features = np.load(img_filename)\n",
    "img_features = np.zeros((1,1,1))\n",
    "cloud_features = np.load(cloud_filename)\n",
    "sem_cloud_features = np.load(sem_clouds_filename)\n",
    "print(f\"Shape of clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 2\n",
    "cloud_filename = f\"{export_ds}/clouds2.npy\"\n",
    "sem_clouds_filename = f\"{export_ds}/new_sem_classes_gt2.npy\"\n",
    "\n",
    "cloud_features_2 = np.load(cloud_filename)\n",
    "sem_cloud_features_2 = np.load(sem_clouds_filename)\n",
    "print(f\"Shape of clouds (2) is {cloud_features_2.shape} and sem clouds (2) is {sem_cloud_features_2.shape}\")\n",
    "\n",
    "# training 3\n",
    "cloud_filename = f\"{export_ds}/clouds3.npy\"\n",
    "sem_clouds_filename = f\"{export_ds}/new_sem_classes_gt3.npy\"\n",
    "\n",
    "cloud_features_3 = np.load(cloud_filename)\n",
    "sem_cloud_features_3 = np.load(sem_clouds_filename)\n",
    "print(f\"Shape of clouds (3) is {cloud_features_3.shape} and sem clouds (3) is {sem_cloud_features_3.shape}\")\n",
    "\n",
    "cloud_features = np.concatenate((cloud_features, cloud_features_2, cloud_features_3))\n",
    "sem_cloud_features = np.concatenate((sem_cloud_features, sem_cloud_features_2, sem_cloud_features_3))\n",
    "\n",
    "print(f\"Shape of the final clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images is (1, 1, 1), clouds is (400, 2, 200, 200) and sem clouds is (400, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "n_process = 400\n",
    "# img_features = img_features[0:n_process, :, :, :]\n",
    "cloud_features = cloud_features[0:n_process, :, :, :]\n",
    "sem_cloud_features = sem_cloud_features[0:n_process, :, :]\n",
    "print(f\"Shape of images is {img_features.shape}, clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the training set: 400\n",
      "Training size:  342\n",
      "Validation size:  38\n",
      "Testing size:  20\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(bandwidth, cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, False, test_train_split=0.95, shuffle=True)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "\n",
    "\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "def train_lidarseg(net, criterion, optimizer, writer, epoch, n_iter, loss_, t0):\n",
    "    net.train()\n",
    "    for batch_idx, (cloud, lidarseg_gt) in enumerate(train_loader):\n",
    "        cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "        \n",
    "        enc_dec_cloud = net(cloud)\n",
    "        loss, loss_total = criterion(enc_dec_cloud, lidarseg_gt)\n",
    "        #loss_embedd = embedded_a.norm(2) + embedded_p.norm(2) + embedded_n.norm(2)\n",
    "        #loss = loss_triplet + 0.001 * loss_embedd\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ += loss_total.item()\n",
    "\n",
    "        writer.add_scalar('Train/Loss', loss, n_iter)\n",
    "        n_iter += 1\n",
    "\n",
    "        if batch_idx % 10 == 9:\n",
    "            t1 = time.time()\n",
    "            print('[Epoch %d, Batch %4d] loss: %.8f time: %.5f lr: %.3e' %\n",
    "                  (epoch + 1, batch_idx + 1, loss_ / 10, (t1 - t0) / 60, lr))\n",
    "            t0 = t1\n",
    "            loss_ = 0.0\n",
    "    return n_iter\n",
    "\n",
    "def validate_lidarseg(net, criterion, optimizer, writer, epoch, n_iter):\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(val_loader):            \n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss, loss_total = criterion(enc_dec_cloud, lidarseg_gt)                                                                                        \n",
    "            writer.add_scalar('Validation/Loss', loss, n_iter)                        \n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "\n",
    "            n_iter += 1\n",
    "            \n",
    "        epoch_p_1 = epoch+1\n",
    "        writer.add_scalar('Validation/AvgPixelAccuracy', avg_pixel_acc.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgJaccardIndex', avg_jacc.avg, epoch_p_1)\n",
    "        writer.add_scalar('Validation/AvgDiceCoefficient', avg_dice.avg, epoch_p_1)  \n",
    "    return n_iter\n",
    "\n",
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_input_clouds = [None] * test_size\n",
    "    all_decoded_clouds = [None] * test_size\n",
    "    all_gt_clouds = [None] * test_size\n",
    "    k = 0\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    n_iter = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(test_loader):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            writer.add_scalar('Test/PixelAccuracy', pixel_acc, n_iter)   \n",
    "            writer.add_scalar('Test/PixelAccuracyPerClass', pixel_acc_per_class, n_iter)   \n",
    "            writer.add_scalar('Test/JaccardIndex', jacc, n_iter)\n",
    "            writer.add_scalar('Test/DiceCoefficient', dice, n_iter)  \n",
    "            \n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):                \n",
    "                all_input_clouds[k] = cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_decoded_clouds[k] = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_gt_clouds[k] = lidarseg_gt.cpu().data.numpy()[i,:,:]\n",
    "                k = k + 1     \n",
    "            n_iter += 1\n",
    "            \n",
    "        writer.add_scalar('Test/AvgPixelAccuracy', avg_pixel_acc.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgJaccardIndex', avg_jacc.avg, n_iter)\n",
    "        writer.add_scalar('Test/AvgDiceCoefficient', avg_dice.avg, n_iter)  \n",
    "\n",
    "    return all_input_clouds, all_decoded_clouds, all_gt_clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using 1 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae1df0a71d444b5bd0679e5116f449d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch   10] loss: 9.97605314 time: 1.58175 lr: 1.000e-03\n",
      "[Epoch 1, Batch   20] loss: 5.04994588 time: 0.92011 lr: 1.000e-03\n",
      "[Epoch 1, Batch   30] loss: 3.85927474 time: 0.92446 lr: 1.000e-03\n",
      "[Epoch 1, Batch   40] loss: 3.35491855 time: 0.91846 lr: 1.000e-03\n",
      "[Epoch 1, Batch   50] loss: 2.99704871 time: 0.91741 lr: 1.000e-03\n",
      "[Epoch 1, Batch   60] loss: 2.87722762 time: 0.93505 lr: 1.000e-03\n",
      "\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "abort = False\n",
    "train_iter = 0\n",
    "val_iter = 0\n",
    "loss_ = 0.0\n",
    "print(f'Starting training using {n_epochs} epochs')\n",
    "for epoch in tqdm(range(n_epochs)):    \n",
    "    lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch, lr=learning_rate)\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_iter = train_lidarseg(net, criterion, optimizer, writer, epoch, train_iter, loss_, t0)    \n",
    "    val_iter = validate_lidarseg(net, criterion, optimizer, writer, epoch, val_iter)\n",
    "    writer.add_scalar('Train/lr', lr, epoch)\n",
    "        \n",
    "print(\"Training finished!\")\n",
    "torch.save(net.state_dict(), model_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n",
      "Testing finished!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting testing...\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "input_clouds, decoded_clouds, gt_clouds = test_lidarseg(net, criterion, writer)\n",
    "\n",
    "np.save(dec_input, input_clouds)\n",
    "np.save(dec_clouds, decoded_clouds)\n",
    "np.save(dec_gt, gt_clouds)\n",
    "\n",
    "writer.close()\n",
    "print(\"Testing finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
