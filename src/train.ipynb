{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training code for S2AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from external_splitter import ExternalSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "from model_unet import ModelUnet\n",
    "from model_segnet import ModelSegnet\n",
    "from model import Model\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "from scheduler import *\n",
    "\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "[Model] We have [2, 20, 45, 140, 180, 140, 45, 20, 7] features.\n",
      "[Model] We have [100, 40, 30, 15, 10, 8, 10, 15, 30, 40, 100] bandwidths.\n",
      "\n",
      "\n",
      "All instances initialized.\n",
      "Saving final model to test_lidarseg_20220510125612\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 100\n",
    "learning_rate = 2.4e-3\n",
    "n_epochs = 3\n",
    "batch_size = 5\n",
    "num_workers = 32\n",
    "n_classes = 7\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "net = Model(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(net.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=1.0e-4,\n",
    "                            nesterov=True)\n",
    "\n",
    "\n",
    "# criterion = MainLoss()\n",
    "criterion = WceLovasz()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "model_save = f'test_lidarseg_{timestamp}'\n",
    "\n",
    "print('\\n')\n",
    "print(f'All instances initialized.')\n",
    "print(f'Saving final model to {model_save}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clouds from /media/scratch/berlukas/nuscenes/test_training/sem_clouds2_tiny.npy.\n",
      "Shape clouds is (500, 2, 200, 200) and sem clouds is (500, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "\n",
    "# training\n",
    "cloud_filename = f\"{export_ds}/test_training/sem_clouds2_tiny.npy\"\n",
    "\n",
    "print(f\"Loading clouds from {cloud_filename}.\")\n",
    "cloud_features = np.load(cloud_filename)\n",
    "\n",
    "\n",
    "sem_cloud_features = np.copy(cloud_features[:, 2, :, :])\n",
    "cloud_features = cloud_features[:, 0:2, :, :]\n",
    "print(f\"Shape clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the training set: 500\n",
      "Training size:  451\n",
      "Validation size:  24\n",
      "Testing size:  25\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, False, test_train_split=0.95, val_train_split=0.05, shuffle=True)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "\n",
    "\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)\n",
    "    \n",
    "def sched(epoch, x):\n",
    "    return 0.95**epoch // 2\n",
    "\n",
    "def cosine_schedule_with_warmup(k, num_epochs, batch_size, dataset_size):\n",
    "    batch_size *= dist.size()\n",
    "    if dist.size() == 1:\n",
    "        warmup_iters = 0\n",
    "    else:\n",
    "        warmup_iters = 1000 // dist.size()\n",
    "\n",
    "    if k < warmup_iters:\n",
    "        return (k + 1) / warmup_iters\n",
    "    else:\n",
    "        iter_per_epoch = (dataset_size + batch_size - 1) // batch_size\n",
    "        ratio = (k - warmup_iters) / (num_epochs * iter_per_epoch)\n",
    "        return 0.5 * (1 + np.cos(np.pi * ratio))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=n_epochs)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#                 optimizer, T_0=2 * train_size, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_filename = f\"{export_ds}/sem_clouds_val.npy\"\n",
    "\n",
    "# print(f\"Loading clouds from {val_filename}.\")\n",
    "# cloud_val = np.load(val_filename)\n",
    "\n",
    "# sem_val_features = np.copy(cloud_val[:, 2, :, :])\n",
    "# val_features = cloud_val[:, 0:2, :, :]\n",
    "# print(f\"Shape clouds is {val_features.shape} and sem clouds is {sem_val_features.shape}\")\n",
    "\n",
    "# train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "# val_set = TrainingSetLidarSeg(val_features, sem_val_features)\n",
    "# split = ExternalSplitter(train_set, val_set)\n",
    "\n",
    "# # Split the data into train, val and optionally test\n",
    "# train_loader, val_loader = split.get_split(batch_size=batch_size, num_workers=num_workers)\n",
    "# train_size = split.get_train_size()\n",
    "# val_size = split.get_val_size()\n",
    "# test_size = 0\n",
    "# print(\"Training size: \", train_size)\n",
    "# print(\"Validation size: \", val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "def train_lidarseg(net, criterion, optimizer, writer, epoch, n_iter, loss_, t0):\n",
    "    net.train()\n",
    "    for batch_idx, (cloud, lidarseg_gt) in enumerate(train_loader):\n",
    "        cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()        \n",
    "        enc_dec_cloud = net(cloud)\n",
    "        loss = criterion(enc_dec_cloud, lidarseg_gt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        loss_ += float(loss)\n",
    "\n",
    "        writer.add_scalar('Train/Loss', float(loss), n_iter)\n",
    "        n_iter += 1\n",
    "\n",
    "        scheduler.step(epoch + batch_idx / train_size)\n",
    "        if batch_idx % 10 == 9:\n",
    "            t1 = time.time()\n",
    "            print('[Epoch %d, Batch %4d] loss: %.8f time: %.5f' %\n",
    "                  (epoch + 1, batch_idx + 1, loss_ / 10, (t1 - t0) / 60))\n",
    "            t0 = t1\n",
    "            loss_ = 0.0\n",
    "    return n_iter\n",
    "\n",
    "def validate_lidarseg(net, criterion, optimizer, writer, epoch, n_iter):\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(val_loader):            \n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(enc_dec_cloud, lidarseg_gt)                                                                                        \n",
    "            writer.add_scalar('Validation/Loss', float(loss), n_iter)                        \n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            mask = lidarseg_gt <= 0\n",
    "            pred_segmentation[mask] = 0\n",
    "            \n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "\n",
    "            n_iter += 1\n",
    "            \n",
    "        epoch_p_1 = epoch+1\n",
    "        writer.add_scalar('Validation/AvgPixelAccuracy', avg_pixel_acc.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgJaccardIndex', avg_jacc.avg, epoch_p_1)\n",
    "        writer.add_scalar('Validation/AvgDiceCoefficient', avg_dice.avg, epoch_p_1)  \n",
    "       \n",
    "        print('\\n')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Pixel Accuracy: {avg_pixel_acc.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Pixel Accuracy per Class: {avg_pixel_acc_per_class.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Jaccard Index: {avg_jacc.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average DICE Coefficient: {avg_dice.avg}')\n",
    "        print('\\n')\n",
    "\n",
    "    return n_iter\n",
    "\n",
    "def save_checkpoint(net, optimizer, criterion, scheduler, n_epoch):\n",
    "    checkpoint_path = f'./checkpoints/{model_save}_{n_epoch}.pth'\n",
    "    torch.save({\n",
    "            'epoch': n_epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "    print('================================')\n",
    "    print(f'Saved checkpoint to {checkpoint_path}')\n",
    "    print('================================')\n",
    "\n",
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_input_clouds = [None] * test_size\n",
    "    all_decoded_clouds = [None] * test_size\n",
    "    all_gt_clouds = [None] * test_size\n",
    "    k = 0\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    n_iter = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(test_loader):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            mask = lidarseg_gt <= 0\n",
    "            pred_segmentation[mask] = 0\n",
    "            \n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            writer.add_scalar('Test/PixelAccuracy', pixel_acc, n_iter)   \n",
    "            writer.add_scalar('Test/PixelAccuracyPerClass', pixel_acc_per_class, n_iter)   \n",
    "            writer.add_scalar('Test/JaccardIndex', jacc, n_iter)\n",
    "            writer.add_scalar('Test/DiceCoefficient', dice, n_iter)  \n",
    "            \n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):                \n",
    "                all_input_clouds[k] = cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_decoded_clouds[k] = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_gt_clouds[k] = lidarseg_gt.cpu().data.numpy()[i,:,:]\n",
    "                k = k + 1     \n",
    "            n_iter += 1\n",
    "            \n",
    "        writer.add_scalar('Test/AvgPixelAccuracy', avg_pixel_acc.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgJaccardIndex', avg_jacc.avg, n_iter)\n",
    "        writer.add_scalar('Test/AvgDiceCoefficient', avg_dice.avg, n_iter)  \n",
    "        \n",
    "        print(f'[Test] Average Pixel Accuracy: {avg_pixel_acc.avg}')\n",
    "        print(f'[Test] Average Pixel Accuracy per Class: {avg_pixel_acc_per_class.avg}')\n",
    "        print(f'[Test] Average Jaccard Index: {avg_jacc.avg}')\n",
    "        print(f'[Test] Average DICE Coefficient: {avg_dice.avg}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return all_input_clouds, all_decoded_clouds, all_gt_clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using 3 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ecba5a1bba4c0f8bed24bc60c1b8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch   10] loss: 2.36530472 time: 1.67833\n",
      "[Epoch 1, Batch   20] loss: 1.67701916 time: 1.00561\n",
      "[Epoch 1, Batch   30] loss: 1.58952714 time: 0.95707\n",
      "[Epoch 1, Batch   40] loss: 1.52439348 time: 1.05976\n",
      "[Epoch 1, Batch   50] loss: 1.51304228 time: 0.96034\n",
      "[Epoch 1, Batch   60] loss: 1.44160120 time: 0.91189\n",
      "[Epoch 1, Batch   70] loss: 1.42773392 time: 0.91509\n",
      "[Epoch 1, Batch   80] loss: 1.40968950 time: 0.94145\n",
      "[Epoch 1, Batch   90] loss: 1.34638046 time: 0.92409\n",
      "[Validation for epoch 1] Average Pixel Accuracy: 0.9396817088127136\n",
      "[Validation for epoch 1] Average Pixel Accuracy per Class: 0.4549888074398041\n",
      "[Validation for epoch 1] Average Jaccard Index: 0.3785834312438965\n",
      "[Validation for epoch 1] Average DICE Coefficient: 0.4520137906074524\n",
      "\n",
      "\n",
      "================================\n",
      "Saved checkpoint to ./checkpoints/test_lidarseg_20220510125612_0.pth\n",
      "================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "E0510 13:06:09.066016 139917011445568 ultratb.py:152] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-476af091b86b>\", line 10, in <module>\n",
      "    train_iter = train_lidarseg(net, criterion, optimizer, writer, epoch, train_iter, loss_, t0)\n",
      "  File \"<ipython-input-6-0e82ea1ad6f0>\", line 13, in train_lidarseg\n",
      "    enc_dec_cloud = net(cloud)\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/berlukas/phaser_ws/src/S2AE/src/model.py\", line 269, in forward\n",
      "    d4 = self.deconv4(e1d3)\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 100, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/berlukas/phaser_ws/src/S2AE/src/so3_conv.py\", line 50, in forward\n",
      "    z = so3_mm(x, y)  # [l * m * n, batch, feature_out, complex]\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/s2cnn-1.0.0-py3.6.egg/s2cnn/so3_mm.py\", line 29, in so3_mm\n",
      "    return _cuda_SO3_mm.apply(x, y)\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/s2cnn-1.0.0-py3.6.egg/s2cnn/so3_mm.py\", line 88, in forward\n",
      "    trans_y_spec=True, device=device)\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/s2cnn-1.0.0-py3.6.egg/s2cnn/so3_mm.py\", line 251, in _setup_so3mm_cuda_kernel\n",
      "    kernel = cuda_utils.compile_kernel(kernel, 'so3_mm.cu', 'main_')\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/s2cnn-1.0.0-py3.6.egg/s2cnn/utils/cuda.py\", line 21, in compile_kernel\n",
      "    ptx = program.compile()\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/pynvrtc/compiler.py\", line 64, in compile\n",
      "    self._interface.nvrtcCompileProgram(self._program, options)\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/pynvrtc/interface.py\", line 221, in nvrtcCompileProgram\n",
      "    code = self._lib.nvrtcCompileProgram(prog, len(options), options_array)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "abort = False\n",
    "train_iter = 0\n",
    "val_iter = 0\n",
    "loss_ = 0.0\n",
    "print(f'Starting training using {n_epochs} epochs')\n",
    "for epoch in tqdm(range(n_epochs)):    \n",
    "#     lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch, lr=learning_rate)\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_iter = train_lidarseg(net, criterion, optimizer, writer, epoch, train_iter, loss_, t0)    \n",
    "    scheduler.step()\n",
    "    \n",
    "    val_iter = validate_lidarseg(net, criterion, optimizer, writer, epoch, val_iter)        \n",
    "    \n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    writer.add_scalar('Train/lr', lr, epoch)\n",
    "    save_checkpoint(net, optimizer, criterion, scheduler, epoch)\n",
    "        \n",
    "print(\"Training finished!\")\n",
    "final_save_path = f'./{model_save}.pkl'\n",
    "torch.save(net.state_dict(), final_save_path)\n",
    "print(f'Saved final weights to {final_save_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_size > 0:\n",
    "    # testing\n",
    "    dec_input = f\"{export_ds}/decoded_input_lidar.npy\"\n",
    "    dec_clouds = f\"{export_ds}/decoded_lidar.npy\"\n",
    "    dec_gt = f\"{export_ds}/decoded_gt_lidar.npy\"\n",
    "    \n",
    "    print(\"Starting testing...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    input_clouds, decoded_clouds, gt_clouds = test_lidarseg(net, criterion, writer)\n",
    "\n",
    "    np.save(dec_input, input_clouds)\n",
    "    np.save(dec_clouds, decoded_clouds)\n",
    "    np.save(dec_gt, gt_clouds)\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Testing finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
