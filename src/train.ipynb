{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training code for S2AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from external_splitter import ExternalSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "from model import Model\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "from scheduler import *\n",
    "\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "[Model] We have [1, 15, 40, 70, 100, 70, 40, 15, 7] features.\n",
      "[Model] We have [120, 40, 30, 15, 10, 8, 10, 15, 30, 40, 120] bandwidths.\n",
      "\n",
      "\n",
      "All instances initialized.\n",
      "Saving final model to test_lidarseg_20220805122723\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 120\n",
    "learning_rate = 2.4e-3\n",
    "n_epochs = 3\n",
    "batch_size = 5\n",
    "num_workers = 32\n",
    "n_classes = 7\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "net = Model(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(net.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=1.0e-4,\n",
    "                            nesterov=True)\n",
    "\n",
    "\n",
    "# criterion = MainLoss()\n",
    "criterion = WceLovasz()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "model_save = f'test_lidarseg_{timestamp}'\n",
    "\n",
    "print('\\n')\n",
    "print(f'All instances initialized.')\n",
    "print(f'Saving final model to {model_save}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clouds from /media/scratch/berlukas/nuscenes/test_training/sem_clouds_01_bw120_tiny.npy.\n",
      "Loaded cloud_features with (400, 2, 240, 240)\n",
      "Shape clouds is (400, 1, 240, 240) and sem clouds is (400, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "log_ds = f'{export_ds}/runs/log_{timestamp}'\n",
    "\n",
    "if not os.path.exists(log_ds):\n",
    "    mode = 0o777\n",
    "    os.mkdir(log_ds, mode)\n",
    "\n",
    "# training\n",
    "cloud_filename = f\"{export_ds}/test_training/sem_clouds_01_bw120_tiny.npy\"\n",
    "# cloud_filename_16 = f\"{export_ds}/test_training/sem_clouds_16_tiny.npy\"\n",
    "\n",
    "print(f\"Loading clouds from {cloud_filename}.\")\n",
    "cloud_features = np.load(cloud_filename)\n",
    "# cloud_features_16 = np.load(cloud_filename_16)\n",
    "print(f'Loaded cloud_features with {cloud_features.shape}')\n",
    "\n",
    "\n",
    "sem_cloud_features = np.copy(cloud_features[:, 1, :, :])\n",
    "# sem_cloud_features = np.copy(cloud_features_16[:, 2, :, :])\n",
    "cloud_features = cloud_features[:, 0, :, :]\n",
    "cloud_features = np.reshape(cloud_features, (-1, 1, 2*bandwidth, 2*bandwidth))\n",
    "\n",
    "\n",
    "# n_process = 200\n",
    "# sem_cloud_features = sem_cloud_features[0:n_process, :, :]\n",
    "# cloud_features = cloud_features[0:n_process, :, :]\n",
    "\n",
    "print(f\"Shape clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the training set: 400\n",
      "Training size:  380\n",
      "Validation size:  20\n",
      "Test size is 0. Configured for external tests\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, False, test_train_split=1.0, val_train_split=0.05, shuffle=True)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "\n",
    "\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)\n",
    "    \n",
    "def sched(epoch, x):\n",
    "    return 0.95**epoch // 2\n",
    "\n",
    "def cosine_schedule_with_warmup(k, num_epochs, batch_size, dataset_size):\n",
    "    batch_size *= dist.size()\n",
    "    if dist.size() == 1:\n",
    "        warmup_iters = 0\n",
    "    else:\n",
    "        warmup_iters = 1000 // dist.size()\n",
    "\n",
    "    if k < warmup_iters:\n",
    "        return (k + 1) / warmup_iters\n",
    "    else:\n",
    "        iter_per_epoch = (dataset_size + batch_size - 1) // batch_size\n",
    "        ratio = (k - warmup_iters) / (num_epochs * iter_per_epoch)\n",
    "        return 0.5 * (1 + np.cos(np.pi * ratio))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=n_epochs)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#                 optimizer, T_0=2 * train_size, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_filename = f\"{export_ds}/sem_clouds_val.npy\"\n",
    "\n",
    "# print(f\"Loading clouds from {val_filename}.\")\n",
    "# cloud_val = np.load(val_filename)\n",
    "\n",
    "# sem_val_features = np.copy(cloud_val[:, 2, :, :])\n",
    "# val_features = cloud_val[:, 0:2, :, :]\n",
    "# print(f\"Shape clouds is {val_features.shape} and sem clouds is {sem_val_features.shape}\")\n",
    "\n",
    "# train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "# val_set = TrainingSetLidarSeg(val_features, sem_val_features)\n",
    "# split = ExternalSplitter(train_set, val_set)\n",
    "\n",
    "# # Split the data into train, val and optionally test\n",
    "# train_loader, val_loader = split.get_split(batch_size=batch_size, num_workers=num_workers)\n",
    "# train_size = split.get_train_size()\n",
    "# val_size = split.get_val_size()\n",
    "# test_size = 0\n",
    "# print(\"Training size: \", train_size)\n",
    "# print(\"Validation size: \", val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "def train_lidarseg(net, criterion, optimizer, writer, epoch, n_iter, loss_, t0):\n",
    "    net.train()\n",
    "    for batch_idx, (cloud, lidarseg_gt) in enumerate(train_loader):\n",
    "        cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()        \n",
    "        enc_dec_cloud = net(cloud)\n",
    "        loss = criterion(enc_dec_cloud, lidarseg_gt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        loss_ += float(loss)\n",
    "\n",
    "        writer.add_scalar('Train/Loss', float(loss), n_iter)\n",
    "        n_iter += 1\n",
    "\n",
    "        scheduler.step(epoch + batch_idx / train_size)\n",
    "        if batch_idx % 10 == 9:\n",
    "            t1 = time.time()\n",
    "            print('[Epoch %d, Batch %4d] loss: %.8f time: %.5f' %\n",
    "                  (epoch + 1, batch_idx + 1, loss_ / 10, (t1 - t0) / 60))\n",
    "            t0 = t1\n",
    "            loss_ = 0.0\n",
    "    return n_iter\n",
    "\n",
    "def validate_lidarseg(net, criterion, optimizer, writer, epoch, n_iter):\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    avg_runtime = AverageMeter()\n",
    "    last_segmentation = np.array([])\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(val_loader):            \n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            startTime = time.time() * 1000\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            executionTime = (time.time() * 1000 - startTime)\n",
    "            avg_runtime.update(executionTime)            \n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(enc_dec_cloud, lidarseg_gt)                                                                                        \n",
    "            writer.add_scalar('Validation/Loss', float(loss), n_iter)                        \n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            mask = lidarseg_gt <= 0\n",
    "            pred_segmentation[mask] = 0\n",
    "            \n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            last_index = enc_dec_cloud.shape[0] - 1\n",
    "            last_segmentation = pred_segmentation.cpu().data.numpy()[last_index,:,:]\n",
    "            \n",
    "            n_iter += 1\n",
    "            \n",
    "        epoch_p_1 = epoch+1\n",
    "        writer.add_scalar('Validation/AvgPixelAccuracy', avg_pixel_acc.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgJaccardIndex', avg_jacc.avg, epoch_p_1)\n",
    "        writer.add_scalar('Validation/AvgDiceCoefficient', avg_dice.avg, epoch_p_1)  \n",
    "       \n",
    "        print('\\n')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Pixel Accuracy: {avg_pixel_acc.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Pixel Accuracy per Class: {avg_pixel_acc_per_class.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Jaccard Index: {avg_jacc.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average DICE Coefficient: {avg_dice.avg}')\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        np.save(f'{log_ds}/seg_epoch-{epoch_p_1}.npy', last_segmentation)\n",
    "        print('Execution time in ms: ' + str(avg_runtime.avg))\n",
    "\n",
    "    return n_iter\n",
    "\n",
    "def save_checkpoint(net, optimizer, criterion, scheduler, n_epoch):\n",
    "    checkpoint_path = f'./checkpoints/{model_save}_{n_epoch}.pth'\n",
    "    torch.save({\n",
    "            'epoch': n_epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "    print('================================')\n",
    "    print(f'Saved checkpoint to {checkpoint_path}')\n",
    "    print('================================')\n",
    "\n",
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_input_clouds = [None] * test_size\n",
    "    all_decoded_clouds = [None] * test_size\n",
    "    all_gt_clouds = [None] * test_size\n",
    "    k = 0\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    n_iter = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(test_loader):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            mask = lidarseg_gt <= 0\n",
    "            pred_segmentation[mask] = 0\n",
    "            \n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            writer.add_scalar('Test/PixelAccuracy', pixel_acc, n_iter)   \n",
    "            writer.add_scalar('Test/PixelAccuracyPerClass', pixel_acc_per_class, n_iter)   \n",
    "            writer.add_scalar('Test/JaccardIndex', jacc, n_iter)\n",
    "            writer.add_scalar('Test/DiceCoefficient', dice, n_iter)  \n",
    "            \n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):                \n",
    "                all_input_clouds[k] = cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_decoded_clouds[k] = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_gt_clouds[k] = lidarseg_gt.cpu().data.numpy()[i,:,:]\n",
    "                k = k + 1     \n",
    "            n_iter += 1\n",
    "            \n",
    "        writer.add_scalar('Test/AvgPixelAccuracy', avg_pixel_acc.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgJaccardIndex', avg_jacc.avg, n_iter)\n",
    "        writer.add_scalar('Test/AvgDiceCoefficient', avg_dice.avg, n_iter)  \n",
    "        \n",
    "        print(f'[Test] Average Pixel Accuracy: {avg_pixel_acc.avg}')\n",
    "        print(f'[Test] Average Pixel Accuracy per Class: {avg_pixel_acc_per_class.avg}')\n",
    "        print(f'[Test] Average Jaccard Index: {avg_jacc.avg}')\n",
    "        print(f'[Test] Average DICE Coefficient: {avg_dice.avg}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return all_input_clouds, all_decoded_clouds, all_gt_clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using 3 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29942aaff0074d76b9f61789c1e5d42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch   10] loss: 4.47983568 time: 2.01071\n",
      "[Epoch 1, Batch   20] loss: 2.90645521 time: 0.90880\n",
      "[Epoch 1, Batch   30] loss: 2.58577433 time: 0.91959\n",
      "[Epoch 1, Batch   40] loss: 2.28467181 time: 0.90715\n",
      "[Epoch 1, Batch   50] loss: 2.35407898 time: 0.91476\n",
      "[Epoch 1, Batch   60] loss: 2.31176209 time: 0.91987\n",
      "[Epoch 1, Batch   70] loss: 2.24997323 time: 0.91317\n",
      "\n",
      "\n",
      "[Validation for epoch 1] Average Pixel Accuracy: 0.8952690958976746\n",
      "[Validation for epoch 1] Average Pixel Accuracy per Class: 0.351178377866745\n",
      "[Validation for epoch 1] Average Jaccard Index: 0.26789548993110657\n",
      "[Validation for epoch 1] Average DICE Coefficient: 0.32843130826950073\n",
      "\n",
      "\n",
      "Execution time in ms: 434.468505859375\n",
      "[Epoch 2, Batch   10] loss: 2.18307531 time: 0.93182\n",
      "[Epoch 2, Batch   20] loss: 2.18480538 time: 0.91481\n",
      "[Epoch 2, Batch   30] loss: 2.15758026 time: 0.91750\n",
      "[Epoch 2, Batch   40] loss: 2.21100116 time: 0.91136\n",
      "[Epoch 2, Batch   50] loss: 2.19017684 time: 0.91536\n",
      "[Epoch 2, Batch   60] loss: 2.10990615 time: 0.94153\n",
      "[Epoch 2, Batch   70] loss: 2.23672364 time: 0.94292\n",
      "\n",
      "\n",
      "[Validation for epoch 2] Average Pixel Accuracy: 0.8538853526115417\n",
      "[Validation for epoch 2] Average Pixel Accuracy per Class: 0.31686994433403015\n",
      "[Validation for epoch 2] Average Jaccard Index: 0.2294396609067917\n",
      "[Validation for epoch 2] Average DICE Coefficient: 0.2855023443698883\n",
      "\n",
      "\n",
      "Execution time in ms: 433.373291015625\n",
      "[Epoch 3, Batch   10] loss: 2.23276455 time: 0.94809\n",
      "[Epoch 3, Batch   20] loss: 2.17104471 time: 0.91570\n",
      "[Epoch 3, Batch   30] loss: 2.18984898 time: 0.91083\n",
      "[Epoch 3, Batch   40] loss: 2.15120033 time: 0.91729\n",
      "[Epoch 3, Batch   50] loss: 2.16096375 time: 0.91641\n",
      "[Epoch 3, Batch   60] loss: 2.07015712 time: 0.92464\n",
      "[Epoch 3, Batch   70] loss: 2.21485372 time: 0.92488\n",
      "\n",
      "\n",
      "[Validation for epoch 3] Average Pixel Accuracy: 0.8974730968475342\n",
      "[Validation for epoch 3] Average Pixel Accuracy per Class: 0.36700552701950073\n",
      "[Validation for epoch 3] Average Jaccard Index: 0.2797779440879822\n",
      "[Validation for epoch 3] Average DICE Coefficient: 0.3452056050300598\n",
      "\n",
      "\n",
      "Execution time in ms: 457.3475341796875\n",
      "\n",
      "Training finished!\n",
      "Saved final weights to ./test_lidarseg_20220805122723.pkl.\n"
     ]
    }
   ],
   "source": [
    "abort = False\n",
    "train_iter = 0\n",
    "val_iter = 0\n",
    "loss_ = 0.0\n",
    "print(f'Starting training using {n_epochs} epochs')\n",
    "for epoch in tqdm(range(n_epochs)):    \n",
    "#     lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch, lr=learning_rate)\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_iter = train_lidarseg(net, criterion, optimizer, writer, epoch, train_iter, loss_, t0)    \n",
    "    scheduler.step()\n",
    "    \n",
    "    val_iter = validate_lidarseg(net, criterion, optimizer, writer, epoch, val_iter)        \n",
    "    \n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    writer.add_scalar('Train/lr', lr, epoch)\n",
    "#     save_checkpoint(net, optimizer, criterion, scheduler, epoch)\n",
    "        \n",
    "print(\"Training finished!\")\n",
    "final_save_path = f'./{model_save}.pkl'\n",
    "torch.save(net.state_dict(), final_save_path)\n",
    "print(f'Saved final weights to {final_save_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_size > 0:\n",
    "    # testing\n",
    "    dec_input = f\"{export_ds}/decoded_input_lidar.npy\"\n",
    "    dec_clouds = f\"{export_ds}/decoded_lidar.npy\"\n",
    "    dec_gt = f\"{export_ds}/decoded_gt_lidar.npy\"\n",
    "    \n",
    "    print(\"Starting testing...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    input_clouds, decoded_clouds, gt_clouds = test_lidarseg(net, criterion, writer)\n",
    "\n",
    "    np.save(dec_input, input_clouds)\n",
    "    np.save(dec_clouds, decoded_clouds)\n",
    "    np.save(dec_gt, gt_clouds)\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Testing finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
