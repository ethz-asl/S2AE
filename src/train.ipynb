{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training code for S2AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from external_splitter import ExternalSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "from model_prior import Model\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "from scheduler import *\n",
    "\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "[Model] We have [2, 5, 10, 20, 10, 5, 17] features.\n",
      "[Model] We have [100, 40, 30, 15, 10, 15, 30, 40, 100] bandwidths.\n",
      "\n",
      "\n",
      "All instances initialized.\n",
      "Saving final model to test_lidarseg_20220524144029\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 100\n",
    "learning_rate = 2.4e-3\n",
    "n_epochs = 3\n",
    "batch_size = 5\n",
    "num_workers = 32\n",
    "n_classes = 17\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "net = Model(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(net.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=1.0e-4,\n",
    "                            nesterov=True)\n",
    "\n",
    "\n",
    "# criterion = MainLoss()\n",
    "criterion = WceLovasz()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "model_save = f'test_lidarseg_{timestamp}'\n",
    "\n",
    "print('\\n')\n",
    "print(f'All instances initialized.')\n",
    "print(f'Saving final model to {model_save}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clouds from /media/scratch/berlukas/nuscenes/test_training/sem_clouds1_tiny.npy.\n",
      "Shape clouds is (200, 2, 200, 200) and sem clouds is (200, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "log_ds = f'{export_ds}/runs/log_{timestamp}'\n",
    "\n",
    "mode = 0o777\n",
    "os.mkdir(log_ds, mode)\n",
    "\n",
    "# training\n",
    "cloud_filename = f\"{export_ds}/test_training/sem_clouds1_tiny.npy\"\n",
    "cloud_filename_16 = f\"{export_ds}/test_training/sem_clouds_16_tiny.npy\"\n",
    "\n",
    "print(f\"Loading clouds from {cloud_filename}.\")\n",
    "cloud_features = np.load(cloud_filename)\n",
    "cloud_features_16 = np.load(cloud_filename_16)\n",
    "\n",
    "\n",
    "# sem_cloud_features = np.copy(cloud_features[:, 2, :, :])\n",
    "sem_cloud_features = np.copy(cloud_features_16[:, 2, :, :])\n",
    "cloud_features = cloud_features[:, 0:2, :, :]\n",
    "\n",
    "\n",
    "n_process = 200\n",
    "sem_cloud_features = sem_cloud_features[0:n_process, :, :]\n",
    "cloud_features = cloud_features[0:n_process, :, :, :]\n",
    "\n",
    "print(f\"Shape clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the training set: 200\n",
      "Training size:  190\n",
      "Validation size:  10\n",
      "Test size is 0. Configured for external tests\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, False, test_train_split=1.0, val_train_split=0.05, shuffle=True)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "\n",
    "\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)\n",
    "    \n",
    "def sched(epoch, x):\n",
    "    return 0.95**epoch // 2\n",
    "\n",
    "def cosine_schedule_with_warmup(k, num_epochs, batch_size, dataset_size):\n",
    "    batch_size *= dist.size()\n",
    "    if dist.size() == 1:\n",
    "        warmup_iters = 0\n",
    "    else:\n",
    "        warmup_iters = 1000 // dist.size()\n",
    "\n",
    "    if k < warmup_iters:\n",
    "        return (k + 1) / warmup_iters\n",
    "    else:\n",
    "        iter_per_epoch = (dataset_size + batch_size - 1) // batch_size\n",
    "        ratio = (k - warmup_iters) / (num_epochs * iter_per_epoch)\n",
    "        return 0.5 * (1 + np.cos(np.pi * ratio))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=n_epochs)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#                 optimizer, T_0=2 * train_size, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_filename = f\"{export_ds}/sem_clouds_val.npy\"\n",
    "\n",
    "# print(f\"Loading clouds from {val_filename}.\")\n",
    "# cloud_val = np.load(val_filename)\n",
    "\n",
    "# sem_val_features = np.copy(cloud_val[:, 2, :, :])\n",
    "# val_features = cloud_val[:, 0:2, :, :]\n",
    "# print(f\"Shape clouds is {val_features.shape} and sem clouds is {sem_val_features.shape}\")\n",
    "\n",
    "# train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "# val_set = TrainingSetLidarSeg(val_features, sem_val_features)\n",
    "# split = ExternalSplitter(train_set, val_set)\n",
    "\n",
    "# # Split the data into train, val and optionally test\n",
    "# train_loader, val_loader = split.get_split(batch_size=batch_size, num_workers=num_workers)\n",
    "# train_size = split.get_train_size()\n",
    "# val_size = split.get_val_size()\n",
    "# test_size = 0\n",
    "# print(\"Training size: \", train_size)\n",
    "# print(\"Validation size: \", val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "def train_lidarseg(net, criterion, optimizer, writer, epoch, n_iter, loss_, t0):\n",
    "    net.train()\n",
    "    for batch_idx, (cloud, lidarseg_gt) in enumerate(train_loader):\n",
    "        cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()        \n",
    "        enc_dec_cloud = net(cloud)\n",
    "        loss = criterion(enc_dec_cloud, lidarseg_gt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        loss_ += float(loss)\n",
    "\n",
    "        writer.add_scalar('Train/Loss', float(loss), n_iter)\n",
    "        n_iter += 1\n",
    "\n",
    "        scheduler.step(epoch + batch_idx / train_size)\n",
    "        if batch_idx % 10 == 9:\n",
    "            t1 = time.time()\n",
    "            print('[Epoch %d, Batch %4d] loss: %.8f time: %.5f' %\n",
    "                  (epoch + 1, batch_idx + 1, loss_ / 10, (t1 - t0) / 60))\n",
    "            t0 = t1\n",
    "            loss_ = 0.0\n",
    "    return n_iter\n",
    "\n",
    "def validate_lidarseg(net, criterion, optimizer, writer, epoch, n_iter):\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    last_segmentation = np.array([])\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(val_loader):            \n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(enc_dec_cloud, lidarseg_gt)                                                                                        \n",
    "            writer.add_scalar('Validation/Loss', float(loss), n_iter)                        \n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            mask = lidarseg_gt <= 0\n",
    "            pred_segmentation[mask] = 0\n",
    "            \n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            last_index = enc_dec_cloud.shape[0] - 1\n",
    "            last_segmentation = pred_segmentation.cpu().data.numpy()[last_index,:,:]\n",
    "            \n",
    "            n_iter += 1\n",
    "            \n",
    "        epoch_p_1 = epoch+1\n",
    "        writer.add_scalar('Validation/AvgPixelAccuracy', avg_pixel_acc.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgJaccardIndex', avg_jacc.avg, epoch_p_1)\n",
    "        writer.add_scalar('Validation/AvgDiceCoefficient', avg_dice.avg, epoch_p_1)  \n",
    "       \n",
    "        print('\\n')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Pixel Accuracy: {avg_pixel_acc.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Pixel Accuracy per Class: {avg_pixel_acc_per_class.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Jaccard Index: {avg_jacc.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average DICE Coefficient: {avg_dice.avg}')\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        np.save(f'{log_ds}/seg_epoch-{epoch_p_1}.npy', last_segmentation)\n",
    "\n",
    "    return n_iter\n",
    "\n",
    "def save_checkpoint(net, optimizer, criterion, scheduler, n_epoch):\n",
    "    checkpoint_path = f'./checkpoints/{model_save}_{n_epoch}.pth'\n",
    "    torch.save({\n",
    "            'epoch': n_epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "    print('================================')\n",
    "    print(f'Saved checkpoint to {checkpoint_path}')\n",
    "    print('================================')\n",
    "\n",
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_input_clouds = [None] * test_size\n",
    "    all_decoded_clouds = [None] * test_size\n",
    "    all_gt_clouds = [None] * test_size\n",
    "    k = 0\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    n_iter = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(test_loader):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            mask = lidarseg_gt <= 0\n",
    "            pred_segmentation[mask] = 0\n",
    "            \n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            writer.add_scalar('Test/PixelAccuracy', pixel_acc, n_iter)   \n",
    "            writer.add_scalar('Test/PixelAccuracyPerClass', pixel_acc_per_class, n_iter)   \n",
    "            writer.add_scalar('Test/JaccardIndex', jacc, n_iter)\n",
    "            writer.add_scalar('Test/DiceCoefficient', dice, n_iter)  \n",
    "            \n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):                \n",
    "                all_input_clouds[k] = cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_decoded_clouds[k] = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_gt_clouds[k] = lidarseg_gt.cpu().data.numpy()[i,:,:]\n",
    "                k = k + 1     \n",
    "            n_iter += 1\n",
    "            \n",
    "        writer.add_scalar('Test/AvgPixelAccuracy', avg_pixel_acc.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgJaccardIndex', avg_jacc.avg, n_iter)\n",
    "        writer.add_scalar('Test/AvgDiceCoefficient', avg_dice.avg, n_iter)  \n",
    "        \n",
    "        print(f'[Test] Average Pixel Accuracy: {avg_pixel_acc.avg}')\n",
    "        print(f'[Test] Average Pixel Accuracy per Class: {avg_pixel_acc_per_class.avg}')\n",
    "        print(f'[Test] Average Jaccard Index: {avg_jacc.avg}')\n",
    "        print(f'[Test] Average DICE Coefficient: {avg_dice.avg}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return all_input_clouds, all_decoded_clouds, all_gt_clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using 3 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ef752345be423cb80cf38aef5e6bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch   10] loss: 5.98997130 time: 0.68844\n",
      "[Epoch 1, Batch   20] loss: 3.76519086 time: 0.11387\n",
      "[Epoch 1, Batch   30] loss: 2.91448929 time: 0.11383\n",
      "\n",
      "\n",
      "[Validation for epoch 1] Average Pixel Accuracy: 0.8871574997901917\n",
      "[Validation for epoch 1] Average Pixel Accuracy per Class: 0.16538576781749725\n",
      "[Validation for epoch 1] Average Jaccard Index: 0.1251787543296814\n",
      "[Validation for epoch 1] Average DICE Coefficient: 0.15716399252414703\n",
      "\n",
      "\n",
      "================================\n",
      "Saved checkpoint to ./checkpoints/test_lidarseg_20220524144029_0.pth\n",
      "================================\n",
      "[Epoch 2, Batch   10] loss: 2.82897151 time: 0.13216\n",
      "[Epoch 2, Batch   20] loss: 2.70264318 time: 0.11392\n",
      "[Epoch 2, Batch   30] loss: 2.66301527 time: 0.11394\n",
      "\n",
      "\n",
      "[Validation for epoch 2] Average Pixel Accuracy: 0.8939549922943115\n",
      "[Validation for epoch 2] Average Pixel Accuracy per Class: 0.18502922356128693\n",
      "[Validation for epoch 2] Average Jaccard Index: 0.14000606536865234\n",
      "[Validation for epoch 2] Average DICE Coefficient: 0.176513671875\n",
      "\n",
      "\n",
      "================================\n",
      "Saved checkpoint to ./checkpoints/test_lidarseg_20220524144029_1.pth\n",
      "================================\n",
      "[Epoch 3, Batch   10] loss: 2.71187351 time: 0.13360\n",
      "[Epoch 3, Batch   20] loss: 2.60901690 time: 0.11481\n",
      "[Epoch 3, Batch   30] loss: 2.55713675 time: 0.11470\n",
      "\n",
      "\n",
      "[Validation for epoch 3] Average Pixel Accuracy: 0.8944524526596069\n",
      "[Validation for epoch 3] Average Pixel Accuracy per Class: 0.1831350475549698\n",
      "[Validation for epoch 3] Average Jaccard Index: 0.13968411087989807\n",
      "[Validation for epoch 3] Average DICE Coefficient: 0.1759086400270462\n",
      "\n",
      "\n",
      "================================\n",
      "Saved checkpoint to ./checkpoints/test_lidarseg_20220524144029_2.pth\n",
      "================================\n",
      "\n",
      "Training finished!\n",
      "Saved final weights to ./test_lidarseg_20220524144029.pkl.\n"
     ]
    }
   ],
   "source": [
    "abort = False\n",
    "train_iter = 0\n",
    "val_iter = 0\n",
    "loss_ = 0.0\n",
    "print(f'Starting training using {n_epochs} epochs')\n",
    "for epoch in tqdm(range(n_epochs)):    \n",
    "#     lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch, lr=learning_rate)\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_iter = train_lidarseg(net, criterion, optimizer, writer, epoch, train_iter, loss_, t0)    \n",
    "    scheduler.step()\n",
    "    \n",
    "    val_iter = validate_lidarseg(net, criterion, optimizer, writer, epoch, val_iter)        \n",
    "    \n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    writer.add_scalar('Train/lr', lr, epoch)\n",
    "    save_checkpoint(net, optimizer, criterion, scheduler, epoch)\n",
    "        \n",
    "print(\"Training finished!\")\n",
    "final_save_path = f'./{model_save}.pkl'\n",
    "torch.save(net.state_dict(), final_save_path)\n",
    "print(f'Saved final weights to {final_save_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_size > 0:\n",
    "    # testing\n",
    "    dec_input = f\"{export_ds}/decoded_input_lidar.npy\"\n",
    "    dec_clouds = f\"{export_ds}/decoded_lidar.npy\"\n",
    "    dec_gt = f\"{export_ds}/decoded_gt_lidar.npy\"\n",
    "    \n",
    "    print(\"Starting testing...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    input_clouds, decoded_clouds, gt_clouds = test_lidarseg(net, criterion, writer)\n",
    "\n",
    "    np.save(dec_input, input_clouds)\n",
    "    np.save(dec_clouds, decoded_clouds)\n",
    "    np.save(dec_gt, gt_clouds)\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Testing finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
