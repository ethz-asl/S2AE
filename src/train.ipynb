{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training code for S2AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy import spatial\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "# from model_encode_decode_simple import ModelEncodeDecodeSimple\n",
    "from model_simple_for_testing import ModelSimpleForTesting\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "All instances initialized.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 100\n",
    "learning_rate = 4.5e-3\n",
    "n_epochs = 5\n",
    "batch_size = 2\n",
    "num_workers = 32\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "net = ModelSimpleForTesting(bandwidth).cuda()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# criterion = L2Loss(alpha=0.5, margin=0.2)\n",
    "criterion = CrossEntropyLoss(n_classes=32)\n",
    "# criterion = NegativeLogLikelihoodLoss(n_classes = 32)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "model_save = 'test_training_params.pkl'\n",
    "\n",
    "print(f\"All instances initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from images from /media/scratch/berlukas/nuscenes/images.npy, clouds from /media/scratch/berlukas/nuscenes/clouds.npy and sem clouds from /media/scratch/berlukas/nuscenes/sem_clouds.npy\n",
      "Shape of images is (850, 1, 200, 200), clouds is (850, 2, 200, 200) and sem clouds is (850, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "img_filename = f\"{export_ds}/images.npy\"\n",
    "cloud_filename = f\"{export_ds}/clouds.npy\"\n",
    "sem_clouds_filename = f\"{export_ds}/sem_clouds.npy\"\n",
    "dec_clouds = f\"{export_ds}/decoded.npy\"\n",
    "dec_indices = f\"{export_ds}/indices.npy\"\n",
    "\n",
    "print(f\"Loading from images from {img_filename}, clouds from {cloud_filename} and sem clouds from {sem_clouds_filename}\")\n",
    "img_features = np.load(img_filename)\n",
    "cloud_features = np.load(cloud_filename)\n",
    "sem_cloud_features = np.load(sem_clouds_filename)\n",
    "sem_cloud_features = sem_cloud_features[:,1,:,:] # select the semantic class\n",
    "print(f\"Shape of images is {img_features.shape}, clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the training set: 850\n",
      "Training size:  688\n",
      "Validation size:  77\n",
      "Testing size:  85\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(bandwidth, cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, False, test_train_split=0.9, shuffle=True)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "\n",
    "\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "def train_lidarseg(net, criterion, optimizer, writer, epoch, n_iter, loss_, t0):\n",
    "    net.train()\n",
    "    for batch_idx, (cloud, lidarseg_gt) in enumerate(train_loader):\n",
    "        cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "        \n",
    "        enc_dec_cloud = net(cloud)\n",
    "        loss, loss_total = criterion(enc_dec_cloud, lidarseg_gt)\n",
    "        #loss_embedd = embedded_a.norm(2) + embedded_p.norm(2) + embedded_n.norm(2)\n",
    "        #loss = loss_triplet + 0.001 * loss_embedd\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ += loss_total.item()\n",
    "\n",
    "        writer.add_scalar('Train/Loss', loss, n_iter)\n",
    "        n_iter += 1\n",
    "\n",
    "        if batch_idx % 20 == 19:\n",
    "            t1 = time.time()\n",
    "            print('[Epoch %d, Batch %4d] loss: %.8f time: %.5f lr: %.3e' %\n",
    "                  (epoch + 1, batch_idx + 1, loss_ / 20, (t1 - t0) / 60, lr))\n",
    "            t0 = t1\n",
    "            loss_ = 0.0\n",
    "    return n_iter\n",
    "\n",
    "def validate_lidarseg(net, criterion, optimizer, writer, epoch, n_iter):\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(val_loader):            \n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss, loss_total = criterion(enc_dec_cloud, lidarseg_gt)                                    \n",
    "                                    \n",
    "            writer.add_scalar('Validation/Loss', loss, n_iter)                        \n",
    "            n_iter += 1\n",
    "    return n_iter\n",
    "\n",
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_decoded_clouds = [None] * test_size\n",
    "    k = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(test_loader):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):\n",
    "                all_decoded_clouds[k] = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                k = k + 1\n",
    "    return all_decoded_clouds            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using 5 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd77833696c34b22b964ffc24ada9afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berlukas/workspace/python/pytorch-venv/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch   20] loss: 8.99576576 time: 0.13155 lr: 4.500e-03\n",
      "[Epoch 1, Batch   40] loss: 4.14547040 time: 0.06140 lr: 4.500e-03\n",
      "[Epoch 1, Batch   60] loss: 3.07147354 time: 0.06138 lr: 4.500e-03\n",
      "[Epoch 1, Batch   80] loss: 2.23263977 time: 0.06158 lr: 4.500e-03\n",
      "[Epoch 1, Batch  100] loss: 1.47192770 time: 0.06167 lr: 4.500e-03\n",
      "[Epoch 1, Batch  120] loss: 1.17208176 time: 0.06144 lr: 4.500e-03\n",
      "[Epoch 1, Batch  140] loss: 1.08096944 time: 0.06138 lr: 4.500e-03\n",
      "[Epoch 1, Batch  160] loss: 1.07031105 time: 0.06136 lr: 4.500e-03\n",
      "[Epoch 1, Batch  180] loss: 1.04862596 time: 0.06146 lr: 4.500e-03\n",
      "[Epoch 1, Batch  200] loss: 1.03873737 time: 0.06147 lr: 4.500e-03\n",
      "[Epoch 1, Batch  220] loss: 1.03602674 time: 0.06141 lr: 4.500e-03\n",
      "[Epoch 1, Batch  240] loss: 0.99072495 time: 0.06139 lr: 4.500e-03\n",
      "[Epoch 1, Batch  260] loss: 0.98027969 time: 0.06135 lr: 4.500e-03\n",
      "[Epoch 1, Batch  280] loss: 0.98334565 time: 0.06139 lr: 4.500e-03\n",
      "[Epoch 1, Batch  300] loss: 1.00736828 time: 0.06128 lr: 4.500e-03\n",
      "[Epoch 1, Batch  320] loss: 0.99124320 time: 0.06126 lr: 4.500e-03\n",
      "[Epoch 1, Batch  340] loss: 0.99149111 time: 0.06131 lr: 4.500e-03\n",
      "[Epoch 2, Batch   20] loss: 0.95105014 time: 0.08945 lr: 4.320e-03\n",
      "[Epoch 2, Batch   40] loss: 0.95226434 time: 0.06184 lr: 4.320e-03\n",
      "[Epoch 2, Batch   60] loss: 0.92639444 time: 0.06165 lr: 4.320e-03\n",
      "[Epoch 2, Batch   80] loss: 0.94474007 time: 0.06163 lr: 4.320e-03\n",
      "[Epoch 2, Batch  100] loss: 0.95554232 time: 0.06169 lr: 4.320e-03\n"
     ]
    }
   ],
   "source": [
    "abort = False\n",
    "train_iter = 0\n",
    "val_iter = 0\n",
    "loss_ = 0.0\n",
    "print(f'Starting training using {n_epochs} epochs')\n",
    "for epoch in tqdm(range(n_epochs)):    \n",
    "    lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch, lr=learning_rate)\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_iter = train_lidarseg(net, criterion, optimizer, writer, epoch, train_iter, loss_, t0)    \n",
    "    val_iter = validate_lidarseg(net, criterion, optimizer, writer, epoch, val_iter)\n",
    "    writer.add_scalar('Train/lr', lr, epoch)\n",
    "\n",
    "print(\"Training finished!\")\n",
    "torch.save(net.state_dict(), model_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n",
      "Testing finished!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting testing...\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "decoded_clouds = test_lidarseg(net, criterion, writer)\n",
    "test_indices = np.array(split.test_indices)\n",
    "\n",
    "np.save(dec_indices, test_indices)\n",
    "np.save(dec_clouds, decoded_clouds)\n",
    "\n",
    "writer.close()\n",
    "print(\"Testing finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
