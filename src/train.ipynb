{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training code for S2AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from external_splitter import ExternalSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "from model_unet import ModelUnet\n",
    "from model_segnet import ModelSegnet\n",
    "from model import Model\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "from metrics import *\n",
    "from average_meter import AverageMeter\n",
    "\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "[Model] We have [2, 20, 45, 140, 180, 140, 45, 20, 7] features.\n",
      "[Model] We have [100, 40, 30, 15, 10, 8, 10, 15, 30, 40, 100] bandwidths.\n",
      "\n",
      "\n",
      "All instances initialized.\n",
      "Saving final model to test_lidarseg_20220506092404\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 100\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 1\n",
    "batch_size = 5\n",
    "num_workers = 32\n",
    "n_classes = 7\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "net = Model(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# criterion = MainLoss()\n",
    "criterion = WceLovasz()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "model_save = f'test_lidarseg_{timestamp}'\n",
    "\n",
    "print('\\n')\n",
    "print(f'All instances initialized.')\n",
    "print(f'Saving final model to {model_save}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clouds from /media/scratch/berlukas/nuscenes/test_training/sem_clouds2_tiny.npy.\n",
      "Shape clouds is (500, 2, 200, 200) and sem clouds is (500, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "\n",
    "# training\n",
    "cloud_filename = f\"{export_ds}/test_training/sem_clouds2_tiny.npy\"\n",
    "\n",
    "print(f\"Loading clouds from {cloud_filename}.\")\n",
    "cloud_features = np.load(cloud_filename)\n",
    "\n",
    "\n",
    "sem_cloud_features = np.copy(cloud_features[:, 2, :, :])\n",
    "cloud_features = cloud_features[:, 0:2, :, :]\n",
    "print(f\"Shape clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the training set: 500\n",
      "Training size:  451\n",
      "Validation size:  24\n",
      "Testing size:  25\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, False, test_train_split=0.95, val_train_split=0.05, shuffle=True)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "\n",
    "\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_filename = f\"{export_ds}/sem_clouds_val.npy\"\n",
    "\n",
    "# print(f\"Loading clouds from {val_filename}.\")\n",
    "# cloud_val = np.load(val_filename)\n",
    "\n",
    "# sem_val_features = np.copy(cloud_val[:, 2, :, :])\n",
    "# val_features = cloud_val[:, 0:2, :, :]\n",
    "# print(f\"Shape clouds is {val_features.shape} and sem clouds is {sem_val_features.shape}\")\n",
    "\n",
    "# train_set = TrainingSetLidarSeg(cloud_features, sem_cloud_features)\n",
    "# val_set = TrainingSetLidarSeg(val_features, sem_val_features)\n",
    "# split = ExternalSplitter(train_set, val_set)\n",
    "\n",
    "# # Split the data into train, val and optionally test\n",
    "# train_loader, val_loader = split.get_split(batch_size=batch_size, num_workers=num_workers)\n",
    "# train_size = split.get_train_size()\n",
    "# val_size = split.get_val_size()\n",
    "# test_size = 0\n",
    "# print(\"Training size: \", train_size)\n",
    "# print(\"Validation size: \", val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "def train_lidarseg(net, criterion, optimizer, writer, epoch, n_iter, loss_, t0):\n",
    "    net.train()\n",
    "    for batch_idx, (cloud, lidarseg_gt) in enumerate(train_loader):\n",
    "        cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()        \n",
    "        enc_dec_cloud = net(cloud)\n",
    "        loss = criterion(enc_dec_cloud, lidarseg_gt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        loss_ += float(loss)\n",
    "\n",
    "        writer.add_scalar('Train/Loss', float(loss), n_iter)\n",
    "        n_iter += 1\n",
    "\n",
    "        if batch_idx % 10 == 9:\n",
    "            t1 = time.time()\n",
    "            print('[Epoch %d, Batch %4d] loss: %.8f time: %.5f lr: %.3e' %\n",
    "                  (epoch + 1, batch_idx + 1, loss_ / 10, (t1 - t0) / 60, lr))\n",
    "            t0 = t1\n",
    "            loss_ = 0.0\n",
    "    return n_iter\n",
    "\n",
    "def validate_lidarseg(net, criterion, optimizer, writer, epoch, n_iter):\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(val_loader):            \n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(enc_dec_cloud, lidarseg_gt)                                                                                        \n",
    "            writer.add_scalar('Validation/Loss', float(loss), n_iter)                        \n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "\n",
    "            n_iter += 1\n",
    "            \n",
    "        epoch_p_1 = epoch+1\n",
    "        writer.add_scalar('Validation/AvgPixelAccuracy', avg_pixel_acc.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, epoch_p_1)   \n",
    "        writer.add_scalar('Validation/AvgJaccardIndex', avg_jacc.avg, epoch_p_1)\n",
    "        writer.add_scalar('Validation/AvgDiceCoefficient', avg_dice.avg, epoch_p_1)  \n",
    "       \n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Pixel Accuracy: {avg_pixel_acc.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Pixel Accuracy per Class: {avg_pixel_acc_per_class.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average Jaccard Index: {avg_jacc.avg}')\n",
    "        print(f'[Validation for epoch {epoch_p_1}] Average DICE Coefficient: {avg_dice.avg}')\n",
    "        print('\\n')\n",
    "\n",
    "    return n_iter\n",
    "\n",
    "def save_checkpoint(net, optimizer, criterion, lr, n_epoch):\n",
    "    checkpoint_path = f'./checkpoints/{model_save}_{n_epoch}.pth'\n",
    "    torch.save({\n",
    "            'epoch': n_epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            'lr': lr,\n",
    "            }, checkpoint_path)\n",
    "    print('================================')\n",
    "    print(f'Saved checkpoint to {checkpoint_path}')\n",
    "    print('================================')\n",
    "\n",
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_input_clouds = [None] * test_size\n",
    "    all_decoded_clouds = [None] * test_size\n",
    "    all_gt_clouds = [None] * test_size\n",
    "    k = 0\n",
    "    avg_pixel_acc = AverageMeter()\n",
    "    avg_pixel_acc_per_class = AverageMeter()\n",
    "    avg_jacc = AverageMeter()\n",
    "    avg_dice = AverageMeter()\n",
    "    n_iter = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(test_loader):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            \n",
    "            pred_segmentation = torch.argmax(enc_dec_cloud, dim=1)\n",
    "            pixel_acc, pixel_acc_per_class, jacc, dice = eval_metrics(lidarseg_gt, pred_segmentation, num_classes = n_classes)\n",
    "            avg_pixel_acc.update(pixel_acc)\n",
    "            avg_pixel_acc_per_class.update(pixel_acc_per_class)\n",
    "            avg_jacc.update(jacc)\n",
    "            avg_dice.update(dice)\n",
    "            \n",
    "            writer.add_scalar('Test/PixelAccuracy', pixel_acc, n_iter)   \n",
    "            writer.add_scalar('Test/PixelAccuracyPerClass', pixel_acc_per_class, n_iter)   \n",
    "            writer.add_scalar('Test/JaccardIndex', jacc, n_iter)\n",
    "            writer.add_scalar('Test/DiceCoefficient', dice, n_iter)  \n",
    "            \n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):                \n",
    "                all_input_clouds[k] = cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_decoded_clouds[k] = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_gt_clouds[k] = lidarseg_gt.cpu().data.numpy()[i,:,:]\n",
    "                k = k + 1     \n",
    "            n_iter += 1\n",
    "            \n",
    "        writer.add_scalar('Test/AvgPixelAccuracy', avg_pixel_acc.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgPixelAccuracyPerClass', avg_pixel_acc_per_class.avg, n_iter)   \n",
    "        writer.add_scalar('Test/AvgJaccardIndex', avg_jacc.avg, n_iter)\n",
    "        writer.add_scalar('Test/AvgDiceCoefficient', avg_dice.avg, n_iter)  \n",
    "        \n",
    "        print(f'[Test] Average Pixel Accuracy: {avg_pixel_acc.avg}')\n",
    "        print(f'[Test] Average Pixel Accuracy per Class: {avg_pixel_acc_per_class.avg}')\n",
    "        print(f'[Test] Average Jaccard Index: {avg_jacc.avg}')\n",
    "        print(f'[Test] Average DICE Coefficient: {avg_dice.avg}')\n",
    "        print('\\n')\n",
    "        \n",
    "    return all_input_clouds, all_decoded_clouds, all_gt_clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using 1 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01d086072ec4e25a7a1da1dadf63b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch   10] loss: 3.11602848 time: 1.50229 lr: 1.000e-03\n",
      "[Epoch 1, Batch   20] loss: 1.73081434 time: 0.96311 lr: 1.000e-03\n",
      "[Epoch 1, Batch   30] loss: 1.53966256 time: 0.94671 lr: 1.000e-03\n",
      "[Epoch 1, Batch   40] loss: 1.43499188 time: 0.96252 lr: 1.000e-03\n",
      "[Epoch 1, Batch   50] loss: 1.35359801 time: 0.95589 lr: 1.000e-03\n",
      "[Epoch 1, Batch   60] loss: 1.30283409 time: 0.95533 lr: 1.000e-03\n",
      "[Epoch 1, Batch   70] loss: 1.28955110 time: 0.95876 lr: 1.000e-03\n",
      "[Epoch 1, Batch   80] loss: 1.24263601 time: 0.94980 lr: 1.000e-03\n",
      "[Epoch 1, Batch   90] loss: 1.25323566 time: 0.97786 lr: 1.000e-03\n",
      "[Validation for epoch 1] Average Pixel Accuracy: 0.722410261631012\n",
      "[Validation for epoch 1] Average Pixel Accuracy per Class: 0.4645899832248688\n",
      "[Validation for epoch 1] Average Jaccard Index: 0.2854815721511841\n",
      "[Validation for epoch 1] Average DICE Coefficient: 0.38330507278442383\n",
      "\n",
      "\n",
      "================================\n",
      "Saved checkpoint to ./checkpoints/test_lidarseg_20220506092404_0.pth\n",
      "================================\n",
      "\n",
      "Training finished!\n",
      "Saved final weights to ./test_lidarseg_20220506092404.pkl.\n"
     ]
    }
   ],
   "source": [
    "abort = False\n",
    "train_iter = 0\n",
    "val_iter = 0\n",
    "loss_ = 0.0\n",
    "print(f'Starting training using {n_epochs} epochs')\n",
    "for epoch in tqdm(range(n_epochs)):    \n",
    "    lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch, lr=learning_rate)\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_iter = train_lidarseg(net, criterion, optimizer, writer, epoch, train_iter, loss_, t0)    \n",
    "    val_iter = validate_lidarseg(net, criterion, optimizer, writer, epoch, val_iter)\n",
    "    writer.add_scalar('Train/lr', lr, epoch)\n",
    "    save_checkpoint(net, optimizer, criterion, lr, epoch)\n",
    "        \n",
    "print(\"Training finished!\")\n",
    "final_save_path = f'./{model_save}.pkl'\n",
    "torch.save(net.state_dict(), final_save_path)\n",
    "print(f'Saved final weights to {final_save_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n",
      "[Test] Average Pixel Accuracy: 0.7101399898529053\n",
      "[Test] Average Pixel Accuracy per Class: 0.48164525628089905\n",
      "[Test] Average Jaccard Index: 0.2869625985622406\n",
      "[Test] Average DICE Coefficient: 0.3884340226650238\n",
      "\n",
      "\n",
      "Testing finished!\n"
     ]
    }
   ],
   "source": [
    "if test_size > 0:\n",
    "    # testing\n",
    "    dec_input = f\"{export_ds}/decoded_input_lidar.npy\"\n",
    "    dec_clouds = f\"{export_ds}/decoded_lidar.npy\"\n",
    "    dec_gt = f\"{export_ds}/decoded_gt_lidar.npy\"\n",
    "    \n",
    "    print(\"Starting testing...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    input_clouds, decoded_clouds, gt_clouds = test_lidarseg(net, criterion, writer)\n",
    "\n",
    "    np.save(dec_input, input_clouds)\n",
    "    np.save(dec_clouds, decoded_clouds)\n",
    "    np.save(dec_gt, gt_clouds)\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Testing finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
