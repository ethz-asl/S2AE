{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training code for S2AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy import spatial\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_splitter import DataSplitter\n",
    "from training_set import TrainingSetLidarSeg\n",
    "from loss import *\n",
    "from model_encode_decode_simple import ModelEncodeDecodeSimple\n",
    "# from model_simple_for_testing import ModelSimpleForTesting\n",
    "from sphere import Sphere\n",
    "from visualize import Visualize\n",
    "from metrics import *\n",
    "    \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CUDA...\n",
      "Setting parameters...\n",
      "Initializing data structures...\n",
      "All instances initialized.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing CUDA...\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Setting parameters...\")\n",
    "bandwidth = 100\n",
    "learning_rate = 4.5e-3\n",
    "n_epochs = 1\n",
    "batch_size = 5\n",
    "num_workers = 32\n",
    "n_classes = 9\n",
    "\n",
    "print(f\"Initializing data structures...\")\n",
    "# net = ModelSimpleForTesting(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "net = ModelEncodeDecodeSimple(bandwidth=bandwidth, n_classes=n_classes).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# criterion = L2Loss(alpha=0.5, margin=0.2)\n",
    "# criterion = CrossEntropyLoss(n_classes=n_classes)\n",
    "criterion = NegativeLogLikelihoodLoss(n_classes=n_classes)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "model_save = 'tensorboard.pkl'\n",
    "\n",
    "print(f\"All instances initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from images from /media/scratch/berlukas/nuscenes/images.npy, clouds from /media/scratch/berlukas/nuscenes/clouds1.npy and sem clouds from /media/scratch/berlukas/nuscenes/sem_classes_gt1.npy\n",
      "Shape of images is (850, 1, 200, 200), clouds is (11230, 2, 200, 200) and sem clouds is (11230, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# export_ds = '/mnt/data/datasets/nuscenes/processed'\n",
    "export_ds = '/media/scratch/berlukas/nuscenes'\n",
    "img_filename = f\"{export_ds}/images.npy\"\n",
    "cloud_filename = f\"{export_ds}/clouds1.npy\"\n",
    "sem_clouds_filename = f\"{export_ds}/sem_classes_gt1.npy\"\n",
    "dec_clouds = f\"{export_ds}/decoded.npy\"\n",
    "dec_indices = f\"{export_ds}/indices.npy\"\n",
    "\n",
    "print(f\"Loading from images from {img_filename}, clouds from {cloud_filename} and sem clouds from {sem_clouds_filename}\")\n",
    "img_features = np.load(img_filename)\n",
    "cloud_features = np.load(cloud_filename)\n",
    "sem_cloud_features = np.load(sem_clouds_filename)\n",
    "print(f\"Shape of images is {img_features.shape}, clouds is {cloud_features.shape} and sem clouds is {sem_cloud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the training set: 11230\n",
      "Training size:  9096\n",
      "Validation size:  1011\n",
      "Testing size:  1123\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data loaders\n",
    "train_set = TrainingSetLidarSeg(bandwidth, cloud_features, sem_cloud_features)\n",
    "print(f\"Total size of the training set: {len(train_set)}\")\n",
    "split = DataSplitter(train_set, False, test_train_split=0.9, shuffle=True)\n",
    "\n",
    "# Split the data into train, val and optionally test\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "\n",
    "\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "def train_lidarseg(net, criterion, optimizer, writer, epoch, n_iter, loss_, t0):\n",
    "    net.train()\n",
    "    for batch_idx, (cloud, lidarseg_gt) in enumerate(train_loader):\n",
    "        cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "        \n",
    "        enc_dec_cloud = net(cloud)\n",
    "        loss, loss_total = criterion(enc_dec_cloud, lidarseg_gt)\n",
    "        #loss_embedd = embedded_a.norm(2) + embedded_p.norm(2) + embedded_n.norm(2)\n",
    "        #loss = loss_triplet + 0.001 * loss_embedd\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ += loss_total.item()\n",
    "\n",
    "        writer.add_scalar('Train/Loss', loss, n_iter)\n",
    "        n_iter += 1\n",
    "\n",
    "        if batch_idx % 100 == 99:\n",
    "            t1 = time.time()\n",
    "            print('[Epoch %d, Batch %4d] loss: %.8f time: %.5f lr: %.3e' %\n",
    "                  (epoch + 1, batch_idx + 1, loss_ / 100, (t1 - t0) / 60, lr))\n",
    "            t0 = t1\n",
    "            loss_ = 0.0\n",
    "    return n_iter\n",
    "\n",
    "def validate_lidarseg(net, criterion, optimizer, writer, epoch, n_iter):\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(val_loader):            \n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss, loss_total = criterion(enc_dec_cloud, lidarseg_gt)                                    \n",
    "                                    \n",
    "            writer.add_scalar('Validation/Loss', loss, n_iter)                        \n",
    "            n_iter += 1\n",
    "    return n_iter\n",
    "\n",
    "def test_lidarseg(net, criterion, writer):\n",
    "    all_decoded_clouds = [None] * test_size\n",
    "    k = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():            \n",
    "        for batch_idx, (cloud, lidarseg_gt) in enumerate(test_loader):\n",
    "            cloud, lidarseg_gt = cloud.cuda().float(), lidarseg_gt.cuda().long()\n",
    "            enc_dec_cloud = net(cloud)\n",
    "            n_batch = enc_dec_cloud.shape[0]\n",
    "            for i in range(0, n_batch):\n",
    "                cur_decoded = enc_dec_cloud.cpu().data.numpy()[i,:,:,:]\n",
    "                all_decoded_clouds[k] = cur_decoded\n",
    "                pred_segmentation = np.argmax(cur_decoded, axis=0)\n",
    "                overall_pixel_acc, avg_pixel_acc, avg_jacc, avg_dice = eval_metrics(lidarseg_gt, pred_segmentation)\n",
    "                k = k + 1\n",
    "    return all_decoded_clouds            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abort = False\n",
    "train_iter = 0\n",
    "val_iter = 0\n",
    "loss_ = 0.0\n",
    "print(f'Starting training using {n_epochs} epochs')\n",
    "for epoch in tqdm(range(n_epochs)):    \n",
    "    lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch, lr=learning_rate)\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_iter = train_lidarseg(net, criterion, optimizer, writer, epoch, train_iter, loss_, t0)    \n",
    "    val_iter = validate_lidarseg(net, criterion, optimizer, writer, epoch, val_iter)\n",
    "    writer.add_scalar('Train/lr', lr, epoch)\n",
    "    \n",
    "print(\"Training finished!\")\n",
    "torch.save(net.state_dict(), model_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n",
      "Testing finished!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting testing...\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "decoded_clouds = test_lidarseg(net, criterion, writer)\n",
    "test_indices = np.array(split.test_indices)\n",
    "\n",
    "np.save(dec_indices, test_indices)\n",
    "np.save(dec_clouds, decoded_clouds)\n",
    "\n",
    "writer.close()\n",
    "print(\"Testing finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.add_scalar('test', 0, 0)\n",
    "writer.add_scalar('test', 1, 1)\n",
    "writer.add_scalar('test', 2, 2)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
