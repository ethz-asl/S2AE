{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from s2cnn import S2Convolution\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from average_meter import AverageMeter\n",
    "from data_source import DataSource\n",
    "from data_splitter import DataSplitter\n",
    "from database_parser import DatabaseParser\n",
    "\n",
    "from loss import ImprovedTripletLoss\n",
    "from mission_indices import MissionIndices\n",
    "from model import Model\n",
    "from sphere import Sphere\n",
    "from training_set import TrainingSet\n",
    "from visualize import Visualize\n",
    "from model_encode_decode_simple import ModelEncodeDecodeSimple\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from s2cnn import so3_near_identity_grid, S2Convolution, s2_near_identity_grid, SO3Convolution, so3_integrate\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.set_device(0)\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "bandwidth = 30\n",
    "restore = False\n",
    "n_epochs = 55\n",
    "batch_size = 13\n",
    "num_workers = 32\n",
    "descriptor_size = 256\n",
    "net_input_size = 2 * bandwidth\n",
    "n_features = 3\n",
    "cache = 1\n",
    "n_data = 52\n",
    "\n",
    "#net = ModelEncodeDecodeSimple(bandwidth).cuda()\n",
    "net = ModelEncodeDecodeSimple(bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading missions db from /mnt/data/datasets/Spherical/test_training_data/missions.csv\n",
      "Read    mission_anchor mission_positive mission_negative\n",
      "0      6725..0000       6725..0000       6725..0000\n",
      "1      6725..0000       6725..0000       6725..0000\n",
      "2      4e53..0000       7799..0000       4e53..0000\n",
      "3      4e53..0000       7799..0000       4e53..0000\n",
      "4      4e53..0000       7799..0000       4e53..0000\n",
      "5      4e53..0000       7799..0000       4e53..0000\n",
      "6      4e53..0000       7799..0000       4e53..0000\n",
      "7      4e53..0000       7799..0000       4e53..0000\n",
      "8      4e53..0000       7799..0000       4e53..0000\n",
      "9      4e53..0000       7799..0000       4e53..0000\n",
      "10     4e53..0000       7799..0000       4e53..0000\n",
      "11     4e53..0000       7799..0000       4e53..0000\n",
      "12     4e53..0000       7799..0000       4e53..0000\n",
      "13     4e53..0000       7799..0000       4e53..0000\n",
      "14     4e53..0000       7799..0000       4e53..0000\n",
      "15     4e53..0000       7799..0000       4e53..0000\n",
      "16     4e53..0000       7799..0000       4e53..0000\n",
      "17     4e53..0000       4e53..0000       4e53..0000\n",
      "18     4e53..0000       4e53..0000       4e53..0000\n",
      "19     4e53..0000       4e53..0000       4e53..0000\n",
      "20     4e53..0000       4e53..0000       4e53..0000\n",
      "21     4e53..0000       4e53..0000       4e53..0000\n",
      "22     4e53..0000       4e53..0000       4e53..0000\n",
      "23     4e53..0000       4e53..0000       4e53..0000\n",
      "24     4e53..0000       4e53..0000       4e53..0000\n",
      "25     4e53..0000       4e53..0000       4e53..0000\n",
      "26     4e53..0000       4e53..0000       4e53..0000\n",
      "27     4e53..0000       4e53..0000       4e53..0000\n",
      "28     4e53..0000       4e53..0000       4e53..0000\n",
      "29     4e53..0000       4e53..0000       4e53..0000\n",
      "30     4e53..0000       4e53..0000       4e53..0000\n",
      "31     4e53..0000       4e53..0000       4e53..0000\n",
      "32     4e53..0000       4e53..0000       4e53..0000\n",
      "33     4e53..0000       4e53..0000       4e53..0000\n",
      "34     4e53..0000       4e53..0000       4e53..0000\n",
      "35     4e53..0000       4e53..0000       4e53..0000\n",
      "36     4e53..0000       4e53..0000       4e53..0000\n",
      "37     4e53..0000       4e53..0000       4e53..0000\n",
      "38     4e53..0000       4e53..0000       4e53..0000\n",
      "39     4e53..0000       4e53..0000       4e53..0000\n",
      "40     4e53..0000       4e53..0000       4e53..0000\n",
      "41     4e53..0000       4e53..0000       4e53..0000\n",
      "42     4e53..0000       4e53..0000       4e53..0000\n",
      "43     4e53..0000       4e53..0000       4e53..0000\n",
      "44     4e53..0000       4e53..0000       4e53..0000\n",
      "45     4e53..0000       4e53..0000       4e53..0000\n",
      "46     4e53..0000       4e53..0000       4e53..0000\n",
      "47     4e53..0000       4e53..0000       4e53..0000\n",
      "48     2d2b..0000       2d2b..0000       4e53..0000\n",
      "49     4e53..0000       4e53..0000       4e53..0000\n",
      "50     4e53..0000       2d2b..0000       4e53..0000\n",
      "51     2d2b..0000       4e53..0000       4e53..0000 in total.\n",
      "Read 52 entries.\n",
      "Loading anchors from:\t/mnt/data/datasets/Spherical/test_training_data//training_anchor_pointclouds/ and /mnt/data/datasets/Spherical/test_training_data//training_anchor_sph_images/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8ed922ac674f47b38e70f96b5a0362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2800df3fdf4be7ba2b39b6dd3614a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading positives from:\t/mnt/data/datasets/Spherical/test_training_data//training_positive_pointclouds/ and /mnt/data/datasets/Spherical/test_training_data//training_positive_sph_images/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b31375760c64433bd4f02ea2f4a46c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61da8a2dc644115a7ea706b7a147153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading negatives from:\t/mnt/data/datasets/Spherical/test_training_data//training_negative_pointclouds/ and /mnt/data/datasets/Spherical/test_training_data//training_negative_sph_images/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd0c3dcad88496aabfef86f8d58112b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4696c7fe8f4c7e843a9c5862288dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done loading dataset.\n",
      "\tAnchor point clouds total: \t52\n",
      "\tAnchor images total: \t\t52\n",
      "\tAnchor poses total: \t\t52\n",
      "\tPositive point clouds total: \t52\n",
      "\tPositive images total: \t\t52\n",
      "\tPositive poses total: \t\t52\n",
      "\tNegative point clouds total: \t52\n",
      "\tNegative images total: \t\t52\n",
      "\tNegative poses total: \t\t52\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/mnt/data/datasets/Spherical/test_training_data/\"\n",
    "db_parser = DatabaseParser(dataset_path)\n",
    "\n",
    "ds = DataSource(dataset_path, cache)\n",
    "ds.load(n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e183390e011b4341973f2d01425e17a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating positive spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe9c284f7d34e6c96460dee56f7c02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating negative spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbee443c6814c3f98821445f0667150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated features\n",
      "Generating features from 0 to 52\n"
     ]
    }
   ],
   "source": [
    "train_set = TrainingSet(restore, bandwidth)\n",
    "train_set.generateAll(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size:  52\n",
      "Training size:  46\n",
      "Validation size:  6\n",
      "Test size is 0. Configured for external tests\n"
     ]
    }
   ],
   "source": [
    "print(\"Total size: \", len(train_set))\n",
    "split = DataSplitter(train_set, restore, test_train_split=1.0, shuffle=True)\n",
    "\n",
    "train_loader, val_loader, test_loader = split.get_split(\n",
    "    batch_size=batch_size, num_workers=num_workers)\n",
    "train_size = split.get_train_size()\n",
    "val_size = split.get_val_size()\n",
    "test_size = split.get_test_size()\n",
    "print(\"Training size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "if test_size == 0:\n",
    "    print('Test size is 0. Configured for external tests')\n",
    "else:\n",
    "    print(\"Testing size: \", test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data1, data2, data3) in enumerate(train_loader):\n",
    "    #data1, data2, data3 = data1.cuda().float(), data2.cuda().float(), data3.cuda().float()      \n",
    "    data1, data2, data3 = data1.float(), data2.float(), data3.float()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([13, 2, 60, 60])\n",
      "computed embedding: torch.Size([13, 5, 20, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "print(f\"input shape: {data1.shape}\")\n",
    "embedded_a = net(data1)\n",
    "print(f\"computed embedding: {embedded_a.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
