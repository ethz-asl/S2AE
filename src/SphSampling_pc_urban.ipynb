{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sampled Dataset of PC Urban\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "from plyfile import PlyData, PlyElement\n",
    "from matplotlib import cm\n",
    "from functools import partial\n",
    "import concurrent.futures\n",
    "import random\n",
    "\n",
    "from sphere import Sphere\n",
    "from dh_grid import DHGrid\n",
    "from laserscan import SemLaserScan\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import pprint\n",
    "from os.path import join\n",
    "import glob\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 elements in the folder.\n"
     ]
    }
   ],
   "source": [
    "dataroot = 'pc urban/'\n",
    "segments = os.listdir(dataroot)\n",
    "\n",
    "print(f'Found {len(segments)} elements in the folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75ba37985a449218b4031515f0b47cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 573 clouds\n"
     ]
    }
   ],
   "source": [
    "def retrieve_clouds(lidar_file, label_file):\n",
    "        # print(f'Loading from {lidar_file} and {label_file}')\n",
    "        plydata = PlyData.read(lidar_file)\n",
    "        vertex = plydata['vertex']\n",
    "        x = vertex['x']\n",
    "        y = vertex['y']\n",
    "        z = vertex['z']        \n",
    "        i = np.zeros_like(x)\n",
    "        classes_and_instance = np.genfromtxt(label_file)\n",
    "        # print(f'classes shape is {classes_and_instance.shape}')\n",
    "        c = classes_and_instance[:,0]                \n",
    "        return np.column_stack((x,y,z,i,c))\n",
    "\n",
    "def extract_label_file(lidar_file):\n",
    "    file_name_semantic_label = lidar_file.split('/')\n",
    "    file_name_semantic_label = file_name_semantic_label[-1].split('.')        \n",
    "    file_name_semantic_label = file_name_semantic_label[0] + '.' + file_name_semantic_label[1] + 'Label.txt'                      \n",
    "    return file_name_semantic_label\n",
    "\n",
    "all_sem_clouds = []\n",
    "for segment in tqdm(segments):\n",
    "    cloud_path = f'{dataroot}/{segment}/PLY/*.ply'\n",
    "    lidar_files = sorted(glob.glob(cloud_path))\n",
    "        \n",
    "    for lidar_file in lidar_files:\n",
    "        label_file = extract_label_file(lidar_file)\n",
    "        label_file = f'{dataroot}/{segment}/Label/{label_file}'\n",
    "        cloud = retrieve_clouds(lidar_file, label_file)\n",
    "        all_sem_clouds.append(cloud)    \n",
    "print(f'Extracted {len(all_sem_clouds)} clouds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e3478fc26c46f1b0ebd79c66475201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to /media/berlukas/Data/data/datasets/s2ae/pc urban//clouds.npy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clouds to /media/berlukas/Data/data/datasets/s2ae/pc urban//archive.npy\n"
     ]
    }
   ],
   "source": [
    "def progresser(sample_idx, grid, auto_position=True, write_safe=False, blocking=True, progress=False):\n",
    "    sample = all_sem_clouds[sample_idx]\n",
    "    sample_sphere = Sphere(sample)\n",
    "    features = sample_sphere.sampleUsingGrid(grid)\n",
    "    return features\n",
    "\n",
    "bw = 50\n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "print(f\"Loading complete. Computing features...\")\n",
    "# parallel\n",
    "sem_idx = np.arange(0, len(all_sem_clouds))\n",
    "sample_func = partial(progresser, grid=grid)\n",
    "sem_features = process_map(sample_func, sem_idx, max_workers=16)            \n",
    "\n",
    "filename = f\"{dataroot}/clouds.npy\"\n",
    "np.save(filename, sem_features)\n",
    "print(f\"Wrote features to {filename}.\")\n",
    "\n",
    "filename = f'archive'\n",
    "np.save(f'{dataroot}/{filename}.npy', all_sem_clouds)\n",
    "print(f'Saved clouds to {dataroot}/{filename}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize extracted pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapIntensityToRGB(i):\n",
    "    return cm.jet(plt.Normalize(min(i), max(i))(i))\n",
    "\n",
    "def visualizeRawPointcloud(pcl, val):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pcl[:, 0:3])\n",
    "    colors = mapIntensityToRGB(val)\n",
    "#     colors = scan.sem_color_lut[pcl[:,4].astype(np.int)]\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.visualization.draw_geometries([pcd], width=640,  height=480)    \n",
    "    \n",
    "def createGrid_old(bw):\n",
    "    n_grid = 2 * bw\n",
    "    k = 0;\n",
    "    points = np.empty([n_grid * n_grid, 2])\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            points[k, 0] = (np.pi*(2*i+1))/(4*bw)\n",
    "            points[k, 1] = (2*np.pi*j)/(2*bw);\n",
    "            k = k + 1;\n",
    "    return points\n",
    "    \n",
    "def convertGridToEuclidean_old(grid):\n",
    "    cart_grid = np.zeros([ grid.shape[0], 3])\n",
    "    cart_grid[:,0] = np.multiply(np.sin(grid[:,0]), np.cos(grid[:,1]))\n",
    "    cart_grid[:,1] = np.multiply(np.sin(grid[:,0]), np.sin(grid[:,1]))\n",
    "    cart_grid[:,2] = np.cos(grid[:,0])\n",
    "    return cart_grid\n",
    "\n",
    "def create_sampling_sphere(bw):\n",
    "    grid = createGrid_old(bw)\n",
    "    xyz_grid = convertGridToEuclidean_old(grid)\n",
    "    intensities = np.zeros((xyz_grid.shape[0],1))\n",
    "    sampling_grid = np.hstack((xyz_grid, np.ones((xyz_grid.shape[0], 1), dtype=xyz_grid.dtype)))\n",
    "    return sampling_grid.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = all_sem_clouds[11]\n",
    "visualizeRawPointcloud(pc, pc[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have these classes: [ 0.  1.  8.  9. 11. 15. 28. 30.]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(pc[:, 4])\n",
    "print(f'we have these classes: {classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_sem_cloud = sem_features[11]\n",
    "\n",
    "cur_sem_cloud = np.reshape(cur_sem_cloud, (3, -1)).T\n",
    "pc = create_sampling_sphere(bw)\n",
    "points_xyz = pc.T[:,0:3]\n",
    "points_xyzl = np.column_stack((points_xyz, cur_sem_cloud[:,2]))\n",
    "\n",
    "visualizeRawPointcloud(points_xyzl, points_xyzl[:, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
