{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sampled Dataset of USL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "from matplotlib import cm\n",
    "from functools import partial\n",
    "import concurrent.futures\n",
    "import random\n",
    "\n",
    "from sphere import Sphere\n",
    "from dh_grid import DHGrid\n",
    "from laserscan import SemLaserScan\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting dataroot to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/sequences/.\n",
      "Setting export path to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed/.\n",
      "Configured 4 sequences.\n",
      "Configured config file ../config/semantic-usl.yaml\n"
     ]
    }
   ],
   "source": [
    "def load_sequence(dataroot, sequence):\n",
    "    scan_paths = f'{dataroot}/{sequence}/velodyne'\n",
    "    scan_names = [os.path.join(dp, f) for dp, dn, fn in os.walk(\n",
    "      os.path.expanduser(scan_paths)) for f in fn]\n",
    "    scan_names.sort()\n",
    "    \n",
    "    label_paths = f'{dataroot}/{sequence}/labels'\n",
    "    label_names = [os.path.join(dp, f) for dp, dn, fn in os.walk(\n",
    "        os.path.expanduser(label_paths)) for f in fn]\n",
    "    label_names.sort()    \n",
    "    assert len(label_names) == len(scan_names)\n",
    "    print(f'Found {len(scan_names)} pointclouds and labels for sequence {sequence}.')\n",
    "    return scan_names, label_names\n",
    "\n",
    "def load_config_file(config_file):    \n",
    "    try:        \n",
    "        CFG = yaml.safe_load(open(config_file, 'r'))        \n",
    "        return CFG\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "        return None\n",
    "    \n",
    "def parse_calibration(dataroot, seq):\n",
    "    filename = dataroot + '/' + seq + '/calib.txt'\n",
    "    calib = {}\n",
    "    calib_file = open(filename)\n",
    "    for line in calib_file:\n",
    "        key, content = line.strip().split(\":\")\n",
    "        values = [float(v) for v in content.strip().split()]\n",
    "\n",
    "        pose = np.zeros((4, 4))\n",
    "        pose[0, 0:4] = values[0:4]\n",
    "        pose[1, 0:4] = values[4:8]\n",
    "        pose[2, 0:4] = values[8:12]\n",
    "        pose[3, 3] = 1.0\n",
    "\n",
    "        calib[key] = pose\n",
    "    calib_file.close()\n",
    "    return calib\n",
    "\n",
    "# dataroot = '/mnt/data/datasets/KITTI/sequences'\n",
    "# dataroot = '/media/berlukas/Data/data/datasets/KITTI/sequences'\n",
    "# dataroot = '/media/berlukas/SSD_1TB/data_odometry_velodyne/dataset/sequences'\n",
    "# dataroot = '/media/scratch/berlukas/nuscenes/kitti'\n",
    "# dataroot = '/media/berlukas/Data/data/datasets/s2ae/SemanticPOSS_dataset/dataset/sequences/'\n",
    "dataroot = '/media/berlukas/Data/data/datasets/s2ae/SemanticUSL/sequences/'\n",
    "\n",
    "sequences = ['03', '12', '21', '32']\n",
    "# sequences = ['03']\n",
    "config_file = '../config/semantic-usl.yaml'\n",
    "# export_ds = '/media/berlukas/Data/data/nuscenes'\n",
    "export_ds = '/media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed/'\n",
    "\n",
    "print(f'Setting dataroot to {dataroot}.')\n",
    "print(f'Setting export path to {export_ds}.')\n",
    "print(f'Configured {len(sequences)} sequences.')\n",
    "print(f'Configured config file {config_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequence 03.\n",
      "Found 300 pointclouds and labels for sequence 03.\n",
      "This sequence has 298 data elements.\n",
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c164bb1d2518430c970b6a18b1b2c2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed//clouds-03.npy.\n",
      "Saved clouds to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed//archive-03.npy\n",
      "Loading sequence 12.\n",
      "Found 300 pointclouds and labels for sequence 12.\n",
      "This sequence has 299 data elements.\n",
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86a096f13d8491c8a3c3b8d347366f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed//clouds-12.npy.\n",
      "Saved clouds to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed//archive-12.npy\n",
      "Loading sequence 21.\n",
      "Found 300 pointclouds and labels for sequence 21.\n",
      "This sequence has 299 data elements.\n",
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fc4cce55b8416db3d3f0361c8b226f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed//clouds-21.npy.\n",
      "Saved clouds to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed//archive-21.npy\n",
      "Loading sequence 32.\n",
      "Found 300 pointclouds and labels for sequence 32.\n",
      "This sequence has 299 data elements.\n",
      "Loading complete. Computing features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7eb83dc65f49659ea06d0cb2353b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed//clouds-32.npy.\n",
      "Saved clouds to /media/berlukas/Data/data/datasets/s2ae/SemanticUSL/processed//archive-32.npy\n"
     ]
    }
   ],
   "source": [
    "all_sem_clouds = []\n",
    "\n",
    "def progresser(sample_idx, grid, auto_position=True, write_safe=False, blocking=True, progress=False):\n",
    "    sample = all_sem_clouds[sample_idx]\n",
    "    sample_sphere = Sphere(sample)\n",
    "#     sample_sphere = all_sem_clouds[sample_idx]\n",
    "    features = sample_sphere.sampleUsingGrid(grid)\n",
    "    return features\n",
    "\n",
    "def parse_poses(dataroot, seq, calib):\n",
    "    file = dataroot + '/' + seq + '/poses.txt'\n",
    "    poses_arr = pd.read_csv(file, delimiter=' ', comment='#', header=None).to_numpy()    \n",
    "    poses = [np.array([[r[0], r[1], r[2], r[3]],\n",
    "                   [r[4], r[5], r[6], r[7]],\n",
    "                   [r[8], r[9], r[10], r[11]],\n",
    "                   [0, 0, 0, 1]]) for r in poses_arr]\n",
    "    \n",
    "    T_C_L = calib['Tr']\n",
    "    n_poses = len(poses)\n",
    "    for i in range(0, n_poses):    \n",
    "        T_G_C = poses[i]\n",
    "        poses[i] = T_G_C @ T_C_L\n",
    "    return poses\n",
    "\n",
    "def get_pointcloud_at(scan, name, label):\n",
    "    pointclouds = []            \n",
    "    scan.open_scan(name)\n",
    "    scan.open_label(label)\n",
    "    scan.colorize()\n",
    "\n",
    "    pc = np.column_stack((scan.points, scan.remissions, scan.sem_label))\n",
    "#     mask = pc[:,4] > 0 # Filter based on labeled data.        \n",
    "    pointclouds.append(pc)\n",
    "    return pointclouds\n",
    "\n",
    "def get_map_at(scan, names, labels, indices):\n",
    "    pointclouds = []    \n",
    "    for idx in indices:\n",
    "        scan.open_scan(names[idx])\n",
    "        scan.open_label(labels[idx])\n",
    "        scan.colorize()\n",
    "        \n",
    "        pc = np.column_stack((scan.points, scan.remissions, scan.sem_label))\n",
    "        mask = pc[:,4] > 0 # Filter based on labeled data.        \n",
    "        pointclouds.append(pc[mask])\n",
    "    return pointclouds\n",
    "\n",
    "def retrieve_poses_at(all_poses, indices):\n",
    "    poses = []\n",
    "    for idx in indices:\n",
    "        poses.append(all_poses[idx])\n",
    "    return poses\n",
    "\n",
    "def combine_pointclouds(pointclouds, poses):\n",
    "    n_data = len(poses)\n",
    "    \n",
    "    pivot = n_data // 2  \n",
    "    T_G_L_pivot = poses[pivot]\n",
    "    T_L_pivot_G = np.linalg.inv(T_G_L_pivot)\n",
    "\n",
    "    acc_points = pointclouds[pivot]\n",
    "    for i in range(0, n_data):\n",
    "        if i == pivot:\n",
    "            continue\n",
    "\n",
    "        T_G_L = poses[i]\n",
    "        T_L_pivot_L = T_L_pivot_G @ T_G_L\n",
    "\n",
    "        points = Utils.transform_pointcloud(pointclouds[i], T_L_pivot_L)\n",
    "        acc_points = np.append(acc_points, points, axis=0)                    \n",
    "    \n",
    "    return acc_points\n",
    "\n",
    "CFG = load_config_file(config_file)\n",
    "color_dict = CFG[\"color_map\"]\n",
    "nclasses = len(color_dict)\n",
    "scan = SemLaserScan(nclasses, color_dict, project=False)\n",
    "bw = 50\n",
    "assert CFG is not None\n",
    "  \n",
    "grid, _ = DHGrid.CreateGrid(bw)\n",
    "for seq in sequences:\n",
    "    print(f'Loading sequence {seq}.')    \n",
    "    scan_names, label_names = load_sequence(dataroot, seq)\n",
    "    calib = parse_calibration(dataroot, seq)\n",
    "    poses = parse_poses(dataroot, seq, calib)    \n",
    "    n_scans = len(scan_names)    \n",
    "    print(f'This sequence has {len(poses)} data elements.')\n",
    "        \n",
    "    all_sem_clouds = []\n",
    "    for i in range(0, n_scans):                \n",
    "        pointcloud = get_pointcloud_at(scan, scan_names[i], label_names[i])        \n",
    "        # all_sem_clouds.append(Sphere(pointcloud[0]))\n",
    "        all_sem_clouds.append(pointcloud[0])\n",
    "        \n",
    "    print(f\"Loading complete. Computing features...\")\n",
    "    # parallel\n",
    "    sem_idx = np.arange(0, len(all_sem_clouds))\n",
    "    sample_func = partial(progresser, grid=grid)\n",
    "    sem_features = process_map(sample_func, sem_idx, max_workers=16)            \n",
    "\n",
    "    filename = f\"{export_ds}/clouds-{seq}.npy\"\n",
    "    np.save(filename, sem_features)\n",
    "    print(f\"Wrote features to {filename}.\")\n",
    "    \n",
    "    filename = f'archive-{seq}'\n",
    "    np.save(f'{export_ds}/{filename}.npy', all_sem_clouds)\n",
    "    print(f'Saved clouds to {export_ds}/{filename}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapIntensityToRGB(i):\n",
    "    return cm.jet(plt.Normalize(min(i), max(i))(i))\n",
    "\n",
    "def visualizeRawPointcloud(pcl, val):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pcl[:, 0:3])\n",
    "    colors = mapIntensityToRGB(val)\n",
    "#     colors = scan.sem_color_lut[pcl[:,4].astype(np.int)]\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:,0:3])\n",
    "    o3d.visualization.draw_geometries([pcd], width=640,  height=480)    \n",
    "    \n",
    "def createGrid_old(bw):\n",
    "    n_grid = 2 * bw\n",
    "    k = 0;\n",
    "    points = np.empty([n_grid * n_grid, 2])\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            points[k, 0] = (np.pi*(2*i+1))/(4*bw)\n",
    "            points[k, 1] = (2*np.pi*j)/(2*bw);\n",
    "            k = k + 1;\n",
    "    return points\n",
    "    \n",
    "def convertGridToEuclidean_old(grid):\n",
    "    cart_grid = np.zeros([ grid.shape[0], 3])\n",
    "    cart_grid[:,0] = np.multiply(np.sin(grid[:,0]), np.cos(grid[:,1]))\n",
    "    cart_grid[:,1] = np.multiply(np.sin(grid[:,0]), np.sin(grid[:,1]))\n",
    "    cart_grid[:,2] = np.cos(grid[:,0])\n",
    "    return cart_grid\n",
    "\n",
    "def create_sampling_sphere(bw):\n",
    "    grid = createGrid_old(bw)\n",
    "    xyz_grid = convertGridToEuclidean_old(grid)\n",
    "    intensities = np.zeros((xyz_grid.shape[0],1))\n",
    "    sampling_grid = np.hstack((xyz_grid, np.ones((xyz_grid.shape[0], 1), dtype=xyz_grid.dtype)))\n",
    "    return sampling_grid.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = all_sem_clouds[0]\n",
    "visualizeRawPointcloud(pc, pc[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sem_features[0,:,:,:].shape\n",
    "sem_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "sampling pointcloud shape is (10000, 3)\n",
      "feature shape is (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "# filename = f\"{export_ds}/clouds-example.npy\"\n",
    "# sem_features = np.load(filename)\n",
    "\n",
    "cur_sem_cloud = sem_features[0]\n",
    "cur_sem_cloud = np.reshape(cur_sem_cloud, (3, -1)).T\n",
    "print(f'{cur_sem_cloud.shape}')\n",
    "pc = create_sampling_sphere(bw)\n",
    "points_xyz = pc.T[:,0:3]\n",
    "print(f\"sampling pointcloud shape is {points_xyz.shape}\")\n",
    "print(f\"feature shape is {cur_sem_cloud.shape}\")\n",
    "points_xyzl = np.column_stack((points_xyz, cur_sem_cloud[:,2]))\n",
    "# points_xyzn = np.column_stack((points_xyz, cur_sem_cloud[:,2]))\n",
    "\n",
    "visualizeRawPointcloud(points_xyzl, points_xyzl[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
